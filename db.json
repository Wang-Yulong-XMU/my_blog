{"meta":{"version":1,"warehouse":"6.0.0"},"models":{"Asset":[{"_id":"source/video/video.mp4","path":"video/video.mp4","modified":0,"renderable":0},{"_id":"node_modules/hexo-theme-async/source/sw.js","path":"sw.js","modified":0,"renderable":1},{"_id":"node_modules/hexo-theme-async/source/failure.ico","path":"failure.ico","modified":0,"renderable":1},{"_id":"node_modules/hexo-theme-async/source/css/index.less","path":"css/index.less","modified":0,"renderable":1},{"_id":"node_modules/hexo-theme-async/source/img/404.jpg","path":"img/404.jpg","modified":0,"renderable":1},{"_id":"node_modules/hexo-theme-async/source/img/avatar.jpg","path":"img/avatar.jpg","modified":0,"renderable":1},{"_id":"node_modules/hexo-theme-async/source/img/banner.png","path":"img/banner.png","modified":0,"renderable":1},{"_id":"node_modules/hexo-theme-async/source/img/block.jpg","path":"img/block.jpg","modified":0,"renderable":1},{"_id":"node_modules/hexo-theme-async/source/img/favicon.svg","path":"img/favicon.svg","modified":0,"renderable":1},{"_id":"node_modules/hexo-theme-async/source/img/friend_404.gif","path":"img/friend_404.gif","modified":0,"renderable":1},{"_id":"node_modules/hexo-theme-async/source/img/index.gif","path":"img/index.gif","modified":0,"renderable":1},{"_id":"node_modules/hexo-theme-async/source/img/index.mp4","path":"img/index.mp4","modified":0,"renderable":1},{"_id":"node_modules/hexo-theme-async/source/img/userimg.jpg","path":"img/userimg.jpg","modified":0,"renderable":1},{"_id":"node_modules/hexo-theme-async/source/js/main.js","path":"js/main.js","modified":0,"renderable":1},{"_id":"node_modules/hexo-theme-async/source/css/plugins/bootstrap.min.css","path":"css/plugins/bootstrap.min.css","modified":0,"renderable":1},{"_id":"node_modules/hexo-theme-async/source/css/plugins/bootstrap.row.css","path":"css/plugins/bootstrap.row.css","modified":0,"renderable":1},{"_id":"node_modules/hexo-theme-async/source/css/plugins/font-awesome.min.css","path":"css/plugins/font-awesome.min.css","modified":0,"renderable":1},{"_id":"node_modules/hexo-theme-async/source/fonts/webfonts/fa-brands-400.woff2","path":"fonts/webfonts/fa-brands-400.woff2","modified":0,"renderable":1},{"_id":"node_modules/hexo-theme-async/source/fonts/webfonts/fa-regular-400.woff2","path":"fonts/webfonts/fa-regular-400.woff2","modified":0,"renderable":1},{"_id":"node_modules/hexo-theme-async/source/fonts/webfonts/fa-solid-900.woff2","path":"fonts/webfonts/fa-solid-900.woff2","modified":0,"renderable":1},{"_id":"node_modules/hexo-theme-async/source/js/plugins/danmu.js","path":"js/plugins/danmu.js","modified":0,"renderable":1},{"_id":"node_modules/hexo-theme-async/source/js/plugins/local_search.js","path":"js/plugins/local_search.js","modified":0,"renderable":1},{"_id":"node_modules/hexo-theme-async/source/js/plugins/typing.js","path":"js/plugins/typing.js","modified":0,"renderable":1}],"Cache":[{"_id":"source/about/index.md","hash":"c48d0f168c6aef7115e59e0581094862a126d43f","modified":1759728661566},{"_id":"source/photo/index.md","hash":"b4a7cb8387cebb1ad1efb5fe5f049853c49b0ba4","modified":1759819029190},{"_id":"source/photo-city-nights/index.md","hash":"b7a43a069a3456eac117eab11d965f77c6633445","modified":1759731062667},{"_id":"source/_posts/welcome to my blog.md","hash":"66904ac26689ee75dd150cff6dffef3df56f36ed","modified":1759816208325},{"_id":"node_modules/hexo-theme-async/_config.yml","hash":"3feff1431bb48a6b3e919b81ebe3b4f2dbc16287","modified":1759655662598},{"_id":"source/_posts/随机（1）.md","hash":"11954f29746ed53b551a3cf9d89b43f184df9b85","modified":1759658755930},{"_id":"node_modules/hexo-theme-async/package.json","hash":"70d57f9d44b0738d03b7e7138f6879bea1d81479","modified":1759647602467},{"_id":"node_modules/hexo-theme-async/plugins.yml","hash":"9e50cd335ae74447b66e7b855ef825c1ba9a5a14","modified":1759647602753},{"_id":"node_modules/hexo-theme-async/languages/en.yml","hash":"84a43a5e8a37e2243512c6e305d9f935a4622beb","modified":1759647602751},{"_id":"node_modules/hexo-theme-async/languages/default.yml","hash":"243f1e303d7e180bfd462cd4bf35bc4ca1afae67","modified":1759647602750},{"_id":"node_modules/hexo-theme-async/languages/zh-Hans.yml","hash":"82e14d9c19a28abe630205575961201269ea895a","modified":1759647602754},{"_id":"node_modules/hexo-theme-async/layout/about.ejs","hash":"fa4d733a809ddd086175d3445f94c8a9bd0362f6","modified":1759647602333},{"_id":"node_modules/hexo-theme-async/layout/404.ejs","hash":"1416ba991457b267229bc4ec9f98540be1893d68","modified":1759647602330},{"_id":"node_modules/hexo-theme-async/layout/archive.ejs","hash":"2603f3bdf7cf68873e5128d45c2c518fad108837","modified":1759647602334},{"_id":"node_modules/hexo-theme-async/layout/comment.ejs","hash":"277d6a650ec6d12973565f675da5172c2112413a","modified":1759647602341},{"_id":"node_modules/hexo-theme-async/layout/index.ejs","hash":"49342e779d1dd71df6285d73c755b3870cca1e80","modified":1759647602419},{"_id":"node_modules/hexo-theme-async/layout/custom.ejs","hash":"3e019f76171e8f4392942f512b53030ff268e2db","modified":1759647602341},{"_id":"node_modules/hexo-theme-async/layout/links.ejs","hash":"c64b58b9bd84495682eadc8b06fe918f05d71da6","modified":1759647602427},{"_id":"node_modules/hexo-theme-async/layout/page.ejs","hash":"1a81e2a16d463f5f944ae3b79f444c2ac4de1ce0","modified":1759647602430},{"_id":"node_modules/hexo-theme-async/layout/project.ejs","hash":"86acaced0704da2d4c4703616a83e696350459c7","modified":1759647602439},{"_id":"node_modules/hexo-theme-async/layout/tag.ejs","hash":"215d15d9b60dbf94115eaacbafddc20a34948659","modified":1759647602441},{"_id":"node_modules/hexo-theme-async/scripts/utils.js","hash":"c0f6d020f71a7751f1d2d2a727e8d811ae84cc6b","modified":1759647602465},{"_id":"node_modules/hexo-theme-async/layout/category.ejs","hash":"69d24101887c9d024fb1de2349794a516fb17ee2","modified":1759647602340},{"_id":"node_modules/hexo-theme-async/layout/post.ejs","hash":"1967b1062c872a7c0e3131aba78aaca7f65a0a18","modified":1759647602438},{"_id":"node_modules/hexo-theme-async/layout/layout.ejs","hash":"beb7634f08a8b364f4db166fc6eeb3ea064300d4","modified":1759647602423},{"_id":"node_modules/hexo-theme-async/source/failure.ico","hash":"5eec5c163400d8ece74286da49130da1ead3f6f6","modified":1759647602443},{"_id":"node_modules/hexo-theme-async/layout/_third-party/cdn.ejs","hash":"964693a454808aa8277027ec3c5dda7f7af01255","modified":1759647602340},{"_id":"node_modules/hexo-theme-async/layout/_third-party/plugin.ejs","hash":"ce86e76f541f6c9b1856567e0bdd5ad24ce519a7","modified":1759647602431},{"_id":"node_modules/hexo-theme-async/layout/_third-party/web-analytics.ejs","hash":"7d6a743d7c18ea619b01a7740db09a1c82945331","modified":1759647602443},{"_id":"node_modules/hexo-theme-async/layout/_third-party/sw.ejs","hash":"277f75522dffa7d985f3400f18b4f60861d2430d","modified":1759647602441},{"_id":"node_modules/hexo-theme-async/layout/_partial/banner.ejs","hash":"a044ac4452808df3a779f75d8955593067484625","modified":1759647602337},{"_id":"node_modules/hexo-theme-async/layout/_partial/footer.ejs","hash":"e3c44b9bd652f46e6e75ede77019102b00f303db","modified":1759647602344},{"_id":"node_modules/hexo-theme-async/layout/_partial/head.ejs","hash":"44daee1120cf8f9b673e4cd5a44ff3c9c7e5247b","modified":1759647602345},{"_id":"node_modules/hexo-theme-async/layout/_partial/header.ejs","hash":"0409d9d28093ceaefcd6e8bd8d3f03553d8b00d0","modified":1759647602346},{"_id":"node_modules/hexo-theme-async/layout/_partial/main.ejs","hash":"63cb1b7baa5b87ae89f15352925df99691c0f2f9","modified":1759647602428},{"_id":"node_modules/hexo-theme-async/layout/_partial/script.ejs","hash":"b7b588bd748add9e2a718e9684a67293b2de96c4","modified":1759647602440},{"_id":"node_modules/hexo-theme-async/layout/_widget/about-card.ejs","hash":"eb5e8539844b28229a3d8bb26452e5800d976c2e","modified":1759647602332},{"_id":"node_modules/hexo-theme-async/layout/_widget/categorie.ejs","hash":"1752c7f055a81e4ad94cd65551f549b190adf17c","modified":1759647602339},{"_id":"node_modules/hexo-theme-async/layout/_widget/header-menu.ejs","hash":"a4a4884e1ad24da186df197fc0d9a75e6f4741c5","modified":1759647602345},{"_id":"node_modules/hexo-theme-async/layout/_widget/header-logo.ejs","hash":"ad750aad202365e0c5d35f20bc3c7954f85c0573","modified":1759647602345},{"_id":"node_modules/hexo-theme-async/layout/_widget/header-search.ejs","hash":"de299cf96a8c7e546511a4196874d44b1db43b57","modified":1759647602346},{"_id":"node_modules/hexo-theme-async/layout/_widget/paginator.ejs","hash":"c7b76e443375b7d2bb15452b34ffcc654105db1f","modified":1759647602430},{"_id":"node_modules/hexo-theme-async/layout/_widget/post-toc.ejs","hash":"c0355e8fb5067bb36976c6a6c587b9393418e861","modified":1759647602436},{"_id":"node_modules/hexo-theme-async/layout/_widget/search.ejs","hash":"573c500a4ecb65b822b3a34168dbcf446c92bc0f","modified":1759647602440},{"_id":"node_modules/hexo-theme-async/layout/_widget/header-theme.ejs","hash":"e3dd05bd47e62edc68a37ed17773211c234f2675","modified":1759647602346},{"_id":"node_modules/hexo-theme-async/scripts/console/plugin.js","hash":"0af4b27cc86549826d24e1908da4aa9966debc14","modified":1759647602462},{"_id":"node_modules/hexo-theme-async/layout/_widget/fixed-btn.ejs","hash":"7423a0638d27a7d980e7bcbc09c55761f37ff17d","modified":1759647602343},{"_id":"node_modules/hexo-theme-async/scripts/events/layout.js","hash":"b07fdc3448461ebe1e10288d0be5d5e616e99e1c","modified":1759647602458},{"_id":"node_modules/hexo-theme-async/scripts/events/config.js","hash":"a2e274fef40ca8fb981b2f0cf09c7b1c727cc93e","modified":1759647602455},{"_id":"node_modules/hexo-theme-async/scripts/filters/index.js","hash":"731a0527c291ba78a98f1fd270cc159789cd7d3f","modified":1759647602457},{"_id":"node_modules/hexo-theme-async/scripts/filters/cdn.js","hash":"b1030d8897da4bee5b66b1d669a2f5f4baee0e06","modified":1759647602454},{"_id":"node_modules/hexo-theme-async/scripts/tags/flink.js","hash":"962f71341a117ab5ff67ec7a2d83125f0f4e7e7d","modified":1759647602456},{"_id":"node_modules/hexo-theme-async/scripts/events/index.js","hash":"18b4e43ac08123bd27ee9619385a9e1d4a0fce0c","modified":1759647602457},{"_id":"node_modules/hexo-theme-async/scripts/tags/gallery.js","hash":"6aa14493096c10b73349b9d6f9ea70cfe65874b1","modified":1759647602456},{"_id":"node_modules/hexo-theme-async/scripts/tags/imgs.js","hash":"0db85bb9b42e0dc0c6ffbd019a7a9a87c7253d3e","modified":1759647602456},{"_id":"node_modules/hexo-theme-async/scripts/tags/caniuse.js","hash":"45df24923a6f3d061f37fcbb953a7492594204f3","modified":1759647602453},{"_id":"node_modules/hexo-theme-async/scripts/tags/note.js","hash":"24db43d8712bbaa6464675d5772c65226094833b","modified":1759647602461},{"_id":"node_modules/hexo-theme-async/scripts/tags/tabs.js","hash":"af6ca352d69cdd9d3ae0e0682b9ab5597fa15261","modified":1759647602463},{"_id":"node_modules/hexo-theme-async/scripts/helper/async_config.js","hash":"e5fc3d52080164ae5ad6d61b2a9950469bbf402b","modified":1759647602452},{"_id":"node_modules/hexo-theme-async/scripts/helper/index.js","hash":"16a2b891096c41467eaeb8ece9648e213c8ce837","modified":1759647602457},{"_id":"node_modules/hexo-theme-async/scripts/helper/list_archives.js","hash":"7aac0c6da98350cbe28b648b2072a491594867a8","modified":1759647602458},{"_id":"node_modules/hexo-theme-async/scripts/helper/list_categories.js","hash":"8a28147c2a0cce315746806673bf5db34fab82f6","modified":1759647602459},{"_id":"node_modules/hexo-theme-async/scripts/helper/tag.js","hash":"718f7207db1e1b8db704d83893f2daffdef9278c","modified":1759647602463},{"_id":"node_modules/hexo-theme-async/scripts/helper/toc.js","hash":"7aefa488acf5abebe205d52e5908e88cf0e14d6e","modified":1759647602464},{"_id":"node_modules/hexo-theme-async/scripts/helper/variable.js","hash":"b5912726f7015076f9671f3e24087c400c781db7","modified":1759647602467},{"_id":"node_modules/hexo-theme-async/source/css/index.less","hash":"816ae0181de8239b1487fa89480bce36abb794cd","modified":1759647602611},{"_id":"node_modules/hexo-theme-async/source/img/404.jpg","hash":"fb4489bc1d30c93d28f7332158c1c6c1416148de","modified":1759647602448},{"_id":"node_modules/hexo-theme-async/source/img/avatar.jpg","hash":"e9e4108ab199202288ee68342a40064649c3d7c1","modified":1759647602450},{"_id":"node_modules/hexo-theme-async/source/img/block.jpg","hash":"1dd350354bd3fe2e7287dfe063d7783e4ab91090","modified":1759647602451},{"_id":"node_modules/hexo-theme-async/source/img/favicon.svg","hash":"ef922eea66dc950fb1327dd4571d777b8aeb8161","modified":1759647602648},{"_id":"node_modules/hexo-theme-async/source/img/friend_404.gif","hash":"8d2d0ebef70a8eb07329f57e645889b0e420fa48","modified":1759647602443},{"_id":"node_modules/hexo-theme-async/source/img/userimg.jpg","hash":"20ac17e521e72dab1765ac8aa6e00dc7b89000ec","modified":1759648999612},{"_id":"node_modules/hexo-theme-async/source/js/main.js","hash":"72a9315d0c05eaaeab2d34d5ca8f76680e73a0b5","modified":1759647602461},{"_id":"node_modules/hexo-theme-async/layout/_third-party/comment/b-comments.ejs","hash":"398c4769c6bb945844e02fdbaddab501afbfc33f","modified":1759647602335},{"_id":"node_modules/hexo-theme-async/layout/_third-party/comment/giscus.ejs","hash":"1ea620f2e4473246272ec7e9f27f8c82f7a5f48e","modified":1759647602344},{"_id":"node_modules/hexo-theme-async/layout/_third-party/comment/index.ejs","hash":"bac629e4c8d28cc16720c17d526e0b9fc7059be3","modified":1759647602417},{"_id":"node_modules/hexo-theme-async/layout/_third-party/comment/twikoo.ejs","hash":"4a0aba54594f6692cf11f756ecac2dfb738bcdf6","modified":1759647602442},{"_id":"node_modules/hexo-theme-async/layout/_third-party/seo/baidu-push.ejs","hash":"2e3d405b5973c5ceed9cc24a5591240c0345cb94","modified":1759647602336},{"_id":"node_modules/hexo-theme-async/layout/_partial/post/post-card-mini.ejs","hash":"e7a47f33c24943229e423ab7b367973a2a014d24","modified":1759647602432},{"_id":"node_modules/hexo-theme-async/layout/_partial/post/post-card.ejs","hash":"9f12b489d174d9df83d1bef35f7a29c337b2d49a","modified":1759647602433},{"_id":"node_modules/hexo-theme-async/layout/_partial/post/post-content.ejs","hash":"2cb43a1657dfcff799f67d8e06c31d4c9b571bca","modified":1759647602433},{"_id":"node_modules/hexo-theme-async/layout/_partial/post/post-copyright.ejs","hash":"c2f0b1c2f9a4fbcf664b418f5ef1f98630d446d8","modified":1759647602434},{"_id":"node_modules/hexo-theme-async/layout/_partial/post/post-info.ejs","hash":"18b2d5d78d36fe1c265eb9c4c6aa285e38626886","modified":1759647602435},{"_id":"node_modules/hexo-theme-async/layout/_partial/post/post-next-prev.ejs","hash":"81920ae69da586274c742e1a6467b3a0145064b9","modified":1759647602436},{"_id":"node_modules/hexo-theme-async/layout/_partial/post/reward.ejs","hash":"664c43ab7346d700d942e30d2a5bdd119743c647","modified":1759647602439},{"_id":"node_modules/hexo-theme-async/layout/_partial/page/about.ejs","hash":"47d2232ac1e20fc29a358c6a66f871ca2ee4c215","modified":1759647602332},{"_id":"node_modules/hexo-theme-async/layout/_partial/page/archive.ejs","hash":"777b20cf888f43e7c0b1dddae2db576c359681c1","modified":1759647602334},{"_id":"node_modules/hexo-theme-async/layout/_partial/page/category.ejs","hash":"00082fc6b67937339416971a201ad3121d30796e","modified":1759647602339},{"_id":"node_modules/hexo-theme-async/layout/_partial/page/index.ejs","hash":"19d539a157993bd92ec61db15cf4f6c8fefbbe11","modified":1759647602412},{"_id":"node_modules/hexo-theme-async/layout/_partial/page/links.ejs","hash":"9d2815f78de8a5c2d1f8d3cae70fb133cff90c80","modified":1759647602426},{"_id":"node_modules/hexo-theme-async/layout/_partial/page/post.ejs","hash":"b55db31dacc274339a1b20a6659fb85bf884ea3a","modified":1759647602437},{"_id":"node_modules/hexo-theme-async/layout/_partial/page/project.ejs","hash":"afdd4092f92dcd2dc8cb79aba2b214a9d209b1a6","modified":1759647602439},{"_id":"node_modules/hexo-theme-async/layout/_partial/page/tag.ejs","hash":"b827961b3a394ab4952508b2e8f8fd2a58a143de","modified":1759647602441},{"_id":"node_modules/hexo-theme-async/layout/_partial/preloader/change-mode-preloader.ejs","hash":"8a6019defee71021eb9bd46549c8a4479546ec9d","modified":1759647602341},{"_id":"node_modules/hexo-theme-async/layout/_partial/preloader/page-preloader.ejs","hash":"a1ce51be9b477e146dcc3bb4fbe9628c1471969e","modified":1759647602429},{"_id":"node_modules/hexo-theme-async/layout/_partial/sidebar/index.ejs","hash":"bbdebed5b66650db2eb295491c6b4be1a3ef28c8","modified":1759647602415},{"_id":"node_modules/hexo-theme-async/source/css/_components/app-frame.less","hash":"888dce7b284a2d2a4cf67e67a76461c0fb1612c8","modified":1759647602468},{"_id":"node_modules/hexo-theme-async/source/css/_components/banner.less","hash":"1c4eda54af31014ba8d5f8d845ccae34001e3061","modified":1759647602468},{"_id":"node_modules/hexo-theme-async/source/css/_components/base.less","hash":"104dab42cb06584989f266765e0d19f38f754553","modified":1759647602468},{"_id":"node_modules/hexo-theme-async/source/css/_components/blockquote.less","hash":"50ba46440e2b59c2d6c3486924ba4d1d16ca7547","modified":1759647602469},{"_id":"node_modules/hexo-theme-async/source/css/_components/btn.less","hash":"eefe6b747d9ab02d5a5b24523994d974e50e62ca","modified":1759647602581},{"_id":"node_modules/hexo-theme-async/source/css/_components/card.less","hash":"7509c0dfe9755006ecfe2c07dad47730878c0b3f","modified":1759647602587},{"_id":"node_modules/hexo-theme-async/source/css/_components/fixed-btn.less","hash":"eee667056b0c2a5432defb26202e230a300aedc4","modified":1759647602598},{"_id":"node_modules/hexo-theme-async/source/css/_components/footer.less","hash":"c393ebf29473130907f19b94a2e78581610772d6","modified":1759647602600},{"_id":"node_modules/hexo-theme-async/source/css/_components/form.less","hash":"3e266bafb5423d5a1ca8dd985167694c2e950b1e","modified":1759647602601},{"_id":"node_modules/hexo-theme-async/source/css/_components/index.less","hash":"6e4233951d5960eb7019aff0fecce9918ac4af91","modified":1759647602601},{"_id":"node_modules/hexo-theme-async/source/css/_components/list.less","hash":"f3d810a84e47272828455362703d283b0d4b3997","modified":1759647602612},{"_id":"node_modules/hexo-theme-async/source/css/_components/menu.less","hash":"75bfe8fb8a1e667bbcb7319f3446ae730bce51ef","modified":1759647602612},{"_id":"node_modules/hexo-theme-async/source/css/_components/message.less","hash":"3be2072551fe6064afb994a39620a135c09c156d","modified":1759647602612},{"_id":"node_modules/hexo-theme-async/source/css/_components/mode-switcher.less","hash":"278b5a4d32df86717cd6564385d71ba8db39b037","modified":1759647602614},{"_id":"node_modules/hexo-theme-async/source/css/_components/pagination.less","hash":"c31230e3f8b989ecd7fd360de5617ce7203d1b3b","modified":1759647602614},{"_id":"node_modules/hexo-theme-async/README.md","hash":"d6b4ce39a096a60b2aeecbc9f217fa5edfff8684","modified":1759647602622},{"_id":"node_modules/hexo-theme-async/source/css/_components/preloader.less","hash":"bdf095448437bd711792dc60ac1c3aca147dadd1","modified":1759647602615},{"_id":"node_modules/hexo-theme-async/source/css/_components/tag-plugins.less","hash":"02e0e9dc97449014c14d05483d860a3bb206ed3e","modified":1759647602618},{"_id":"node_modules/hexo-theme-async/source/css/_components/timeline.less","hash":"e6e7b4bac39298e5d00031b0ea99313f6adcc127","modified":1759647602618},{"_id":"node_modules/hexo-theme-async/source/css/_components/title-divider.less","hash":"67d38725dcc5910f5283a01f4cffe46042a3cc67","modified":1759647602618},{"_id":"node_modules/hexo-theme-async/source/css/_components/toc.less","hash":"c6b0c29537f37506b74c432a5a3cbfa2f8182dfb","modified":1759647602619},{"_id":"node_modules/hexo-theme-async/source/css/_components/top-bar.less","hash":"805fedd1c19271f024497f34c0debe027d9d1ea5","modified":1759647602619},{"_id":"node_modules/hexo-theme-async/source/css/_components/video.less","hash":"62a48f016c23f23aec13062b0241feb702fb6a85","modified":1759647602620},{"_id":"node_modules/hexo-theme-async/source/css/_components/publication.less","hash":"87d16ff0a4c85ae5284f7128663d4f4e7731ee5e","modified":1759647602615},{"_id":"node_modules/hexo-theme-async/source/css/_variables/index.less","hash":"25061e189684118fbb5ce9e4119f60e331968065","modified":1759647602610},{"_id":"node_modules/hexo-theme-async/source/css/plugins/bootstrap.row.css","hash":"d06c09ad9e0e351893cb2503e54f9b19f821f540","modified":1759647602325},{"_id":"node_modules/hexo-theme-async/source/css/plugins/font-awesome.min.css","hash":"6137a9883adce70d78c7f7bbe8af27d91b627d32","modified":1759647602328},{"_id":"node_modules/hexo-theme-async/source/fonts/webfonts/fa-regular-400.woff2","hash":"fbb0d9d51850d824d7eba66e0e210c778d6d42f5","modified":1759647602677},{"_id":"node_modules/hexo-theme-async/source/sw.js","hash":"01b07120762ab155c99478afb36075d4d7e2bc9c","modified":1759647602463},{"_id":"node_modules/hexo-theme-async/source/js/plugins/typing.js","hash":"49e5af4f80da9bfd46a81495711822de87719656","modified":1759647602464},{"_id":"node_modules/hexo-theme-async/layout/_partial/sidebar/card/email.ejs","hash":"d8ceed83ba3b0af7d361ca9254a62afd6e8f741e","modified":1759647602342},{"_id":"node_modules/hexo-theme-async/layout/_partial/sidebar/card/social.ejs","hash":"4543c52a486a87f718fcfe03b6ef59bd771759d5","modified":1759647602440},{"_id":"node_modules/hexo-theme-async/layout/_partial/sidebar/card/info.ejs","hash":"dfa5d015d7174ad02df0099dadfab3e35e85dd59","modified":1759647602421},{"_id":"node_modules/hexo-theme-async/layout/_partial/sidebar/card/user.ejs","hash":"e883b30653e848ccd2e4697af18201aaa167445e","modified":1759647602443},{"_id":"node_modules/hexo-theme-async/source/css/_components/plugins/index.less","hash":"2c87ceca6a2642d6d4a31f6a6e293c54fb9494e9","modified":1759647602607},{"_id":"node_modules/hexo-theme-async/source/css/_components/plugins/comment/bcomments.less","hash":"2a42ff8315ae2788421111b30c00755bdcac8497","modified":1759647602469},{"_id":"node_modules/hexo-theme-async/source/css/_components/plugins/comment/index.less","hash":"0460c4f8b74cca3a8e0856772a4008aae301fc01","modified":1759647602603},{"_id":"node_modules/hexo-theme-async/source/css/_components/plugins/comment/twikoo.less","hash":"cb7d1a04b997a5d55bbd8eef351b2c5822e771da","modified":1759647602620},{"_id":"node_modules/hexo-theme-async/source/css/_components/plugins/fancybox/index.less","hash":"320806390e7e78dee470350c503a8d89e651ddb8","modified":1759647602605},{"_id":"node_modules/hexo-theme-async/source/css/_components/plugins/highlight/index.less","hash":"16629604ef46d58f80f5b89bd188817f3961634a","modified":1759647602606},{"_id":"node_modules/hexo-theme-async/source/css/_components/plugins/highlight/minix.less","hash":"d406de6ef930380b9e1f94aabcd3c42ae092c8ef","modified":1759647602613},{"_id":"node_modules/hexo-theme-async/source/css/_components/plugins/highlight/theme.less","hash":"a29e8c7c05663d074f4441e28361dc8d261e854f","modified":1759647602618},{"_id":"node_modules/hexo-theme-async/source/css/_components/plugins/read-mode/index.less","hash":"1e3da7b7eff2c38c5939f41e2ce85f98ffb7f527","modified":1759647602608},{"_id":"node_modules/hexo-theme-async/source/css/_components/plugins/read-mode/read-mode.less","hash":"0906dcdfa3bfe1b68ff6b0316ae3df2364c3d334","modified":1759647602615},{"_id":"node_modules/hexo-theme-async/source/css/_components/plugins/read-mode/single-column.less","hash":"72e2a6e8165128abb02127aef6b90441aeb92f9e","modified":1759647602616},{"_id":"node_modules/hexo-theme-async/source/css/_components/plugins/reward/index.less","hash":"519f7bb569b1fa5185f839592e58ab5ba7b88dbe","modified":1759647602608},{"_id":"node_modules/hexo-theme-async/source/css/_components/plugins/reward/reward.less","hash":"cf7a4f869b869830c38a0fad3c1be30d8d33b17a","modified":1759647602616},{"_id":"node_modules/hexo-theme-async/source/css/_components/plugins/search/index.less","hash":"120696c464c600fc43c7974e0d5ff8f5de1b9366","modified":1759647602609},{"_id":"node_modules/hexo-theme-async/source/css/_components/plugins/search/search.less","hash":"c04623b8102f806c0c140481424decad9edd5ec3","modified":1759647602616},{"_id":"node_modules/hexo-theme-async/source/css/_components/plugins/highlight/highlight/diff.less","hash":"d6b8be31a7c8ba8cd676cc039786673cf0b6eccc","modified":1759647602592},{"_id":"node_modules/hexo-theme-async/source/css/_components/plugins/highlight/highlight/index.less","hash":"bc02b3c4fed5f61e7182b73b7364fd5528d59fbe","modified":1759647602605},{"_id":"node_modules/hexo-theme-async/source/css/_components/plugins/highlight/prismjs/diff.less","hash":"1da7d603b515e02305951456dcb8b8ddf6e322f3","modified":1759647602596},{"_id":"node_modules/hexo-theme-async/source/css/_components/plugins/highlight/prismjs/index.less","hash":"54da8eddcddef000713468025443498b72ee4b86","modified":1759647602606},{"_id":"node_modules/hexo-theme-async/source/css/_components/plugins/highlight/prismjs/line-number.less","hash":"9bd2cce0c88408f64f5712d076542b623a270ab7","modified":1759647602611},{"_id":"node_modules/hexo-theme-async/source/fonts/webfonts/fa-brands-400.woff2","hash":"f988b2efe9434b0af28943708d33dd3afad9a5ba","modified":1759647602665},{"_id":"node_modules/hexo-theme-async/source/fonts/webfonts/fa-solid-900.woff2","hash":"80644191098f863f25be27841c0d92c452cf2327","modified":1759647602743},{"_id":"node_modules/hexo-theme-async/README_zh-CN.md","hash":"f246ff5feb3963b40cfe49510afaf6947736ab5b","modified":1759647602622},{"_id":"node_modules/hexo-theme-async/source/css/plugins/bootstrap.min.css","hash":"b6f019002b87e86a07275eacd3c35a13c378b5c3","modified":1759647602322},{"_id":"node_modules/hexo-theme-async/source/img/banner.png","hash":"1fc9c8d054e401d68788027107c9392f047aa5fa","modified":1759647602647},{"_id":"node_modules/hexo-theme-async/source/js/plugins/local_search.js","hash":"7fe911937bd357f4236aabd8d43ae486bad8cd8e","modified":1759647602461},{"_id":"node_modules/hexo-theme-async/source/js/plugins/danmu.js","hash":"872bf0ed19608d130bed4a2ef42119681603ab9c","modified":1759647602455},{"_id":"source/video/video.mp4","hash":"23e6005867a6af87a82be3117dbfc9cffc0c92d5","modified":1759653720554},{"_id":"node_modules/hexo-theme-async/source/img/index.mp4","hash":"23e6005867a6af87a82be3117dbfc9cffc0c92d5","modified":1759653720554},{"_id":"node_modules/hexo-theme-async/source/img/index.gif","hash":"9b0362aa749b9ecf9607931f85a9409af4ea5a16","modified":1759655527023},{"_id":"public/about/index.html","hash":"14ac66bc812769c8634c77cc3660cb2a6390f6c8","modified":1759808931242},{"_id":"public/photo-city-nights/index.html","hash":"8813aa04a4c8e31523848a47471dc8ae25d6793e","modified":1759731656014},{"_id":"public/photo/index.html","hash":"d1d0d8527eb7e50f536770ee5e75fde1403c0321","modified":1759808931242},{"_id":"public/2025/10/05/随机（1）/index.html","hash":"d215a2b7ed3047ba188d67e907b0282dfa07909e","modified":1759736288247},{"_id":"public/2025/10/05/welcome to my blog/index.html","hash":"4da3b754e36e887255787dfbffa69fc05ce3e6aa","modified":1759737840541},{"_id":"public/archives/2025/index.html","hash":"3e7f29e5b5054a7af8e43601aef3d76140ee582c","modified":1759816161679},{"_id":"public/archives/index.html","hash":"6893c7ee97443a6c59f868f4dc75eee3480baa90","modified":1759816161679},{"_id":"public/archives/2025/10/index.html","hash":"1ab004456fecefbae5fd63a854b144b2710be2cc","modified":1759816161679},{"_id":"public/index.html","hash":"f104ae3a4e63c560087e3f78db62e074237b3691","modified":1759816161679},{"_id":"public/failure.ico","hash":"5eec5c163400d8ece74286da49130da1ead3f6f6","modified":1759731656014},{"_id":"public/img/avatar.jpg","hash":"e9e4108ab199202288ee68342a40064649c3d7c1","modified":1759731656014},{"_id":"public/img/404.jpg","hash":"fb4489bc1d30c93d28f7332158c1c6c1416148de","modified":1759731656014},{"_id":"public/img/block.jpg","hash":"1dd350354bd3fe2e7287dfe063d7783e4ab91090","modified":1759731656014},{"_id":"public/img/favicon.svg","hash":"ef922eea66dc950fb1327dd4571d777b8aeb8161","modified":1759731656014},{"_id":"public/fonts/webfonts/fa-regular-400.woff2","hash":"fbb0d9d51850d824d7eba66e0e210c778d6d42f5","modified":1759731656014},{"_id":"public/img/friend_404.gif","hash":"8d2d0ebef70a8eb07329f57e645889b0e420fa48","modified":1759731656014},{"_id":"public/img/userimg.jpg","hash":"20ac17e521e72dab1765ac8aa6e00dc7b89000ec","modified":1759731656014},{"_id":"public/sw.js","hash":"01b07120762ab155c99478afb36075d4d7e2bc9c","modified":1759731656014},{"_id":"public/js/main.js","hash":"72a9315d0c05eaaeab2d34d5ca8f76680e73a0b5","modified":1759731656014},{"_id":"public/js/plugins/local_search.js","hash":"7fe911937bd357f4236aabd8d43ae486bad8cd8e","modified":1759731656014},{"_id":"public/js/plugins/danmu.js","hash":"872bf0ed19608d130bed4a2ef42119681603ab9c","modified":1759731656014},{"_id":"public/js/plugins/typing.js","hash":"49e5af4f80da9bfd46a81495711822de87719656","modified":1759731656014},{"_id":"public/css/plugins/font-awesome.min.css","hash":"6137a9883adce70d78c7f7bbe8af27d91b627d32","modified":1759731656014},{"_id":"public/css/plugins/bootstrap.row.css","hash":"d06c09ad9e0e351893cb2503e54f9b19f821f540","modified":1759731656014},{"_id":"public/css/plugins/bootstrap.min.css","hash":"b6f019002b87e86a07275eacd3c35a13c378b5c3","modified":1759731656014},{"_id":"public/fonts/webfonts/fa-brands-400.woff2","hash":"f988b2efe9434b0af28943708d33dd3afad9a5ba","modified":1759731656014},{"_id":"public/fonts/webfonts/fa-solid-900.woff2","hash":"80644191098f863f25be27841c0d92c452cf2327","modified":1759731656014},{"_id":"public/img/banner.png","hash":"1fc9c8d054e401d68788027107c9392f047aa5fa","modified":1759731656014},{"_id":"public/css/index.css","hash":"377bc329bfd88c001d4bc5aaa9f9276fccdd97f8","modified":1759731656014},{"_id":"public/video/video.mp4","hash":"23e6005867a6af87a82be3117dbfc9cffc0c92d5","modified":1759731656014},{"_id":"public/img/index.mp4","hash":"23e6005867a6af87a82be3117dbfc9cffc0c92d5","modified":1759731656014},{"_id":"public/img/index.gif","hash":"9b0362aa749b9ecf9607931f85a9409af4ea5a16","modified":1759731656014},{"_id":"source/city-nights/index.md","hash":"b7a43a069a3456eac117eab11d965f77c6633445","modified":1759731062667},{"_id":"source/photo/city-nights/index.md","hash":"7019d1dd8df8f1417c011e48fc6f10c181273b0c","modified":1759819130593},{"_id":"public/photo/city-nights/index.html","hash":"f11274c9cad5b4f5ec22fef699601ec61bb38794","modified":1759742742867},{"_id":"source/_posts/随记（1）.md","hash":"cdbea73b711c073a0372fe3b5001a7ae6017246a","modified":1759816197783},{"_id":"public/2025/10/05/随记（1）/index.html","hash":"1fa624267bff30d11e475047cfeb9660caab2cc0","modified":1759816161679},{"_id":"source/_posts/1）分层贝叶斯：朴素贝叶斯.md","hash":"737828629f5d720a8f91049861df0604ba98c482","modified":1759816259216},{"_id":"public/2025/10/06/1）分层贝叶斯：朴素贝叶斯/index.html","hash":"17643d7194dc585a9649bed67c732d6fbaee85fd","modified":1759816161679},{"_id":"public/2025/10/06/welcome to my blog/index.html","hash":"ff89a3a196162c3310f0323f0e3b8e6a82ce24d0","modified":1759808931242},{"_id":"public/categories/随笔/index.html","hash":"535e9b17dc064fba12079f79a87e0ce87055d51c","modified":1759816161679},{"_id":"public/categories/算法/index.html","hash":"805f872eccffd945bf1b73a0cb1820ed789055b3","modified":1759816161679},{"_id":"source/_posts/2）分层贝叶斯：贝叶斯算法 .md","hash":"6ff91cddda495456cc4cfae6db44b206a6d2b4ac","modified":1759816219860},{"_id":"public/2025/10/07/2）分层贝叶斯：贝叶斯算法 /index.html","hash":"beec8dda865183f8529618420c63f8ae790c0efe","modified":1759816161679},{"_id":"public/2025/10/01/welcome to my blog/index.html","hash":"0ef57880c6c9ee493eaf820b2db89e9267e395f8","modified":1759816161679},{"_id":"source/_posts/3）分层贝叶斯：MCMC.md","hash":"7397e07d25860551eecace508facb5b2fff26598","modified":1759818001859}],"Category":[{"name":"随笔","_id":"cuidgPvbAZnTxLik2uzhatT8N"},{"name":"算法","_id":"cuid-Y7OmDUvgoEY9pvx4ncaC"},{"name":"随","_id":"cuidyhJbrI0ZoOltLyYMg3ZNp"}],"Data":[],"Page":[{"title":"关于","layout":"about","date":"2025-10-05T08:45:43.000Z","_content":"\n#### \n\n##### 📝 博客分类\n\n    \n\n    \n\n> 本站文章主要分为以下几类，希望能为您带来价值：\n> \n> 1. **技术分享：** 解决实际工作中的问题，提供教程和踩坑记录。\n> \n> 2. **思考随笔：** 关于个人成长和研究学习的想法。\n\n---\n\n    \n\n    \n\n##### ☕ 我的兴趣爱好\n\n    \n\n    \n\n    \n\n> - **旅行与探索：** 从方寸到天地。\n> \n> - **摄影：** 记录与分享。\n> \n> - **小说&电影：** 精神食粮。\n\n ---\n\n    \n\n    \n\n##### ✉️ 联系我\n\n    \n\n    \n\n> 如果您有任何问题、建议，可以通过以下方式找到我：\n> \n> - *Email：wiaolongxi@gmail.com*\n\n---\n\n                                      \n\n                            \n\n## AI for Anything\n","source":"about/index.md","raw":"---\ntitle: 关于\nlayout: about\ndate: 2025-10-05 16:45:43\n---\n\n#### \n\n##### 📝 博客分类\n\n    \n\n    \n\n> 本站文章主要分为以下几类，希望能为您带来价值：\n> \n> 1. **技术分享：** 解决实际工作中的问题，提供教程和踩坑记录。\n> \n> 2. **思考随笔：** 关于个人成长和研究学习的想法。\n\n---\n\n    \n\n    \n\n##### ☕ 我的兴趣爱好\n\n    \n\n    \n\n    \n\n> - **旅行与探索：** 从方寸到天地。\n> \n> - **摄影：** 记录与分享。\n> \n> - **小说&电影：** 精神食粮。\n\n ---\n\n    \n\n    \n\n##### ✉️ 联系我\n\n    \n\n    \n\n> 如果您有任何问题、建议，可以通过以下方式找到我：\n> \n> - *Email：wiaolongxi@gmail.com*\n\n---\n\n                                      \n\n                            \n\n## AI for Anything\n","updated":"2025-10-06T05:31:01.566Z","path":"about/index.html","comments":1,"_id":"cuid7bV7CX0JJOLWOlKPY9-p0","content":"<h4 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h4><h5 id=\"📝-博客分类\"><a href=\"#📝-博客分类\" class=\"headerlink\" title=\"📝 博客分类\"></a>📝 博客分类</h5><p>    </p>\n<p>    </p>\n<blockquote>\n<p>本站文章主要分为以下几类，希望能为您带来价值：</p>\n<ol>\n<li><p><strong>技术分享：</strong> 解决实际工作中的问题，提供教程和踩坑记录。</p>\n</li>\n<li><p><strong>思考随笔：</strong> 关于个人成长和研究学习的想法。</p>\n</li>\n</ol>\n</blockquote>\n<hr>\n<p>    </p>\n<p>    </p>\n<h5 id=\"☕-我的兴趣爱好\"><a href=\"#☕-我的兴趣爱好\" class=\"headerlink\" title=\"☕ 我的兴趣爱好\"></a>☕ 我的兴趣爱好</h5><p>    </p>\n<p>    </p>\n<p>    </p>\n<blockquote>\n<ul>\n<li><p><strong>旅行与探索：</strong> 从方寸到天地。</p>\n</li>\n<li><p><strong>摄影：</strong> 记录与分享。</p>\n</li>\n<li><p><strong>小说&amp;电影：</strong> 精神食粮。</p>\n</li>\n</ul>\n</blockquote>\n<hr>\n<p>    </p>\n<p>    </p>\n<h5 id=\"✉️-联系我\"><a href=\"#✉️-联系我\" class=\"headerlink\" title=\"✉️ 联系我\"></a>✉️ 联系我</h5><p>    </p>\n<p>    </p>\n<blockquote>\n<p>如果您有任何问题、建议，可以通过以下方式找到我：</p>\n<ul>\n<li><em>Email：<a href=\"mailto:&#119;&#x69;&#97;&#111;&#108;&#x6f;&#x6e;&#103;&#x78;&#x69;&#x40;&#103;&#109;&#x61;&#x69;&#x6c;&#x2e;&#99;&#111;&#x6d;\">wiaolongxi@gmail.com</a></em></li>\n</ul>\n</blockquote>\n<hr>\n<p>                                      </p>\n<p>                            </p>\n<h2 id=\"AI-for-Anything\"><a href=\"#AI-for-Anything\" class=\"headerlink\" title=\"AI for Anything\"></a>AI for Anything</h2>","excerpt":"","more":"<h4 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h4><h5 id=\"📝-博客分类\"><a href=\"#📝-博客分类\" class=\"headerlink\" title=\"📝 博客分类\"></a>📝 博客分类</h5><p>    </p>\n<p>    </p>\n<blockquote>\n<p>本站文章主要分为以下几类，希望能为您带来价值：</p>\n<ol>\n<li><p><strong>技术分享：</strong> 解决实际工作中的问题，提供教程和踩坑记录。</p>\n</li>\n<li><p><strong>思考随笔：</strong> 关于个人成长和研究学习的想法。</p>\n</li>\n</ol>\n</blockquote>\n<hr>\n<p>    </p>\n<p>    </p>\n<h5 id=\"☕-我的兴趣爱好\"><a href=\"#☕-我的兴趣爱好\" class=\"headerlink\" title=\"☕ 我的兴趣爱好\"></a>☕ 我的兴趣爱好</h5><p>    </p>\n<p>    </p>\n<p>    </p>\n<blockquote>\n<ul>\n<li><p><strong>旅行与探索：</strong> 从方寸到天地。</p>\n</li>\n<li><p><strong>摄影：</strong> 记录与分享。</p>\n</li>\n<li><p><strong>小说&amp;电影：</strong> 精神食粮。</p>\n</li>\n</ul>\n</blockquote>\n<hr>\n<p>    </p>\n<p>    </p>\n<h5 id=\"✉️-联系我\"><a href=\"#✉️-联系我\" class=\"headerlink\" title=\"✉️ 联系我\"></a>✉️ 联系我</h5><p>    </p>\n<p>    </p>\n<blockquote>\n<p>如果您有任何问题、建议，可以通过以下方式找到我：</p>\n<ul>\n<li><em>Email：<a href=\"mailto:&#119;&#x69;&#97;&#111;&#108;&#x6f;&#x6e;&#103;&#x78;&#x69;&#x40;&#103;&#109;&#x61;&#x69;&#x6c;&#x2e;&#99;&#111;&#x6d;\">wiaolongxi@gmail.com</a></em></li>\n</ul>\n</blockquote>\n<hr>\n<p>                                      </p>\n<p>                            </p>\n<h2 id=\"AI-for-Anything\"><a href=\"#AI-for-Anything\" class=\"headerlink\" title=\"AI for Anything\"></a>AI for Anything</h2>"},{"title":"摄影集","date":"2025-10-05T14:30:00.000Z","single_column":true,"_content":"---\n\n<div class=\"row\">\n\n{% gallery '厦门大学' '古与今的交汇点。' '/photo/city-nights' /img/resize/DSC_3109-已增强-降噪-1_resized.jpg %}\n\n</div>\n\n> 所有照片均为原创，请勿随意转载。\n","source":"photo/index.md","raw":"---\n---\ntitle: 摄影集\ndate: 2025-10-05 22:30:00 # 当前日期\nsingle_column: true\n---\n---\n\n<div class=\"row\">\n\n{% gallery '厦门大学' '古与今的交汇点。' '/photo/city-nights' /img/resize/DSC_3109-已增强-降噪-1_resized.jpg %}\n\n</div>\n\n> 所有照片均为原创，请勿随意转载。\n","updated":"2025-10-07T06:37:09.190Z","path":"photo/index.html","_id":"cuidT8-gued9xqM8OTYXpkXMz","comments":1,"layout":"page","content":"<hr>\n<div class=\"row\">\n\n\n<div class=\"col-lg-6\">\n    <a href=\"/photo/city-nights\" class=\"trm-portfolio-item trm-scroll-animation\">\n        <div class=\"trm-cover-frame\" style=\"padding-bottom:80%\">\n            <img class=\"trm-cover no-fancybox\" src=\"/img/resize/DSC_3109-%E5%B7%B2%E5%A2%9E%E5%BC%BA-%E9%99%8D%E5%99%AA-1_resized.jpg\" alt=\"Group Image Gallery\"  data-tag='post-image' onload='this.onload=null;this.style.opacity=1;' loading=\"lazy\" onerror='this.onerror=null;this.src=\"/img/404.jpg\"'>\n        </div>\n        <div class=\"trm-item-description\">\n            <div>\n                <h6>厦门大学</h6>\n                <p style=\"margin: 5px 0 0;font-size: .9rem;opacity: .8;\">古与今的交汇点。</p>\n            </div>\n        </div>\n    </a>\n</div>\n\n</div>\n\n<blockquote>\n<p>所有照片均为原创，请勿随意转载。</p>\n</blockquote>\n","excerpt":"","more":"<hr>\n<div class=\"row\">\n\n\n<div class=\"col-lg-6\">\n    <a href=\"/photo/city-nights\" class=\"trm-portfolio-item trm-scroll-animation\">\n        <div class=\"trm-cover-frame\" style=\"padding-bottom:80%\">\n            <img class=\"trm-cover no-fancybox\" src=\"/img/resize/DSC_3109-%E5%B7%B2%E5%A2%9E%E5%BC%BA-%E9%99%8D%E5%99%AA-1_resized.jpg\" alt=\"Group Image Gallery\">\n        </div>\n        <div class=\"trm-item-description\">\n            <div>\n                <h6>厦门大学</h6>\n                <p style=\"margin: 5px 0 0;font-size: .9rem;opacity: .8;\">古与今的交汇点。</p>\n            </div>\n        </div>\n    </a>\n</div>\n\n</div>\n\n<blockquote>\n<p>所有照片均为原创，请勿随意转载。</p>\n</blockquote>\n"},{"title":"🏛️ 厦大","date":"2025-10-05T16:05:00.000Z","single_column":true,"_content":"\n{% galleryGroup %}\n{% galleryItem /img/resize/DSC_3134-5_resized.jpg /img/photo/DSC_3134-5.jpg %}\n{% galleryItem /img/resize/DSC_3128-4_resized.jpg /img/photo/DSC_3128-4.jpg %}\n\n{% galleryItem /img/resize/DSC_3110-2_resized.jpg /img/photo/DSC_3110-2.jpg %}\n\n{% galleryItem /img/resize/DSC_3113-%E5%B7%B2%E5%A2%9E%E5%BC%BA-%E9%99%8D%E5%99%AA-3_resized.jpg /img/photo/DSC_3113-%E5%B7%B2%E5%A2%9E%E5%BC%BA-%E9%99%8D%E5%99%AA-3.jpg %}\n\n{% galleryItem /img/resize/DSC_3135-%E5%B7%B2%E5%A2%9E%E5%BC%BA-%E9%99%8D%E5%99%AA-6_resized.jpg /img/photo/DSC_3135-%E5%B7%B2%E5%A2%9E%E5%BC%BA-%E9%99%8D%E5%99%AA-6.jpg %}\n\n{% galleryItem /img/resize/DSC_3141-%E5%B7%B2%E5%A2%9E%E5%BC%BA-%E9%99%8D%E5%99%AA-7_resized.jpg /img/photo/DSC_3141-%E5%B7%B2%E5%A2%9E%E5%BC%BA-%E9%99%8D%E5%99%AA-7.jpg %}\n\n{% galleryItem /img/resize/DSC_3159-8_resized.jpg /img/photo/DSC_3159-8.jpg %}\n\n{% galleryItem /img/resize/DSC_3177-9_resized.jpg /img/photo/DSC_3177-9.jpg %}\n\n{% endgalleryGroup %}\n\n> 所有照片均为原创，请勿随意转载。\n","source":"photo/city-nights/index.md","raw":"---\ntitle: 🏛️ 厦大\ndate: 2025-10-06 00:05:00\nsingle_column: true # 建议保持单栏，让图片铺满视野\n---\n\n{% galleryGroup %}\n{% galleryItem /img/resize/DSC_3134-5_resized.jpg /img/photo/DSC_3134-5.jpg %}\n{% galleryItem /img/resize/DSC_3128-4_resized.jpg /img/photo/DSC_3128-4.jpg %}\n\n{% galleryItem /img/resize/DSC_3110-2_resized.jpg /img/photo/DSC_3110-2.jpg %}\n\n{% galleryItem /img/resize/DSC_3113-%E5%B7%B2%E5%A2%9E%E5%BC%BA-%E9%99%8D%E5%99%AA-3_resized.jpg /img/photo/DSC_3113-%E5%B7%B2%E5%A2%9E%E5%BC%BA-%E9%99%8D%E5%99%AA-3.jpg %}\n\n{% galleryItem /img/resize/DSC_3135-%E5%B7%B2%E5%A2%9E%E5%BC%BA-%E9%99%8D%E5%99%AA-6_resized.jpg /img/photo/DSC_3135-%E5%B7%B2%E5%A2%9E%E5%BC%BA-%E9%99%8D%E5%99%AA-6.jpg %}\n\n{% galleryItem /img/resize/DSC_3141-%E5%B7%B2%E5%A2%9E%E5%BC%BA-%E9%99%8D%E5%99%AA-7_resized.jpg /img/photo/DSC_3141-%E5%B7%B2%E5%A2%9E%E5%BC%BA-%E9%99%8D%E5%99%AA-7.jpg %}\n\n{% galleryItem /img/resize/DSC_3159-8_resized.jpg /img/photo/DSC_3159-8.jpg %}\n\n{% galleryItem /img/resize/DSC_3177-9_resized.jpg /img/photo/DSC_3177-9.jpg %}\n\n{% endgalleryGroup %}\n\n> 所有照片均为原创，请勿随意转载。\n","updated":"2025-10-07T06:38:50.593Z","path":"photo/city-nights/index.html","_id":"cuidq4oxC8tcsVsJetqzo_yQ1","comments":1,"layout":"page","content":"<div class=\"fj-gallery no-fancybox\">\n    <img src=\"/img/resize/DSC_3134-5_resized.jpg\" data-src=\"/img/photo/DSC_3134-5.jpg\"  data-tag='post-image' onload='this.onload=null;this.style.opacity=1;' loading=\"lazy\" onerror='this.onerror=null;this.src=\"/img/404.jpg\"'><img src=\"/img/resize/DSC_3128-4_resized.jpg\" data-src=\"/img/photo/DSC_3128-4.jpg\"  data-tag='post-image' onload='this.onload=null;this.style.opacity=1;' loading=\"lazy\" onerror='this.onerror=null;this.src=\"/img/404.jpg\"'><img src=\"/img/resize/DSC_3110-2_resized.jpg\" data-src=\"/img/photo/DSC_3110-2.jpg\"  data-tag='post-image' onload='this.onload=null;this.style.opacity=1;' loading=\"lazy\" onerror='this.onerror=null;this.src=\"/img/404.jpg\"'><img src=\"/img/resize/DSC_3113-%E5%B7%B2%E5%A2%9E%E5%BC%BA-%E9%99%8D%E5%99%AA-3_resized.jpg\" data-src=\"/img/photo/DSC_3113-%E5%B7%B2%E5%A2%9E%E5%BC%BA-%E9%99%8D%E5%99%AA-3.jpg\"  data-tag='post-image' onload='this.onload=null;this.style.opacity=1;' loading=\"lazy\" onerror='this.onerror=null;this.src=\"/img/404.jpg\"'><img src=\"/img/resize/DSC_3135-%E5%B7%B2%E5%A2%9E%E5%BC%BA-%E9%99%8D%E5%99%AA-6_resized.jpg\" data-src=\"/img/photo/DSC_3135-%E5%B7%B2%E5%A2%9E%E5%BC%BA-%E9%99%8D%E5%99%AA-6.jpg\"  data-tag='post-image' onload='this.onload=null;this.style.opacity=1;' loading=\"lazy\" onerror='this.onerror=null;this.src=\"/img/404.jpg\"'><img src=\"/img/resize/DSC_3141-%E5%B7%B2%E5%A2%9E%E5%BC%BA-%E9%99%8D%E5%99%AA-7_resized.jpg\" data-src=\"/img/photo/DSC_3141-%E5%B7%B2%E5%A2%9E%E5%BC%BA-%E9%99%8D%E5%99%AA-7.jpg\"  data-tag='post-image' onload='this.onload=null;this.style.opacity=1;' loading=\"lazy\" onerror='this.onerror=null;this.src=\"/img/404.jpg\"'><img src=\"/img/resize/DSC_3159-8_resized.jpg\" data-src=\"/img/photo/DSC_3159-8.jpg\"  data-tag='post-image' onload='this.onload=null;this.style.opacity=1;' loading=\"lazy\" onerror='this.onerror=null;this.src=\"/img/404.jpg\"'><img src=\"/img/resize/DSC_3177-9_resized.jpg\" data-src=\"/img/photo/DSC_3177-9.jpg\"  data-tag='post-image' onload='this.onload=null;this.style.opacity=1;' loading=\"lazy\" onerror='this.onerror=null;this.src=\"/img/404.jpg\"'>\n</div>\n\n<blockquote>\n<p>所有照片均为原创，请勿随意转载。</p>\n</blockquote>\n","excerpt":"","more":"<div class=\"fj-gallery no-fancybox\">\n    <img src=\"/img/resize/DSC_3134-5_resized.jpg\" data-src=\"/img/photo/DSC_3134-5.jpg\"><img src=\"/img/resize/DSC_3128-4_resized.jpg\" data-src=\"/img/photo/DSC_3128-4.jpg\"><img src=\"/img/resize/DSC_3110-2_resized.jpg\" data-src=\"/img/photo/DSC_3110-2.jpg\"><img src=\"/img/resize/DSC_3113-%E5%B7%B2%E5%A2%9E%E5%BC%BA-%E9%99%8D%E5%99%AA-3_resized.jpg\" data-src=\"/img/photo/DSC_3113-%E5%B7%B2%E5%A2%9E%E5%BC%BA-%E9%99%8D%E5%99%AA-3.jpg\"><img src=\"/img/resize/DSC_3135-%E5%B7%B2%E5%A2%9E%E5%BC%BA-%E9%99%8D%E5%99%AA-6_resized.jpg\" data-src=\"/img/photo/DSC_3135-%E5%B7%B2%E5%A2%9E%E5%BC%BA-%E9%99%8D%E5%99%AA-6.jpg\"><img src=\"/img/resize/DSC_3141-%E5%B7%B2%E5%A2%9E%E5%BC%BA-%E9%99%8D%E5%99%AA-7_resized.jpg\" data-src=\"/img/photo/DSC_3141-%E5%B7%B2%E5%A2%9E%E5%BC%BA-%E9%99%8D%E5%99%AA-7.jpg\"><img src=\"/img/resize/DSC_3159-8_resized.jpg\" data-src=\"/img/photo/DSC_3159-8.jpg\"><img src=\"/img/resize/DSC_3177-9_resized.jpg\" data-src=\"/img/photo/DSC_3177-9.jpg\">\n</div>\n\n<blockquote>\n<p>所有照片均为原创，请勿随意转载。</p>\n</blockquote>\n"}],"Post":[{"title":"欢迎来到我的博客！","date":"2025-10-01T05:00:00.000Z","katex":false,"cover":"/img/780.png","_content":"\n## 🥳 博客开张大吉！\n\nHello world！经过一段时间的折腾和搭建，我的个人博客终于正式上线啦！\n\n能够在这个小小的数字空间里，拥有一个完全属于自己的角落，是一件令人兴奋的事情。\n\n### 💡 为什么搭建这个博客？\n\n我的初衷非常简单：**记录、分享。**\n\n1. **记录我的成长轨迹：** 我会在这里记录我在 学习和工作中的心得体会、踩过的坑以及解决方案。\n2. **分享经验与知识：** 希望我的经验能够帮助到有同样困惑的朋友。这里将是我的知识库。\n\n### 🧭 博客内容预告\n\n本站点的文章会主要聚焦于以下几个方面，请随意通过顶部的菜单探索感兴趣的内容：\n\n* **技术探索：** 深入探讨agent,llm,ml。\n* **摄影作品与心得：** 分享我的作品，并记录每一次快门背后的思考。\n* **生活随笔：** 记录一些关于效率、阅读、个人成长等非技术类的零散思考。\n\n### 🎉 特别鸣谢\n\n最后，我要特别感谢 **Hexo** 框架以及 **Hexo-Theme-Async** 主题的作者们。正是因为有优秀的开源社区，才让我能快速搭建起如此美观且功能强大的博客。\n\n> 感谢开源！感谢所有贡献者！\n","source":"_posts/welcome to my blog.md","raw":"---\ntitle: 欢迎来到我的博客！\ndate: 2025-10-01 13:00\n\ncategories: 随笔\n\nkatex: false\ncover: /img/780.png\n---\n\n## 🥳 博客开张大吉！\n\nHello world！经过一段时间的折腾和搭建，我的个人博客终于正式上线啦！\n\n能够在这个小小的数字空间里，拥有一个完全属于自己的角落，是一件令人兴奋的事情。\n\n### 💡 为什么搭建这个博客？\n\n我的初衷非常简单：**记录、分享。**\n\n1. **记录我的成长轨迹：** 我会在这里记录我在 学习和工作中的心得体会、踩过的坑以及解决方案。\n2. **分享经验与知识：** 希望我的经验能够帮助到有同样困惑的朋友。这里将是我的知识库。\n\n### 🧭 博客内容预告\n\n本站点的文章会主要聚焦于以下几个方面，请随意通过顶部的菜单探索感兴趣的内容：\n\n* **技术探索：** 深入探讨agent,llm,ml。\n* **摄影作品与心得：** 分享我的作品，并记录每一次快门背后的思考。\n* **生活随笔：** 记录一些关于效率、阅读、个人成长等非技术类的零散思考。\n\n### 🎉 特别鸣谢\n\n最后，我要特别感谢 **Hexo** 框架以及 **Hexo-Theme-Async** 主题的作者们。正是因为有优秀的开源社区，才让我能快速搭建起如此美观且功能强大的博客。\n\n> 感谢开源！感谢所有贡献者！\n","slug":"welcome to my blog","published":1,"updated":"2025-10-07T05:50:08.325Z","_id":"cuidlEkf9c-m5o6skFFETl2wC","comments":1,"layout":"post","photos":[],"content":"<h2 id=\"🥳-博客开张大吉！\"><a href=\"#🥳-博客开张大吉！\" class=\"headerlink\" title=\"🥳 博客开张大吉！\"></a>🥳 博客开张大吉！</h2><p>Hello world！经过一段时间的折腾和搭建，我的个人博客终于正式上线啦！</p>\n<p>能够在这个小小的数字空间里，拥有一个完全属于自己的角落，是一件令人兴奋的事情。</p>\n<h3 id=\"💡-为什么搭建这个博客？\"><a href=\"#💡-为什么搭建这个博客？\" class=\"headerlink\" title=\"💡 为什么搭建这个博客？\"></a>💡 为什么搭建这个博客？</h3><p>我的初衷非常简单：<strong>记录、分享。</strong></p>\n<ol>\n<li><strong>记录我的成长轨迹：</strong> 我会在这里记录我在 学习和工作中的心得体会、踩过的坑以及解决方案。</li>\n<li><strong>分享经验与知识：</strong> 希望我的经验能够帮助到有同样困惑的朋友。这里将是我的知识库。</li>\n</ol>\n<h3 id=\"🧭-博客内容预告\"><a href=\"#🧭-博客内容预告\" class=\"headerlink\" title=\"🧭 博客内容预告\"></a>🧭 博客内容预告</h3><p>本站点的文章会主要聚焦于以下几个方面，请随意通过顶部的菜单探索感兴趣的内容：</p>\n<ul>\n<li><strong>技术探索：</strong> 深入探讨agent,llm,ml。</li>\n<li><strong>摄影作品与心得：</strong> 分享我的作品，并记录每一次快门背后的思考。</li>\n<li><strong>生活随笔：</strong> 记录一些关于效率、阅读、个人成长等非技术类的零散思考。</li>\n</ul>\n<h3 id=\"🎉-特别鸣谢\"><a href=\"#🎉-特别鸣谢\" class=\"headerlink\" title=\"🎉 特别鸣谢\"></a>🎉 特别鸣谢</h3><p>最后，我要特别感谢 <strong>Hexo</strong> 框架以及 <strong>Hexo-Theme-Async</strong> 主题的作者们。正是因为有优秀的开源社区，才让我能快速搭建起如此美观且功能强大的博客。</p>\n<blockquote>\n<p>感谢开源！感谢所有贡献者！</p>\n</blockquote>\n","excerpt":"","more":"<h2 id=\"🥳-博客开张大吉！\"><a href=\"#🥳-博客开张大吉！\" class=\"headerlink\" title=\"🥳 博客开张大吉！\"></a>🥳 博客开张大吉！</h2><p>Hello world！经过一段时间的折腾和搭建，我的个人博客终于正式上线啦！</p>\n<p>能够在这个小小的数字空间里，拥有一个完全属于自己的角落，是一件令人兴奋的事情。</p>\n<h3 id=\"💡-为什么搭建这个博客？\"><a href=\"#💡-为什么搭建这个博客？\" class=\"headerlink\" title=\"💡 为什么搭建这个博客？\"></a>💡 为什么搭建这个博客？</h3><p>我的初衷非常简单：<strong>记录、分享。</strong></p>\n<ol>\n<li><strong>记录我的成长轨迹：</strong> 我会在这里记录我在 学习和工作中的心得体会、踩过的坑以及解决方案。</li>\n<li><strong>分享经验与知识：</strong> 希望我的经验能够帮助到有同样困惑的朋友。这里将是我的知识库。</li>\n</ol>\n<h3 id=\"🧭-博客内容预告\"><a href=\"#🧭-博客内容预告\" class=\"headerlink\" title=\"🧭 博客内容预告\"></a>🧭 博客内容预告</h3><p>本站点的文章会主要聚焦于以下几个方面，请随意通过顶部的菜单探索感兴趣的内容：</p>\n<ul>\n<li><strong>技术探索：</strong> 深入探讨agent,llm,ml。</li>\n<li><strong>摄影作品与心得：</strong> 分享我的作品，并记录每一次快门背后的思考。</li>\n<li><strong>生活随笔：</strong> 记录一些关于效率、阅读、个人成长等非技术类的零散思考。</li>\n</ul>\n<h3 id=\"🎉-特别鸣谢\"><a href=\"#🎉-特别鸣谢\" class=\"headerlink\" title=\"🎉 特别鸣谢\"></a>🎉 特别鸣谢</h3><p>最后，我要特别感谢 <strong>Hexo</strong> 框架以及 <strong>Hexo-Theme-Async</strong> 主题的作者们。正是因为有优秀的开源社区，才让我能快速搭建起如此美观且功能强大的博客。</p>\n<blockquote>\n<p>感谢开源！感谢所有贡献者！</p>\n</blockquote>\n"},{"title":"随记（1）","date":"2025-10-05T05:57:11.000Z","katex":false,"cover":"/img/sea_llm.png","_content":"\n## 海水养殖碳足迹垂直大模型尚未解决的一些问题\n\n1. 多值问题：当检索到的投入品有多个取值时，如何计算取值（平均，加权平均？）\n\n2. 投入品名称问题：当多个检索文档检索到同一类投入品时，往往因为投入品具体名称不同而分成不同类投入品建立清单（如尼龙6，尼龙，尼龙绳），怎么判断投入品应该加入清单？\n\n3. 排放因子对应问题：由于投入品名称不统一，难以完全对应排放因子所在的数据库中的投入品名称\n\n4. 基于分阶段检索的问题：我们是分阶段检索建立表格，这完全依赖于在数值收集和处理中对数据附带标签，对于没有提及具体阶段的数据，如何处理？\n\n---\n\n**以上问题人类是怎么做的？拿到的数据（文献，现场调研报告，公司年报等）应该仍然有以上问题，人类如何解决？**\n\n### 固定表格以及投入品名称时会造成更多难以接受的问题\n\n1. 检索困难：当投入品名称固定时，因为名称本身与原始数据中名称不一致造成无法检索到对应数据，以至于无法生成\n\n2. 难以应对多种多样的养殖品种和养殖模式：不同的品种或者模式投入品都大不相同，难以使用一张固定的表格全部覆盖，失去泛化性\n\n## 有待量化的一些指标和量化方法\n\n因为“复利效应”，即多次小概率失误导致大概率错误，所以每个环节都不得不保证卓越的准确性\n\n#### **量化指标**\n\n表格解耦准确率：因为大量数据来源图表，我们之前选择了LLM解耦表格的做法，但是如何确定LLM能够完整无误的解耦？\n\n召回率：RAG经典的评测指标，量化是否完整的检索到了所需的数据\n\n端到端准确率：量化最终生成的清单准确性，我们的应用场景下目前人工对比为最优方法\n\n#### 量化方法\n\n表格解耦率：使用目前收集到的表格数据，进行解耦，采用人工校对加LLM校对的方法。对完整解耦后的表格人工对比每项是否准确，也可以对每个解耦的单元，把原始表格和解耦数据送入LLM，AI校对输出T/F（AI校对已经成为AI领域流行方法）。\n\n召回率指标：这些指标依赖于一个有标注的数据集，标注出了【问题，相关文档】数据对，这需要大量人工精力。能否使用经典的RAG数据集代替而不是手动标注海水养殖数据？\n\n文本相关度：直接将召回的文本对比原问题，计算BLUE等无需数据标注的指标\n\n端到端准确率：基于课题组“鱼虾贝藻”等已经算过清单的数据，对比模型生成清单（问题在于，这样的数据只有个位数，会不会过少？）\n","source":"_posts/随记（1）.md","raw":"---\ntitle: 随记（1）\ndate: 2025-10-05 13:57:11\n\ncategory: 随笔\n\nkatex: false\ncover: /img/sea_llm.png\n---\n\n## 海水养殖碳足迹垂直大模型尚未解决的一些问题\n\n1. 多值问题：当检索到的投入品有多个取值时，如何计算取值（平均，加权平均？）\n\n2. 投入品名称问题：当多个检索文档检索到同一类投入品时，往往因为投入品具体名称不同而分成不同类投入品建立清单（如尼龙6，尼龙，尼龙绳），怎么判断投入品应该加入清单？\n\n3. 排放因子对应问题：由于投入品名称不统一，难以完全对应排放因子所在的数据库中的投入品名称\n\n4. 基于分阶段检索的问题：我们是分阶段检索建立表格，这完全依赖于在数值收集和处理中对数据附带标签，对于没有提及具体阶段的数据，如何处理？\n\n---\n\n**以上问题人类是怎么做的？拿到的数据（文献，现场调研报告，公司年报等）应该仍然有以上问题，人类如何解决？**\n\n### 固定表格以及投入品名称时会造成更多难以接受的问题\n\n1. 检索困难：当投入品名称固定时，因为名称本身与原始数据中名称不一致造成无法检索到对应数据，以至于无法生成\n\n2. 难以应对多种多样的养殖品种和养殖模式：不同的品种或者模式投入品都大不相同，难以使用一张固定的表格全部覆盖，失去泛化性\n\n## 有待量化的一些指标和量化方法\n\n因为“复利效应”，即多次小概率失误导致大概率错误，所以每个环节都不得不保证卓越的准确性\n\n#### **量化指标**\n\n表格解耦准确率：因为大量数据来源图表，我们之前选择了LLM解耦表格的做法，但是如何确定LLM能够完整无误的解耦？\n\n召回率：RAG经典的评测指标，量化是否完整的检索到了所需的数据\n\n端到端准确率：量化最终生成的清单准确性，我们的应用场景下目前人工对比为最优方法\n\n#### 量化方法\n\n表格解耦率：使用目前收集到的表格数据，进行解耦，采用人工校对加LLM校对的方法。对完整解耦后的表格人工对比每项是否准确，也可以对每个解耦的单元，把原始表格和解耦数据送入LLM，AI校对输出T/F（AI校对已经成为AI领域流行方法）。\n\n召回率指标：这些指标依赖于一个有标注的数据集，标注出了【问题，相关文档】数据对，这需要大量人工精力。能否使用经典的RAG数据集代替而不是手动标注海水养殖数据？\n\n文本相关度：直接将召回的文本对比原问题，计算BLUE等无需数据标注的指标\n\n端到端准确率：基于课题组“鱼虾贝藻”等已经算过清单的数据，对比模型生成清单（问题在于，这样的数据只有个位数，会不会过少？）\n","slug":"随记（1）","published":1,"updated":"2025-10-07T05:49:57.783Z","_id":"cuid9DXnwje6XE0HKV7s7I578","comments":1,"layout":"post","photos":[],"content":"<h2 id=\"海水养殖碳足迹垂直大模型尚未解决的一些问题\"><a href=\"#海水养殖碳足迹垂直大模型尚未解决的一些问题\" class=\"headerlink\" title=\"海水养殖碳足迹垂直大模型尚未解决的一些问题\"></a>海水养殖碳足迹垂直大模型尚未解决的一些问题</h2><ol>\n<li><p>多值问题：当检索到的投入品有多个取值时，如何计算取值（平均，加权平均？）</p>\n</li>\n<li><p>投入品名称问题：当多个检索文档检索到同一类投入品时，往往因为投入品具体名称不同而分成不同类投入品建立清单（如尼龙6，尼龙，尼龙绳），怎么判断投入品应该加入清单？</p>\n</li>\n<li><p>排放因子对应问题：由于投入品名称不统一，难以完全对应排放因子所在的数据库中的投入品名称</p>\n</li>\n<li><p>基于分阶段检索的问题：我们是分阶段检索建立表格，这完全依赖于在数值收集和处理中对数据附带标签，对于没有提及具体阶段的数据，如何处理？</p>\n</li>\n</ol>\n<hr>\n<p><strong>以上问题人类是怎么做的？拿到的数据（文献，现场调研报告，公司年报等）应该仍然有以上问题，人类如何解决？</strong></p>\n<h3 id=\"固定表格以及投入品名称时会造成更多难以接受的问题\"><a href=\"#固定表格以及投入品名称时会造成更多难以接受的问题\" class=\"headerlink\" title=\"固定表格以及投入品名称时会造成更多难以接受的问题\"></a>固定表格以及投入品名称时会造成更多难以接受的问题</h3><ol>\n<li><p>检索困难：当投入品名称固定时，因为名称本身与原始数据中名称不一致造成无法检索到对应数据，以至于无法生成</p>\n</li>\n<li><p>难以应对多种多样的养殖品种和养殖模式：不同的品种或者模式投入品都大不相同，难以使用一张固定的表格全部覆盖，失去泛化性</p>\n</li>\n</ol>\n<h2 id=\"有待量化的一些指标和量化方法\"><a href=\"#有待量化的一些指标和量化方法\" class=\"headerlink\" title=\"有待量化的一些指标和量化方法\"></a>有待量化的一些指标和量化方法</h2><p>因为“复利效应”，即多次小概率失误导致大概率错误，所以每个环节都不得不保证卓越的准确性</p>\n<h4 id=\"量化指标\"><a href=\"#量化指标\" class=\"headerlink\" title=\"量化指标\"></a><strong>量化指标</strong></h4><p>表格解耦准确率：因为大量数据来源图表，我们之前选择了LLM解耦表格的做法，但是如何确定LLM能够完整无误的解耦？</p>\n<p>召回率：RAG经典的评测指标，量化是否完整的检索到了所需的数据</p>\n<p>端到端准确率：量化最终生成的清单准确性，我们的应用场景下目前人工对比为最优方法</p>\n<h4 id=\"量化方法\"><a href=\"#量化方法\" class=\"headerlink\" title=\"量化方法\"></a>量化方法</h4><p>表格解耦率：使用目前收集到的表格数据，进行解耦，采用人工校对加LLM校对的方法。对完整解耦后的表格人工对比每项是否准确，也可以对每个解耦的单元，把原始表格和解耦数据送入LLM，AI校对输出T&#x2F;F（AI校对已经成为AI领域流行方法）。</p>\n<p>召回率指标：这些指标依赖于一个有标注的数据集，标注出了【问题，相关文档】数据对，这需要大量人工精力。能否使用经典的RAG数据集代替而不是手动标注海水养殖数据？</p>\n<p>文本相关度：直接将召回的文本对比原问题，计算BLUE等无需数据标注的指标</p>\n<p>端到端准确率：基于课题组“鱼虾贝藻”等已经算过清单的数据，对比模型生成清单（问题在于，这样的数据只有个位数，会不会过少？）</p>\n","excerpt":"","more":"<h2 id=\"海水养殖碳足迹垂直大模型尚未解决的一些问题\"><a href=\"#海水养殖碳足迹垂直大模型尚未解决的一些问题\" class=\"headerlink\" title=\"海水养殖碳足迹垂直大模型尚未解决的一些问题\"></a>海水养殖碳足迹垂直大模型尚未解决的一些问题</h2><ol>\n<li><p>多值问题：当检索到的投入品有多个取值时，如何计算取值（平均，加权平均？）</p>\n</li>\n<li><p>投入品名称问题：当多个检索文档检索到同一类投入品时，往往因为投入品具体名称不同而分成不同类投入品建立清单（如尼龙6，尼龙，尼龙绳），怎么判断投入品应该加入清单？</p>\n</li>\n<li><p>排放因子对应问题：由于投入品名称不统一，难以完全对应排放因子所在的数据库中的投入品名称</p>\n</li>\n<li><p>基于分阶段检索的问题：我们是分阶段检索建立表格，这完全依赖于在数值收集和处理中对数据附带标签，对于没有提及具体阶段的数据，如何处理？</p>\n</li>\n</ol>\n<hr>\n<p><strong>以上问题人类是怎么做的？拿到的数据（文献，现场调研报告，公司年报等）应该仍然有以上问题，人类如何解决？</strong></p>\n<h3 id=\"固定表格以及投入品名称时会造成更多难以接受的问题\"><a href=\"#固定表格以及投入品名称时会造成更多难以接受的问题\" class=\"headerlink\" title=\"固定表格以及投入品名称时会造成更多难以接受的问题\"></a>固定表格以及投入品名称时会造成更多难以接受的问题</h3><ol>\n<li><p>检索困难：当投入品名称固定时，因为名称本身与原始数据中名称不一致造成无法检索到对应数据，以至于无法生成</p>\n</li>\n<li><p>难以应对多种多样的养殖品种和养殖模式：不同的品种或者模式投入品都大不相同，难以使用一张固定的表格全部覆盖，失去泛化性</p>\n</li>\n</ol>\n<h2 id=\"有待量化的一些指标和量化方法\"><a href=\"#有待量化的一些指标和量化方法\" class=\"headerlink\" title=\"有待量化的一些指标和量化方法\"></a>有待量化的一些指标和量化方法</h2><p>因为“复利效应”，即多次小概率失误导致大概率错误，所以每个环节都不得不保证卓越的准确性</p>\n<h4 id=\"量化指标\"><a href=\"#量化指标\" class=\"headerlink\" title=\"量化指标\"></a><strong>量化指标</strong></h4><p>表格解耦准确率：因为大量数据来源图表，我们之前选择了LLM解耦表格的做法，但是如何确定LLM能够完整无误的解耦？</p>\n<p>召回率：RAG经典的评测指标，量化是否完整的检索到了所需的数据</p>\n<p>端到端准确率：量化最终生成的清单准确性，我们的应用场景下目前人工对比为最优方法</p>\n<h4 id=\"量化方法\"><a href=\"#量化方法\" class=\"headerlink\" title=\"量化方法\"></a>量化方法</h4><p>表格解耦率：使用目前收集到的表格数据，进行解耦，采用人工校对加LLM校对的方法。对完整解耦后的表格人工对比每项是否准确，也可以对每个解耦的单元，把原始表格和解耦数据送入LLM，AI校对输出T&#x2F;F（AI校对已经成为AI领域流行方法）。</p>\n<p>召回率指标：这些指标依赖于一个有标注的数据集，标注出了【问题，相关文档】数据对，这需要大量人工精力。能否使用经典的RAG数据集代替而不是手动标注海水养殖数据？</p>\n<p>文本相关度：直接将召回的文本对比原问题，计算BLUE等无需数据标注的指标</p>\n<p>端到端准确率：基于课题组“鱼虾贝藻”等已经算过清单的数据，对比模型生成清单（问题在于，这样的数据只有个位数，会不会过少？）</p>\n"},{"title":"分层贝叶斯（1）：贝叶斯方法","date":"2025-10-06T08:55:00.000Z","katex":true,"cover":"/img/bys.png","_content":"\n# 🚀 贝叶斯定理：先验、后验与似然\n\n## 引言：当“信念”遇到“数据”\n\n在日常生活中，我们经常根据新的观察来修正自己的看法或信念。**贝叶斯方法（Bayesian Method)** 正是将这种人类的认知过程——**信念的更新**——用严谨的数学形式固定下来。\n\n在机器学习与统计学中，**贝叶斯定理**提供了一个框架，用于**量化不确定性**，并根据观测数据对模型参数进行更新。本文将从最简单的 **朴素贝叶斯（Naive Bayes）** 分类器入手，通过一个直观的**抛硬币实验**，理解贝叶斯公式中的三大核心要素：**先验（Prior）**、**似然（Likelihood）** 与 **后验（Posterior）**。\n\n---\n\n## 一、引子：抛硬币实验与参数  $\\theta$\n\n假设你手中有一枚硬币，我们想知道它**正面朝上的真实概率** $\\theta$ 是多少。这个 $\\theta$ 是我们模型中的**参数（Parameter）**，它是我们希望根据数据推断的 **“真相”**。\n\n### 1. 初始信念：先验分布 $P(\\theta)$\n\n在观测数据 $D$ 之前，我们对 $\\theta$ 的取值已有初始信念，这就是 **先验分布（Prior Distribution）**。\n\n* **定义：** $P(\\theta)$ 描述了观测数据之前，我们对参数 $\\theta$ **所有可能取值的信念与不确定性**。\n* **要求：** **先验必须是一个概率分布**，而非单一数值。\n* **作用：** 代表我们在看到数据之前的主观假设，它会影响数据量小时的推断结果。\n\n### 2. 数据支持度：似然函数 $P(D|\\theta)$\n\n假设我们抛硬币 $N$ 次，得到 $k$ 次正面（数据记为 $D$）。由于抛硬币符合**二项分布**，其似然函数 $P(D|\\theta)$ 为：\n$$P(D|\\theta) = \\binom{N}{k} \\theta^k (1-\\theta)^{N-k}$$\n\n* **定义：** 似然函数 $P(D|\\theta)$ 描述在**给定特定参数 $\\theta$** 的情况下，观察到现有数据 $D$ 的**相对概率**。\n* **核心：** 似然反映了 **“数据对参数 $\\theta$ 的支持度”**。它是一个关于 $\\theta$ 的函数，不要求归一化。\n\n### 3. 更新后的信念：后验分布 $P(\\theta|D)$\n\n结合了先验和似然后，我们更新对 $\\theta$ 的认识，得到**后验分布（Posterior Distribution）**。\n\n后验分布的计算遵循**贝叶斯定理**：\n\n$$P(\\theta|D) = \\frac{P(D|\\theta) P(\\theta)}{P(D)}$$\n\n* **定义：** 后验分布 $P(\\theta|D)$ 描述了在观测数据后，对参数 $\\theta$ 的**更新信念**。\n* **直观意义：** 后验是**先验信息**与**数据证据**的加权融合。\n* **推断简化：** 在参数推断中，我们通常只关注后验的形状，故可使用比例形式：\n\n$$P(\\theta|D) \\propto P(D|\\theta) P(\\theta)$$\n\n---\n\n## 二、贝叶斯定理与朴素贝叶斯\n\n### 1. 证据项 $P(D)$ 的角色\n\n公式分母 $P(D)$ 为**证据（Evidence）或边缘似然（Marginal Likelihood)**，它的定义是一个积分：\n\n$$P(D) = \\int P(D|\\theta) P(\\theta) \\, d\\theta$$\n\n* **功能：** $P(D)$ 确保后验分布积分为 1，实现**归一化**。\n* **推断焦点：** 由于 $P(D)$ 是一个常数，且在复杂模型中难以解析求解，我们在推断参数**形状**时，通常只关注**分子（联合概率密度）**。\n\n### 2. 从参数推断到分类任务\n\n硬币实验说明了贝叶斯推断的思想：**利用数据修正对参数的信念**。在分类问题中，我们推广为：**利用特征数据 $X$ 修正对类别 $C$ 的信念**，从而得到：\n\n$$P(C|X) = \\frac{P(X|C) P(C)}{P(X)}$$\n\n* $P(C|X)$：**后验**分类概率。\n* $P(X|C)$：特征的**似然**（在 $C$ 下生成 $X$ 的概率）。\n* $P(C)$：类别的**先验**概率。\n\n### 3. “朴素”假设：条件独立性\n\n由于特征 $X = \\{x_1, x_2, \\dots, x_n\\}$ 通常维度高且特征间可能相关，直接计算 $P(X|C)$ 非常复杂。\n\n**朴素贝叶斯**通过一个“朴素”的假设来简化计算：**在给定类别 $C$ 的条件下，所有特征间条件独立。**\n\n$$P(X|C) = \\prod_{i=1}^n P(x_i|C)$$\n\n这一独立性假设极大简化了计算，使朴素贝叶斯在文本分类等任务中表现高效。\n\n# 🔄 贝叶斯算法的核心：推断与预测\n\n贝叶斯算法的核心在于一个**学习循环**，它利用新的证据持续更新我们对模型参数的信念。这个过程可以清晰地划分为两大阶段：**参数推断（Inference）和新数据预测（Prediction）**。\n\n---\n\n## 一、推断阶段：参数信念的更新\n\n推断阶段的目标是利用观测到的数据 $D$ 来修正模型参数 $\\theta$ 的不确定性，核心在于计算**后验分布 $P(\\theta|D)$**。\n\n### 1. 基础更新：贝叶斯定理\n\n在接收到一批数据 $D$ 时，我们使用贝叶斯定理进行一次性更新：\n\n$$P(\\theta|D) = \\frac{P(D|\\theta) P(\\theta)}{P(D)}$$\n\n在推断**参数形状**时，我们通常只关注**分子**（似然与先验的联合概率密度 $P(D|\\theta) P(\\theta)$），因为后验分布的形状完全由它决定。\n\n### 2. 连续学习：顺序贝叶斯更新\n\n贝叶斯更新可以进行**迭代**。当我们获得第二批数据 $D'$ 时，上一步的**后验 $P(\\theta|D)$** 直接成为下一步的**先验**：\n\n$$P(\\theta|D, D') = \\frac{P(D'|\\theta) P(\\theta|D)}{P(D')}$$\n\n这种**顺序贝叶斯更新（Sequential Bayesian Updating）** 遵循直觉：每次看到新数据，都用它来修正已有的信念。\n\n**迭代更新过程的抽象表示：**\n\n$$\\text{Prior} \\xrightarrow[\\text{data } D]{\\text{update}} \\text{Posterior} \\xrightarrow[\\text{as new prior}]{\\text{new data } D'} \\text{Posterior}' \\xrightarrow[\\text{as new prior}]{\\text{new data } D''} \\cdots$$\n\n每次新的观测，都会使我们的不确定性逐步减少，后验分布也会越来越集中在真实参数附近。\n\n---\n\n## 二、证据项 $P(D)$：归一化与模型解释\n\n证据项 $P(D)$ 位于贝叶斯定理的分母，被称为**边缘似然（Marginal Likelihood）**。\n\n### 1. 由归一化条件推导积分表达式\n\n由于后验分布必须归一化 ($\\int P(\\theta|D)\\, d\\theta = 1$)，我们可以推导出 $P(D)$ 的积分表达式：\n\n$$P(D) = \\int P(D|\\theta) P(\\theta)\\, d\\theta$$\n\n* **直觉解释：** 数据的概率 $P(D)$ 是对 **“在所有可能参数下生成数据的可能性 $P(D|\\theta)$”** 的加权平均（由先验 $P(\\theta)$ 加权）。\n* **功能：** $P(D)$ 在推断阶段作为**归一化常数**。\n\n### 2. 离散参数的求和形式\n\n当参数 $\\theta$ 是离散变量时，积分变为求和：\n\n$$P(D) = \\sum_{\\theta} P(D|\\theta) P(\\theta)$$\n\n---\n\n## 三、预测阶段：量化新数据的不确定性\n\n预测阶段的目标是利用推断出的**后验分布 $P(\\theta|D)$**，来计算和预测**新数据 $\\tilde{D}$ 的分布 $P(\\tilde{D}|D)$**。\n\n### 预测分布公式\n\n我们通过对所有可能的参数 $\\theta$ 进行积分，将参数的不确定性整合到预测中：\n\n$$P(\\tilde{D}|D) = \\int P(\\tilde{D}|\\theta) P(\\theta|D) \\, d\\theta$$\n\n* **$P(\\tilde{D}|\\theta)$：** 在给定**特定参数 $\\theta$** 下，新数据 $\\tilde{D}$ 出现的概率（预测似然）。\n* **$P(\\theta|D)$：** 对参数 $\\theta$ 的**后验信念**，充当加权函数。\n\n**贝叶斯预测的关键：** 预测分布 $P(\\tilde{D}|D)$ 提供了**完整的概率分布**，这自然地整合了模型参数 $\\theta$ 的**所有不确定性**，从而提供比点估计更稳健的预测区间。\n\n---\n\n## 走向分层贝叶斯\n\n简单的贝叶斯模型假设所有数据点都由**单一的 $\\theta$** 或**独立的 $\\theta_m$** 产生，但当我们面对**多个相似但独立的子问题**（如不同城市、不同样本群体）时，这种独立分析会导致数据稀疏的个体推断不可信。\n\n此时，我们需要引入**分层（Hierarchical）结构**和**超参数（Hyperparameters）**，让参数之间也具有关联性，从而实现**信息共享**。\n\n下一篇：**分层贝叶斯（二）：从参数到分层**。\n","source":"_posts/1）分层贝叶斯：朴素贝叶斯.md","raw":"---\ntitle: 分层贝叶斯（1）：贝叶斯方法\ndate: 2025-10-06 16:55\n\ncategory: 算法\n\nkatex: true\ncover: /img/bys.png\n\n---\n\n# 🚀 贝叶斯定理：先验、后验与似然\n\n## 引言：当“信念”遇到“数据”\n\n在日常生活中，我们经常根据新的观察来修正自己的看法或信念。**贝叶斯方法（Bayesian Method)** 正是将这种人类的认知过程——**信念的更新**——用严谨的数学形式固定下来。\n\n在机器学习与统计学中，**贝叶斯定理**提供了一个框架，用于**量化不确定性**，并根据观测数据对模型参数进行更新。本文将从最简单的 **朴素贝叶斯（Naive Bayes）** 分类器入手，通过一个直观的**抛硬币实验**，理解贝叶斯公式中的三大核心要素：**先验（Prior）**、**似然（Likelihood）** 与 **后验（Posterior）**。\n\n---\n\n## 一、引子：抛硬币实验与参数  $\\theta$\n\n假设你手中有一枚硬币，我们想知道它**正面朝上的真实概率** $\\theta$ 是多少。这个 $\\theta$ 是我们模型中的**参数（Parameter）**，它是我们希望根据数据推断的 **“真相”**。\n\n### 1. 初始信念：先验分布 $P(\\theta)$\n\n在观测数据 $D$ 之前，我们对 $\\theta$ 的取值已有初始信念，这就是 **先验分布（Prior Distribution）**。\n\n* **定义：** $P(\\theta)$ 描述了观测数据之前，我们对参数 $\\theta$ **所有可能取值的信念与不确定性**。\n* **要求：** **先验必须是一个概率分布**，而非单一数值。\n* **作用：** 代表我们在看到数据之前的主观假设，它会影响数据量小时的推断结果。\n\n### 2. 数据支持度：似然函数 $P(D|\\theta)$\n\n假设我们抛硬币 $N$ 次，得到 $k$ 次正面（数据记为 $D$）。由于抛硬币符合**二项分布**，其似然函数 $P(D|\\theta)$ 为：\n$$P(D|\\theta) = \\binom{N}{k} \\theta^k (1-\\theta)^{N-k}$$\n\n* **定义：** 似然函数 $P(D|\\theta)$ 描述在**给定特定参数 $\\theta$** 的情况下，观察到现有数据 $D$ 的**相对概率**。\n* **核心：** 似然反映了 **“数据对参数 $\\theta$ 的支持度”**。它是一个关于 $\\theta$ 的函数，不要求归一化。\n\n### 3. 更新后的信念：后验分布 $P(\\theta|D)$\n\n结合了先验和似然后，我们更新对 $\\theta$ 的认识，得到**后验分布（Posterior Distribution）**。\n\n后验分布的计算遵循**贝叶斯定理**：\n\n$$P(\\theta|D) = \\frac{P(D|\\theta) P(\\theta)}{P(D)}$$\n\n* **定义：** 后验分布 $P(\\theta|D)$ 描述了在观测数据后，对参数 $\\theta$ 的**更新信念**。\n* **直观意义：** 后验是**先验信息**与**数据证据**的加权融合。\n* **推断简化：** 在参数推断中，我们通常只关注后验的形状，故可使用比例形式：\n\n$$P(\\theta|D) \\propto P(D|\\theta) P(\\theta)$$\n\n---\n\n## 二、贝叶斯定理与朴素贝叶斯\n\n### 1. 证据项 $P(D)$ 的角色\n\n公式分母 $P(D)$ 为**证据（Evidence）或边缘似然（Marginal Likelihood)**，它的定义是一个积分：\n\n$$P(D) = \\int P(D|\\theta) P(\\theta) \\, d\\theta$$\n\n* **功能：** $P(D)$ 确保后验分布积分为 1，实现**归一化**。\n* **推断焦点：** 由于 $P(D)$ 是一个常数，且在复杂模型中难以解析求解，我们在推断参数**形状**时，通常只关注**分子（联合概率密度）**。\n\n### 2. 从参数推断到分类任务\n\n硬币实验说明了贝叶斯推断的思想：**利用数据修正对参数的信念**。在分类问题中，我们推广为：**利用特征数据 $X$ 修正对类别 $C$ 的信念**，从而得到：\n\n$$P(C|X) = \\frac{P(X|C) P(C)}{P(X)}$$\n\n* $P(C|X)$：**后验**分类概率。\n* $P(X|C)$：特征的**似然**（在 $C$ 下生成 $X$ 的概率）。\n* $P(C)$：类别的**先验**概率。\n\n### 3. “朴素”假设：条件独立性\n\n由于特征 $X = \\{x_1, x_2, \\dots, x_n\\}$ 通常维度高且特征间可能相关，直接计算 $P(X|C)$ 非常复杂。\n\n**朴素贝叶斯**通过一个“朴素”的假设来简化计算：**在给定类别 $C$ 的条件下，所有特征间条件独立。**\n\n$$P(X|C) = \\prod_{i=1}^n P(x_i|C)$$\n\n这一独立性假设极大简化了计算，使朴素贝叶斯在文本分类等任务中表现高效。\n\n# 🔄 贝叶斯算法的核心：推断与预测\n\n贝叶斯算法的核心在于一个**学习循环**，它利用新的证据持续更新我们对模型参数的信念。这个过程可以清晰地划分为两大阶段：**参数推断（Inference）和新数据预测（Prediction）**。\n\n---\n\n## 一、推断阶段：参数信念的更新\n\n推断阶段的目标是利用观测到的数据 $D$ 来修正模型参数 $\\theta$ 的不确定性，核心在于计算**后验分布 $P(\\theta|D)$**。\n\n### 1. 基础更新：贝叶斯定理\n\n在接收到一批数据 $D$ 时，我们使用贝叶斯定理进行一次性更新：\n\n$$P(\\theta|D) = \\frac{P(D|\\theta) P(\\theta)}{P(D)}$$\n\n在推断**参数形状**时，我们通常只关注**分子**（似然与先验的联合概率密度 $P(D|\\theta) P(\\theta)$），因为后验分布的形状完全由它决定。\n\n### 2. 连续学习：顺序贝叶斯更新\n\n贝叶斯更新可以进行**迭代**。当我们获得第二批数据 $D'$ 时，上一步的**后验 $P(\\theta|D)$** 直接成为下一步的**先验**：\n\n$$P(\\theta|D, D') = \\frac{P(D'|\\theta) P(\\theta|D)}{P(D')}$$\n\n这种**顺序贝叶斯更新（Sequential Bayesian Updating）** 遵循直觉：每次看到新数据，都用它来修正已有的信念。\n\n**迭代更新过程的抽象表示：**\n\n$$\\text{Prior} \\xrightarrow[\\text{data } D]{\\text{update}} \\text{Posterior} \\xrightarrow[\\text{as new prior}]{\\text{new data } D'} \\text{Posterior}' \\xrightarrow[\\text{as new prior}]{\\text{new data } D''} \\cdots$$\n\n每次新的观测，都会使我们的不确定性逐步减少，后验分布也会越来越集中在真实参数附近。\n\n---\n\n## 二、证据项 $P(D)$：归一化与模型解释\n\n证据项 $P(D)$ 位于贝叶斯定理的分母，被称为**边缘似然（Marginal Likelihood）**。\n\n### 1. 由归一化条件推导积分表达式\n\n由于后验分布必须归一化 ($\\int P(\\theta|D)\\, d\\theta = 1$)，我们可以推导出 $P(D)$ 的积分表达式：\n\n$$P(D) = \\int P(D|\\theta) P(\\theta)\\, d\\theta$$\n\n* **直觉解释：** 数据的概率 $P(D)$ 是对 **“在所有可能参数下生成数据的可能性 $P(D|\\theta)$”** 的加权平均（由先验 $P(\\theta)$ 加权）。\n* **功能：** $P(D)$ 在推断阶段作为**归一化常数**。\n\n### 2. 离散参数的求和形式\n\n当参数 $\\theta$ 是离散变量时，积分变为求和：\n\n$$P(D) = \\sum_{\\theta} P(D|\\theta) P(\\theta)$$\n\n---\n\n## 三、预测阶段：量化新数据的不确定性\n\n预测阶段的目标是利用推断出的**后验分布 $P(\\theta|D)$**，来计算和预测**新数据 $\\tilde{D}$ 的分布 $P(\\tilde{D}|D)$**。\n\n### 预测分布公式\n\n我们通过对所有可能的参数 $\\theta$ 进行积分，将参数的不确定性整合到预测中：\n\n$$P(\\tilde{D}|D) = \\int P(\\tilde{D}|\\theta) P(\\theta|D) \\, d\\theta$$\n\n* **$P(\\tilde{D}|\\theta)$：** 在给定**特定参数 $\\theta$** 下，新数据 $\\tilde{D}$ 出现的概率（预测似然）。\n* **$P(\\theta|D)$：** 对参数 $\\theta$ 的**后验信念**，充当加权函数。\n\n**贝叶斯预测的关键：** 预测分布 $P(\\tilde{D}|D)$ 提供了**完整的概率分布**，这自然地整合了模型参数 $\\theta$ 的**所有不确定性**，从而提供比点估计更稳健的预测区间。\n\n---\n\n## 走向分层贝叶斯\n\n简单的贝叶斯模型假设所有数据点都由**单一的 $\\theta$** 或**独立的 $\\theta_m$** 产生，但当我们面对**多个相似但独立的子问题**（如不同城市、不同样本群体）时，这种独立分析会导致数据稀疏的个体推断不可信。\n\n此时，我们需要引入**分层（Hierarchical）结构**和**超参数（Hyperparameters）**，让参数之间也具有关联性，从而实现**信息共享**。\n\n下一篇：**分层贝叶斯（二）：从参数到分层**。\n","slug":"1）分层贝叶斯：朴素贝叶斯","published":1,"updated":"2025-10-07T05:50:59.216Z","_id":"cuidwUe8hGfUsuJ1gxvO-9axr","comments":1,"layout":"post","photos":[],"content":"<h1 id=\"🚀-贝叶斯定理：先验、后验与似然\"><a href=\"#🚀-贝叶斯定理：先验、后验与似然\" class=\"headerlink\" title=\"🚀 贝叶斯定理：先验、后验与似然\"></a>🚀 贝叶斯定理：先验、后验与似然</h1><h2 id=\"引言：当“信念”遇到“数据”\"><a href=\"#引言：当“信念”遇到“数据”\" class=\"headerlink\" title=\"引言：当“信念”遇到“数据”\"></a>引言：当“信念”遇到“数据”</h2><p>在日常生活中，我们经常根据新的观察来修正自己的看法或信念。<strong>贝叶斯方法（Bayesian Method)</strong> 正是将这种人类的认知过程——<strong>信念的更新</strong>——用严谨的数学形式固定下来。</p>\n<p>在机器学习与统计学中，<strong>贝叶斯定理</strong>提供了一个框架，用于<strong>量化不确定性</strong>，并根据观测数据对模型参数进行更新。本文将从最简单的 <strong>朴素贝叶斯（Naive Bayes）</strong> 分类器入手，通过一个直观的<strong>抛硬币实验</strong>，理解贝叶斯公式中的三大核心要素：<strong>先验（Prior）</strong>、<strong>似然（Likelihood）</strong> 与 <strong>后验（Posterior）</strong>。</p>\n<hr>\n<h2 id=\"一、引子：抛硬币实验与参数-theta\"><a href=\"#一、引子：抛硬币实验与参数-theta\" class=\"headerlink\" title=\"一、引子：抛硬币实验与参数  $\\theta$\"></a>一、引子：抛硬币实验与参数  $\\theta$</h2><p>假设你手中有一枚硬币，我们想知道它<strong>正面朝上的真实概率</strong> $\\theta$ 是多少。这个 $\\theta$ 是我们模型中的<strong>参数（Parameter）</strong>，它是我们希望根据数据推断的 <strong>“真相”</strong>。</p>\n<h3 id=\"1-初始信念：先验分布-P-theta\"><a href=\"#1-初始信念：先验分布-P-theta\" class=\"headerlink\" title=\"1. 初始信念：先验分布 $P(\\theta)$\"></a>1. 初始信念：先验分布 $P(\\theta)$</h3><p>在观测数据 $D$ 之前，我们对 $\\theta$ 的取值已有初始信念，这就是 <strong>先验分布（Prior Distribution）</strong>。</p>\n<ul>\n<li><strong>定义：</strong> $P(\\theta)$ 描述了观测数据之前，我们对参数 $\\theta$ <strong>所有可能取值的信念与不确定性</strong>。</li>\n<li><strong>要求：</strong> <strong>先验必须是一个概率分布</strong>，而非单一数值。</li>\n<li><strong>作用：</strong> 代表我们在看到数据之前的主观假设，它会影响数据量小时的推断结果。</li>\n</ul>\n<h3 id=\"2-数据支持度：似然函数-P-D-theta\"><a href=\"#2-数据支持度：似然函数-P-D-theta\" class=\"headerlink\" title=\"2. 数据支持度：似然函数 $P(D|\\theta)$\"></a>2. 数据支持度：似然函数 $P(D|\\theta)$</h3><p>假设我们抛硬币 $N$ 次，得到 $k$ 次正面（数据记为 $D$）。由于抛硬币符合<strong>二项分布</strong>，其似然函数 $P(D|\\theta)$ 为：<br>$$P(D|\\theta) &#x3D; \\binom{N}{k} \\theta^k (1-\\theta)^{N-k}$$</p>\n<ul>\n<li><strong>定义：</strong> 似然函数 $P(D|\\theta)$ 描述在<strong>给定特定参数 $\\theta$</strong> 的情况下，观察到现有数据 $D$ 的<strong>相对概率</strong>。</li>\n<li><strong>核心：</strong> 似然反映了 <strong>“数据对参数 $\\theta$ 的支持度”</strong>。它是一个关于 $\\theta$ 的函数，不要求归一化。</li>\n</ul>\n<h3 id=\"3-更新后的信念：后验分布-P-theta-D\"><a href=\"#3-更新后的信念：后验分布-P-theta-D\" class=\"headerlink\" title=\"3. 更新后的信念：后验分布 $P(\\theta|D)$\"></a>3. 更新后的信念：后验分布 $P(\\theta|D)$</h3><p>结合了先验和似然后，我们更新对 $\\theta$ 的认识，得到<strong>后验分布（Posterior Distribution）</strong>。</p>\n<p>后验分布的计算遵循<strong>贝叶斯定理</strong>：</p>\n<p>$$P(\\theta|D) &#x3D; \\frac{P(D|\\theta) P(\\theta)}{P(D)}$$</p>\n<ul>\n<li><strong>定义：</strong> 后验分布 $P(\\theta|D)$ 描述了在观测数据后，对参数 $\\theta$ 的<strong>更新信念</strong>。</li>\n<li><strong>直观意义：</strong> 后验是<strong>先验信息</strong>与<strong>数据证据</strong>的加权融合。</li>\n<li><strong>推断简化：</strong> 在参数推断中，我们通常只关注后验的形状，故可使用比例形式：</li>\n</ul>\n<p>$$P(\\theta|D) \\propto P(D|\\theta) P(\\theta)$$</p>\n<hr>\n<h2 id=\"二、贝叶斯定理与朴素贝叶斯\"><a href=\"#二、贝叶斯定理与朴素贝叶斯\" class=\"headerlink\" title=\"二、贝叶斯定理与朴素贝叶斯\"></a>二、贝叶斯定理与朴素贝叶斯</h2><h3 id=\"1-证据项-P-D-的角色\"><a href=\"#1-证据项-P-D-的角色\" class=\"headerlink\" title=\"1. 证据项 $P(D)$ 的角色\"></a>1. 证据项 $P(D)$ 的角色</h3><p>公式分母 $P(D)$ 为<strong>证据（Evidence）或边缘似然（Marginal Likelihood)</strong>，它的定义是一个积分：</p>\n<p>$$P(D) &#x3D; \\int P(D|\\theta) P(\\theta) , d\\theta$$</p>\n<ul>\n<li><strong>功能：</strong> $P(D)$ 确保后验分布积分为 1，实现<strong>归一化</strong>。</li>\n<li><strong>推断焦点：</strong> 由于 $P(D)$ 是一个常数，且在复杂模型中难以解析求解，我们在推断参数<strong>形状</strong>时，通常只关注<strong>分子（联合概率密度）</strong>。</li>\n</ul>\n<h3 id=\"2-从参数推断到分类任务\"><a href=\"#2-从参数推断到分类任务\" class=\"headerlink\" title=\"2. 从参数推断到分类任务\"></a>2. 从参数推断到分类任务</h3><p>硬币实验说明了贝叶斯推断的思想：<strong>利用数据修正对参数的信念</strong>。在分类问题中，我们推广为：<strong>利用特征数据 $X$ 修正对类别 $C$ 的信念</strong>，从而得到：</p>\n<p>$$P(C|X) &#x3D; \\frac{P(X|C) P(C)}{P(X)}$$</p>\n<ul>\n<li>$P(C|X)$：<strong>后验</strong>分类概率。</li>\n<li>$P(X|C)$：特征的<strong>似然</strong>（在 $C$ 下生成 $X$ 的概率）。</li>\n<li>$P(C)$：类别的<strong>先验</strong>概率。</li>\n</ul>\n<h3 id=\"3-“朴素”假设：条件独立性\"><a href=\"#3-“朴素”假设：条件独立性\" class=\"headerlink\" title=\"3. “朴素”假设：条件独立性\"></a>3. “朴素”假设：条件独立性</h3><p>由于特征 $X &#x3D; {x_1, x_2, \\dots, x_n}$ 通常维度高且特征间可能相关，直接计算 $P(X|C)$ 非常复杂。</p>\n<p><strong>朴素贝叶斯</strong>通过一个“朴素”的假设来简化计算：<strong>在给定类别 $C$ 的条件下，所有特征间条件独立。</strong></p>\n<p>$$P(X|C) &#x3D; \\prod_{i&#x3D;1}^n P(x_i|C)$$</p>\n<p>这一独立性假设极大简化了计算，使朴素贝叶斯在文本分类等任务中表现高效。</p>\n<h1 id=\"🔄-贝叶斯算法的核心：推断与预测\"><a href=\"#🔄-贝叶斯算法的核心：推断与预测\" class=\"headerlink\" title=\"🔄 贝叶斯算法的核心：推断与预测\"></a>🔄 贝叶斯算法的核心：推断与预测</h1><p>贝叶斯算法的核心在于一个<strong>学习循环</strong>，它利用新的证据持续更新我们对模型参数的信念。这个过程可以清晰地划分为两大阶段：<strong>参数推断（Inference）和新数据预测（Prediction）</strong>。</p>\n<hr>\n<h2 id=\"一、推断阶段：参数信念的更新\"><a href=\"#一、推断阶段：参数信念的更新\" class=\"headerlink\" title=\"一、推断阶段：参数信念的更新\"></a>一、推断阶段：参数信念的更新</h2><p>推断阶段的目标是利用观测到的数据 $D$ 来修正模型参数 $\\theta$ 的不确定性，核心在于计算<strong>后验分布 $P(\\theta|D)$</strong>。</p>\n<h3 id=\"1-基础更新：贝叶斯定理\"><a href=\"#1-基础更新：贝叶斯定理\" class=\"headerlink\" title=\"1. 基础更新：贝叶斯定理\"></a>1. 基础更新：贝叶斯定理</h3><p>在接收到一批数据 $D$ 时，我们使用贝叶斯定理进行一次性更新：</p>\n<p>$$P(\\theta|D) &#x3D; \\frac{P(D|\\theta) P(\\theta)}{P(D)}$$</p>\n<p>在推断<strong>参数形状</strong>时，我们通常只关注<strong>分子</strong>（似然与先验的联合概率密度 $P(D|\\theta) P(\\theta)$），因为后验分布的形状完全由它决定。</p>\n<h3 id=\"2-连续学习：顺序贝叶斯更新\"><a href=\"#2-连续学习：顺序贝叶斯更新\" class=\"headerlink\" title=\"2. 连续学习：顺序贝叶斯更新\"></a>2. 连续学习：顺序贝叶斯更新</h3><p>贝叶斯更新可以进行<strong>迭代</strong>。当我们获得第二批数据 $D’$ 时，上一步的<strong>后验 $P(\\theta|D)$</strong> 直接成为下一步的<strong>先验</strong>：</p>\n<p>$$P(\\theta|D, D’) &#x3D; \\frac{P(D’|\\theta) P(\\theta|D)}{P(D’)}$$</p>\n<p>这种<strong>顺序贝叶斯更新（Sequential Bayesian Updating）</strong> 遵循直觉：每次看到新数据，都用它来修正已有的信念。</p>\n<p><strong>迭代更新过程的抽象表示：</strong></p>\n<p>$$\\text{Prior} \\xrightarrow[\\text{data } D]{\\text{update}} \\text{Posterior} \\xrightarrow[\\text{as new prior}]{\\text{new data } D’} \\text{Posterior}’ \\xrightarrow[\\text{as new prior}]{\\text{new data } D’’} \\cdots$$</p>\n<p>每次新的观测，都会使我们的不确定性逐步减少，后验分布也会越来越集中在真实参数附近。</p>\n<hr>\n<h2 id=\"二、证据项-P-D-：归一化与模型解释\"><a href=\"#二、证据项-P-D-：归一化与模型解释\" class=\"headerlink\" title=\"二、证据项 $P(D)$：归一化与模型解释\"></a>二、证据项 $P(D)$：归一化与模型解释</h2><p>证据项 $P(D)$ 位于贝叶斯定理的分母，被称为<strong>边缘似然（Marginal Likelihood）</strong>。</p>\n<h3 id=\"1-由归一化条件推导积分表达式\"><a href=\"#1-由归一化条件推导积分表达式\" class=\"headerlink\" title=\"1. 由归一化条件推导积分表达式\"></a>1. 由归一化条件推导积分表达式</h3><p>由于后验分布必须归一化 ($\\int P(\\theta|D), d\\theta &#x3D; 1$)，我们可以推导出 $P(D)$ 的积分表达式：</p>\n<p>$$P(D) &#x3D; \\int P(D|\\theta) P(\\theta), d\\theta$$</p>\n<ul>\n<li><strong>直觉解释：</strong> 数据的概率 $P(D)$ 是对 <strong>“在所有可能参数下生成数据的可能性 $P(D|\\theta)$”</strong> 的加权平均（由先验 $P(\\theta)$ 加权）。</li>\n<li><strong>功能：</strong> $P(D)$ 在推断阶段作为<strong>归一化常数</strong>。</li>\n</ul>\n<h3 id=\"2-离散参数的求和形式\"><a href=\"#2-离散参数的求和形式\" class=\"headerlink\" title=\"2. 离散参数的求和形式\"></a>2. 离散参数的求和形式</h3><p>当参数 $\\theta$ 是离散变量时，积分变为求和：</p>\n<p>$$P(D) &#x3D; \\sum_{\\theta} P(D|\\theta) P(\\theta)$$</p>\n<hr>\n<h2 id=\"三、预测阶段：量化新数据的不确定性\"><a href=\"#三、预测阶段：量化新数据的不确定性\" class=\"headerlink\" title=\"三、预测阶段：量化新数据的不确定性\"></a>三、预测阶段：量化新数据的不确定性</h2><p>预测阶段的目标是利用推断出的<strong>后验分布 $P(\\theta|D)$</strong>，来计算和预测<strong>新数据 $\\tilde{D}$ 的分布 $P(\\tilde{D}|D)$</strong>。</p>\n<h3 id=\"预测分布公式\"><a href=\"#预测分布公式\" class=\"headerlink\" title=\"预测分布公式\"></a>预测分布公式</h3><p>我们通过对所有可能的参数 $\\theta$ 进行积分，将参数的不确定性整合到预测中：</p>\n<p>$$P(\\tilde{D}|D) &#x3D; \\int P(\\tilde{D}|\\theta) P(\\theta|D) , d\\theta$$</p>\n<ul>\n<li><strong>$P(\\tilde{D}|\\theta)$：</strong> 在给定<strong>特定参数 $\\theta$</strong> 下，新数据 $\\tilde{D}$ 出现的概率（预测似然）。</li>\n<li><strong>$P(\\theta|D)$：</strong> 对参数 $\\theta$ 的<strong>后验信念</strong>，充当加权函数。</li>\n</ul>\n<p><strong>贝叶斯预测的关键：</strong> 预测分布 $P(\\tilde{D}|D)$ 提供了<strong>完整的概率分布</strong>，这自然地整合了模型参数 $\\theta$ 的<strong>所有不确定性</strong>，从而提供比点估计更稳健的预测区间。</p>\n<hr>\n<h2 id=\"走向分层贝叶斯\"><a href=\"#走向分层贝叶斯\" class=\"headerlink\" title=\"走向分层贝叶斯\"></a>走向分层贝叶斯</h2><p>简单的贝叶斯模型假设所有数据点都由<strong>单一的 $\\theta$</strong> 或<strong>独立的 $\\theta_m$</strong> 产生，但当我们面对<strong>多个相似但独立的子问题</strong>（如不同城市、不同样本群体）时，这种独立分析会导致数据稀疏的个体推断不可信。</p>\n<p>此时，我们需要引入<strong>分层（Hierarchical）结构</strong>和<strong>超参数（Hyperparameters）</strong>，让参数之间也具有关联性，从而实现<strong>信息共享</strong>。</p>\n<p>下一篇：<strong>分层贝叶斯（二）：从参数到分层</strong>。</p>\n","excerpt":"","more":"<h1 id=\"🚀-贝叶斯定理：先验、后验与似然\"><a href=\"#🚀-贝叶斯定理：先验、后验与似然\" class=\"headerlink\" title=\"🚀 贝叶斯定理：先验、后验与似然\"></a>🚀 贝叶斯定理：先验、后验与似然</h1><h2 id=\"引言：当“信念”遇到“数据”\"><a href=\"#引言：当“信念”遇到“数据”\" class=\"headerlink\" title=\"引言：当“信念”遇到“数据”\"></a>引言：当“信念”遇到“数据”</h2><p>在日常生活中，我们经常根据新的观察来修正自己的看法或信念。<strong>贝叶斯方法（Bayesian Method)</strong> 正是将这种人类的认知过程——<strong>信念的更新</strong>——用严谨的数学形式固定下来。</p>\n<p>在机器学习与统计学中，<strong>贝叶斯定理</strong>提供了一个框架，用于<strong>量化不确定性</strong>，并根据观测数据对模型参数进行更新。本文将从最简单的 <strong>朴素贝叶斯（Naive Bayes）</strong> 分类器入手，通过一个直观的<strong>抛硬币实验</strong>，理解贝叶斯公式中的三大核心要素：<strong>先验（Prior）</strong>、<strong>似然（Likelihood）</strong> 与 <strong>后验（Posterior）</strong>。</p>\n<hr>\n<h2 id=\"一、引子：抛硬币实验与参数-theta\"><a href=\"#一、引子：抛硬币实验与参数-theta\" class=\"headerlink\" title=\"一、引子：抛硬币实验与参数  $\\theta$\"></a>一、引子：抛硬币实验与参数  $\\theta$</h2><p>假设你手中有一枚硬币，我们想知道它<strong>正面朝上的真实概率</strong> $\\theta$ 是多少。这个 $\\theta$ 是我们模型中的<strong>参数（Parameter）</strong>，它是我们希望根据数据推断的 <strong>“真相”</strong>。</p>\n<h3 id=\"1-初始信念：先验分布-P-theta\"><a href=\"#1-初始信念：先验分布-P-theta\" class=\"headerlink\" title=\"1. 初始信念：先验分布 $P(\\theta)$\"></a>1. 初始信念：先验分布 $P(\\theta)$</h3><p>在观测数据 $D$ 之前，我们对 $\\theta$ 的取值已有初始信念，这就是 <strong>先验分布（Prior Distribution）</strong>。</p>\n<ul>\n<li><strong>定义：</strong> $P(\\theta)$ 描述了观测数据之前，我们对参数 $\\theta$ <strong>所有可能取值的信念与不确定性</strong>。</li>\n<li><strong>要求：</strong> <strong>先验必须是一个概率分布</strong>，而非单一数值。</li>\n<li><strong>作用：</strong> 代表我们在看到数据之前的主观假设，它会影响数据量小时的推断结果。</li>\n</ul>\n<h3 id=\"2-数据支持度：似然函数-P-D-theta\"><a href=\"#2-数据支持度：似然函数-P-D-theta\" class=\"headerlink\" title=\"2. 数据支持度：似然函数 $P(D|\\theta)$\"></a>2. 数据支持度：似然函数 $P(D|\\theta)$</h3><p>假设我们抛硬币 $N$ 次，得到 $k$ 次正面（数据记为 $D$）。由于抛硬币符合<strong>二项分布</strong>，其似然函数 $P(D|\\theta)$ 为：<br>$$P(D|\\theta) &#x3D; \\binom{N}{k} \\theta^k (1-\\theta)^{N-k}$$</p>\n<ul>\n<li><strong>定义：</strong> 似然函数 $P(D|\\theta)$ 描述在<strong>给定特定参数 $\\theta$</strong> 的情况下，观察到现有数据 $D$ 的<strong>相对概率</strong>。</li>\n<li><strong>核心：</strong> 似然反映了 <strong>“数据对参数 $\\theta$ 的支持度”</strong>。它是一个关于 $\\theta$ 的函数，不要求归一化。</li>\n</ul>\n<h3 id=\"3-更新后的信念：后验分布-P-theta-D\"><a href=\"#3-更新后的信念：后验分布-P-theta-D\" class=\"headerlink\" title=\"3. 更新后的信念：后验分布 $P(\\theta|D)$\"></a>3. 更新后的信念：后验分布 $P(\\theta|D)$</h3><p>结合了先验和似然后，我们更新对 $\\theta$ 的认识，得到<strong>后验分布（Posterior Distribution）</strong>。</p>\n<p>后验分布的计算遵循<strong>贝叶斯定理</strong>：</p>\n<p>$$P(\\theta|D) &#x3D; \\frac{P(D|\\theta) P(\\theta)}{P(D)}$$</p>\n<ul>\n<li><strong>定义：</strong> 后验分布 $P(\\theta|D)$ 描述了在观测数据后，对参数 $\\theta$ 的<strong>更新信念</strong>。</li>\n<li><strong>直观意义：</strong> 后验是<strong>先验信息</strong>与<strong>数据证据</strong>的加权融合。</li>\n<li><strong>推断简化：</strong> 在参数推断中，我们通常只关注后验的形状，故可使用比例形式：</li>\n</ul>\n<p>$$P(\\theta|D) \\propto P(D|\\theta) P(\\theta)$$</p>\n<hr>\n<h2 id=\"二、贝叶斯定理与朴素贝叶斯\"><a href=\"#二、贝叶斯定理与朴素贝叶斯\" class=\"headerlink\" title=\"二、贝叶斯定理与朴素贝叶斯\"></a>二、贝叶斯定理与朴素贝叶斯</h2><h3 id=\"1-证据项-P-D-的角色\"><a href=\"#1-证据项-P-D-的角色\" class=\"headerlink\" title=\"1. 证据项 $P(D)$ 的角色\"></a>1. 证据项 $P(D)$ 的角色</h3><p>公式分母 $P(D)$ 为<strong>证据（Evidence）或边缘似然（Marginal Likelihood)</strong>，它的定义是一个积分：</p>\n<p>$$P(D) &#x3D; \\int P(D|\\theta) P(\\theta) , d\\theta$$</p>\n<ul>\n<li><strong>功能：</strong> $P(D)$ 确保后验分布积分为 1，实现<strong>归一化</strong>。</li>\n<li><strong>推断焦点：</strong> 由于 $P(D)$ 是一个常数，且在复杂模型中难以解析求解，我们在推断参数<strong>形状</strong>时，通常只关注<strong>分子（联合概率密度）</strong>。</li>\n</ul>\n<h3 id=\"2-从参数推断到分类任务\"><a href=\"#2-从参数推断到分类任务\" class=\"headerlink\" title=\"2. 从参数推断到分类任务\"></a>2. 从参数推断到分类任务</h3><p>硬币实验说明了贝叶斯推断的思想：<strong>利用数据修正对参数的信念</strong>。在分类问题中，我们推广为：<strong>利用特征数据 $X$ 修正对类别 $C$ 的信念</strong>，从而得到：</p>\n<p>$$P(C|X) &#x3D; \\frac{P(X|C) P(C)}{P(X)}$$</p>\n<ul>\n<li>$P(C|X)$：<strong>后验</strong>分类概率。</li>\n<li>$P(X|C)$：特征的<strong>似然</strong>（在 $C$ 下生成 $X$ 的概率）。</li>\n<li>$P(C)$：类别的<strong>先验</strong>概率。</li>\n</ul>\n<h3 id=\"3-“朴素”假设：条件独立性\"><a href=\"#3-“朴素”假设：条件独立性\" class=\"headerlink\" title=\"3. “朴素”假设：条件独立性\"></a>3. “朴素”假设：条件独立性</h3><p>由于特征 $X &#x3D; {x_1, x_2, \\dots, x_n}$ 通常维度高且特征间可能相关，直接计算 $P(X|C)$ 非常复杂。</p>\n<p><strong>朴素贝叶斯</strong>通过一个“朴素”的假设来简化计算：<strong>在给定类别 $C$ 的条件下，所有特征间条件独立。</strong></p>\n<p>$$P(X|C) &#x3D; \\prod_{i&#x3D;1}^n P(x_i|C)$$</p>\n<p>这一独立性假设极大简化了计算，使朴素贝叶斯在文本分类等任务中表现高效。</p>\n<h1 id=\"🔄-贝叶斯算法的核心：推断与预测\"><a href=\"#🔄-贝叶斯算法的核心：推断与预测\" class=\"headerlink\" title=\"🔄 贝叶斯算法的核心：推断与预测\"></a>🔄 贝叶斯算法的核心：推断与预测</h1><p>贝叶斯算法的核心在于一个<strong>学习循环</strong>，它利用新的证据持续更新我们对模型参数的信念。这个过程可以清晰地划分为两大阶段：<strong>参数推断（Inference）和新数据预测（Prediction）</strong>。</p>\n<hr>\n<h2 id=\"一、推断阶段：参数信念的更新\"><a href=\"#一、推断阶段：参数信念的更新\" class=\"headerlink\" title=\"一、推断阶段：参数信念的更新\"></a>一、推断阶段：参数信念的更新</h2><p>推断阶段的目标是利用观测到的数据 $D$ 来修正模型参数 $\\theta$ 的不确定性，核心在于计算<strong>后验分布 $P(\\theta|D)$</strong>。</p>\n<h3 id=\"1-基础更新：贝叶斯定理\"><a href=\"#1-基础更新：贝叶斯定理\" class=\"headerlink\" title=\"1. 基础更新：贝叶斯定理\"></a>1. 基础更新：贝叶斯定理</h3><p>在接收到一批数据 $D$ 时，我们使用贝叶斯定理进行一次性更新：</p>\n<p>$$P(\\theta|D) &#x3D; \\frac{P(D|\\theta) P(\\theta)}{P(D)}$$</p>\n<p>在推断<strong>参数形状</strong>时，我们通常只关注<strong>分子</strong>（似然与先验的联合概率密度 $P(D|\\theta) P(\\theta)$），因为后验分布的形状完全由它决定。</p>\n<h3 id=\"2-连续学习：顺序贝叶斯更新\"><a href=\"#2-连续学习：顺序贝叶斯更新\" class=\"headerlink\" title=\"2. 连续学习：顺序贝叶斯更新\"></a>2. 连续学习：顺序贝叶斯更新</h3><p>贝叶斯更新可以进行<strong>迭代</strong>。当我们获得第二批数据 $D’$ 时，上一步的<strong>后验 $P(\\theta|D)$</strong> 直接成为下一步的<strong>先验</strong>：</p>\n<p>$$P(\\theta|D, D’) &#x3D; \\frac{P(D’|\\theta) P(\\theta|D)}{P(D’)}$$</p>\n<p>这种<strong>顺序贝叶斯更新（Sequential Bayesian Updating）</strong> 遵循直觉：每次看到新数据，都用它来修正已有的信念。</p>\n<p><strong>迭代更新过程的抽象表示：</strong></p>\n<p>$$\\text{Prior} \\xrightarrow[\\text{data } D]{\\text{update}} \\text{Posterior} \\xrightarrow[\\text{as new prior}]{\\text{new data } D’} \\text{Posterior}’ \\xrightarrow[\\text{as new prior}]{\\text{new data } D’’} \\cdots$$</p>\n<p>每次新的观测，都会使我们的不确定性逐步减少，后验分布也会越来越集中在真实参数附近。</p>\n<hr>\n<h2 id=\"二、证据项-P-D-：归一化与模型解释\"><a href=\"#二、证据项-P-D-：归一化与模型解释\" class=\"headerlink\" title=\"二、证据项 $P(D)$：归一化与模型解释\"></a>二、证据项 $P(D)$：归一化与模型解释</h2><p>证据项 $P(D)$ 位于贝叶斯定理的分母，被称为<strong>边缘似然（Marginal Likelihood）</strong>。</p>\n<h3 id=\"1-由归一化条件推导积分表达式\"><a href=\"#1-由归一化条件推导积分表达式\" class=\"headerlink\" title=\"1. 由归一化条件推导积分表达式\"></a>1. 由归一化条件推导积分表达式</h3><p>由于后验分布必须归一化 ($\\int P(\\theta|D), d\\theta &#x3D; 1$)，我们可以推导出 $P(D)$ 的积分表达式：</p>\n<p>$$P(D) &#x3D; \\int P(D|\\theta) P(\\theta), d\\theta$$</p>\n<ul>\n<li><strong>直觉解释：</strong> 数据的概率 $P(D)$ 是对 <strong>“在所有可能参数下生成数据的可能性 $P(D|\\theta)$”</strong> 的加权平均（由先验 $P(\\theta)$ 加权）。</li>\n<li><strong>功能：</strong> $P(D)$ 在推断阶段作为<strong>归一化常数</strong>。</li>\n</ul>\n<h3 id=\"2-离散参数的求和形式\"><a href=\"#2-离散参数的求和形式\" class=\"headerlink\" title=\"2. 离散参数的求和形式\"></a>2. 离散参数的求和形式</h3><p>当参数 $\\theta$ 是离散变量时，积分变为求和：</p>\n<p>$$P(D) &#x3D; \\sum_{\\theta} P(D|\\theta) P(\\theta)$$</p>\n<hr>\n<h2 id=\"三、预测阶段：量化新数据的不确定性\"><a href=\"#三、预测阶段：量化新数据的不确定性\" class=\"headerlink\" title=\"三、预测阶段：量化新数据的不确定性\"></a>三、预测阶段：量化新数据的不确定性</h2><p>预测阶段的目标是利用推断出的<strong>后验分布 $P(\\theta|D)$</strong>，来计算和预测<strong>新数据 $\\tilde{D}$ 的分布 $P(\\tilde{D}|D)$</strong>。</p>\n<h3 id=\"预测分布公式\"><a href=\"#预测分布公式\" class=\"headerlink\" title=\"预测分布公式\"></a>预测分布公式</h3><p>我们通过对所有可能的参数 $\\theta$ 进行积分，将参数的不确定性整合到预测中：</p>\n<p>$$P(\\tilde{D}|D) &#x3D; \\int P(\\tilde{D}|\\theta) P(\\theta|D) , d\\theta$$</p>\n<ul>\n<li><strong>$P(\\tilde{D}|\\theta)$：</strong> 在给定<strong>特定参数 $\\theta$</strong> 下，新数据 $\\tilde{D}$ 出现的概率（预测似然）。</li>\n<li><strong>$P(\\theta|D)$：</strong> 对参数 $\\theta$ 的<strong>后验信念</strong>，充当加权函数。</li>\n</ul>\n<p><strong>贝叶斯预测的关键：</strong> 预测分布 $P(\\tilde{D}|D)$ 提供了<strong>完整的概率分布</strong>，这自然地整合了模型参数 $\\theta$ 的<strong>所有不确定性</strong>，从而提供比点估计更稳健的预测区间。</p>\n<hr>\n<h2 id=\"走向分层贝叶斯\"><a href=\"#走向分层贝叶斯\" class=\"headerlink\" title=\"走向分层贝叶斯\"></a>走向分层贝叶斯</h2><p>简单的贝叶斯模型假设所有数据点都由<strong>单一的 $\\theta$</strong> 或<strong>独立的 $\\theta_m$</strong> 产生，但当我们面对<strong>多个相似但独立的子问题</strong>（如不同城市、不同样本群体）时，这种独立分析会导致数据稀疏的个体推断不可信。</p>\n<p>此时，我们需要引入<strong>分层（Hierarchical）结构</strong>和<strong>超参数（Hyperparameters）</strong>，让参数之间也具有关联性，从而实现<strong>信息共享</strong>。</p>\n<p>下一篇：<strong>分层贝叶斯（二）：从参数到分层</strong>。</p>\n"},{"title":"分层贝叶斯（2）：从参数到分层","date":"2025-10-07T03:52:00.000Z","katex":true,"cover":"/img/bys.png","_content":"\n# 🚀 分层贝叶斯：从参数到分层\n\n## 引言：模型背后的“真相”——参数\n\n在上一篇文章中，我们通过抛硬币实验介绍了贝叶斯定理的三大核心要素：先验、似然和后验。现在，我们需要将焦点放在那个不可见的、但支配着数据生成的关键要素——**参数（Parameter）$\\theta$**。\n\n在贝叶斯统计中，我们把模型中所有**不可直接观测的、但影响观测结果的未知量**都视为参数。\n\n* **抛硬币中：** $\\theta$ 是硬币正面朝上的**真实概率**。\n* **回归分析中：** $\\theta$ 可能是**回归系数**或**误差项的方差**。\n\n我们的终极目标就是利用数据 $D$，通过贝叶斯推断，找出这些参数 $\\theta$ 的**后验分布** $P(\\theta|D)$。而在预测阶段我们就可以通过这些参数预测新的数据分布$D_0$。\n\n---\n\n## 一、为什么需要升级到分层模型？\n\n当我们处理的数据集变得复杂，包含**多个相似但独立的单元**时，简单的贝叶斯模型就会遇到瓶颈。\n\n### 1. 简单贝叶斯模型（独立分析）的缺陷\n\n假设我们在 100 个不同的城市进行抛硬币实验，并为每个城市 $m$ 的参数 $\\theta_m$ 单独建立一个贝叶斯模型：\n\n$$P(\\theta_m|D_m) \\propto P(D_m|\\theta_m) \\cdot P(\\theta_m)$$\n\n* **问题：** 城市之间**信息隔绝**。如果城市 A 数据充足，它能得到可靠的 $\\theta_A$ 估计；但如果城市 B 只抛了 2 次硬币，它只能得到一个极端的、不可信的 $\\theta_B$ 估计。\n* **缺陷：** 模型没有利用到一个基本事实——**所有城市都属于“城市”这个群体**，它们可能共享一些共同的平均属性。\n\n### 2. 解决方案：引入超先验和分层结构\n\n我们期望能够构建一个统一的模型解决所有城市抛硬币的问题，为了让模型能够 **“互相学习”** 和 **“共享信息”**，我们需要在模型中引入一个新的层级和概念：**超参数**。\n\n* **超参数 ($\\alpha$)：** 假设所有城市 $\\theta_m$ 的真实概率都来自于一个**共同的超分布**（例如 $\\text{Beta}(\\alpha_1, \\alpha_2)$）。这里的 $\\alpha_1$ 和 $\\alpha_2$ 就是**超参数**。\n* **目的：** 通过超参数 $\\alpha$，我们不再假设每个城市的先验 $P(\\theta_m)$ 是固定的，而是假设 **$\\theta_m$ 的先验是由 $\\alpha$ 来控制的**。\n\n这就形成了**分层贝叶斯模型（Hierarchical Bayesian Model, HBM）**。\n\n换而言之，在简单贝叶斯中，我们设定先验，结合似然更新参数$\\theta$。而在分层贝叶斯中，我们不再人为设定先验，而是设定一个**超先验**$\\alpha$用以更新先验$\\theta$。\n\n### 案例引入:\n\n 假设我们在 $M$ 个不同的城市进行抛硬币实验。每个城市 $m$ 都有其独特的真实正面概率 $\\theta_1\\theta_2...\\theta_M$。我们不能简单地孤立分析每一个城市，而是需要让它们**共享信息**。\n\n **数据准备：将“城市”映射为索引**\n\n 我们首先将城市名称转换为模型可以处理的**分组索引**，并将观测数据绑定到这些索引上。\n\n| 城市名称    | 抛掷次数 ($N_m$) | 正面次数 ($k_m$) | **城市索引 ($m$)** | **局部参数 ($\\theta_m$)** |\n|:------- |:------------ |:------------ |:-------------- |:--------------------- |\n| 上海      | 1000         | 510          | 1              | $\\theta_1$            |\n| 深圳      | 10           | 9            | 2              | $\\theta_2$            |\n| 广州      | 500          | 260          | 3              | $\\theta_3$            |\n| **...** | **...**      | **...**      | **...**        | **...**               |\n| 纽约      | 800          | 415          | $M$            | $\\theta_M$            |\n\n* **参数数量：** 如果有 $M$ 个城市，模型将会有 $M$ 个需要推断的**局部参数** $\\theta_1, \\theta_2, \\dots, \\theta_M$。 **模型结构：通过索引建立层级联系** 城市信息不作为输入特征，而是作为**分组标签**，被编码到模型的**概率依赖结构**中，这是分层贝叶斯的核心。\n\n* **步骤 A：定义局部参数（底层）** 对于每个城市索引 $m$，我们假设其数据 $k_m$ 的生成依赖于其独特的局部参数 $\\theta_m$：\n  $$\\text{似然层：} k_m \\sim \\text{Binomial}(N_m, \\theta_m) \\quad \\text{其中 } m=1,\\dots,M$$\n  这告诉模型：“上海的数据只依赖于 $\\theta_{\\text{上海}}$。”\n\n* **步骤 B：定义超先验（中层，引入群体）** 关键在于下一层，我们通过**超参数 $\\alpha$** 将所有这些 $\\theta_m$ 联系起来。我们用通用的条件概率表达来描述这种依赖：\n  $$\\text{先验层：} \\theta_m \\sim p(\\theta|\\alpha)$$\n  这里的 $\\alpha$就是**超参数**，它代表了 **所有城市（即整个群体）** 的平均倾向和分散程度。\n  这个步骤告诉模型：“所有城市的硬币概率 $\\theta_m$ 都是从同一个 **全球硬币概率分布（由 $\\alpha$ 控制）** 中抽样出来的。”\n  这就是分层贝叶斯如何将“城市”数据给到模型的本质：\n  \n  城市信息作为**分组标签**，将数据分割成 $M$ 个单元，并让所有这 $M$ 个单元通过共同的**超先验 $\\alpha$** 产生联系，实现**信息共享**。\n\n---\n\n## 二、分层贝叶斯：从局部到全局的学习\n\n分层结构允许数据在不同层级流动，实现**数据共性**的提取。\n\n### 1. 分层结构的本质\n\n| 层级           | 参数/变量      | 信息流向的**结构依赖**                | 物理意义         |\n|:------------ |:---------- |:---------------------------- |:------------ |\n| **顶层 (超先验)** | $\\alpha$   | **$\\alpha$ 决定 $\\theta$ 的先验** | 群体的平均属性或变异性。 |\n| **中层 (先验)**  | $\\theta_m$ | **$\\theta$ 决定 $D$ 的似然**      | 单个城市硬币的真实概率。 |\n| **底层 (似然)**  | $D_m$      | **数据生成**                     | 单个城市的抛硬币结果。  |\n\n### 2. 推断流向与收缩效应\n\n在 HBM 中，数据的影响是双向的，从而催生了 HBM 最强大的优势——**收缩效应（Shrinkage）**。\n\n* **自下而上（集体学习）：** 所有城市的局部数据 $D_m$ 汇聚起来，共同推断出**超参数 $\\alpha$ 的后验分布** $P(\\alpha|D)$。我们用全体数据获得了关于**群体平均特征**的“集体智慧”。\n* **自上而下（收缩反馈）：** 这个**数据驱动的全局后验 $P(\\alpha|D)$**，反馈给每个局部参数 $\\theta_m$。它充当了每个 $\\theta_m$ 的 **“更明智的先验”**。\n* **收缩的魔力：** 对于数据稀疏的城市 B，其不可靠的估计值会被强大的**群体平均值**（由 $\\alpha$ 控制）**拉回**。这避免了极端的估计，使得模型推断更加**稳健**和**可信**。\n\n---\n\n## 三、结语：MCMC 的必然性\n\n分层贝叶斯模型虽然强大，但它带来了巨大的计算挑战：我们需要计算所有参数 **$(\\theta_1, \\dots, \\theta_M, \\alpha)$ 的联合后验分布**。\n\n由于参数数量庞大（高维）且后验形状复杂，传统的积分求解方法失效。因此，我们必须依赖于 **MCMC（Markov Chain Monte Carlo）采样算法**来近似。MCMC 成为了实现分层贝叶斯模型的**计算引擎**，它通过**样本的频率**来逼近复杂的**概率密度**。\n\n下一篇文章，我们将专注于 MCMC 的原理，彻底解开它是如何解决高维积分这一“维度灾难”的。\n","source":"_posts/2）分层贝叶斯：贝叶斯算法 .md","raw":"---\ntitle: 分层贝叶斯（2）：从参数到分层\ndate:  2025-10-07 11:52\n\ncategory: 算法\n\nkatex: true\ncover: /img/bys.png\n\n---\n\n# 🚀 分层贝叶斯：从参数到分层\n\n## 引言：模型背后的“真相”——参数\n\n在上一篇文章中，我们通过抛硬币实验介绍了贝叶斯定理的三大核心要素：先验、似然和后验。现在，我们需要将焦点放在那个不可见的、但支配着数据生成的关键要素——**参数（Parameter）$\\theta$**。\n\n在贝叶斯统计中，我们把模型中所有**不可直接观测的、但影响观测结果的未知量**都视为参数。\n\n* **抛硬币中：** $\\theta$ 是硬币正面朝上的**真实概率**。\n* **回归分析中：** $\\theta$ 可能是**回归系数**或**误差项的方差**。\n\n我们的终极目标就是利用数据 $D$，通过贝叶斯推断，找出这些参数 $\\theta$ 的**后验分布** $P(\\theta|D)$。而在预测阶段我们就可以通过这些参数预测新的数据分布$D_0$。\n\n---\n\n## 一、为什么需要升级到分层模型？\n\n当我们处理的数据集变得复杂，包含**多个相似但独立的单元**时，简单的贝叶斯模型就会遇到瓶颈。\n\n### 1. 简单贝叶斯模型（独立分析）的缺陷\n\n假设我们在 100 个不同的城市进行抛硬币实验，并为每个城市 $m$ 的参数 $\\theta_m$ 单独建立一个贝叶斯模型：\n\n$$P(\\theta_m|D_m) \\propto P(D_m|\\theta_m) \\cdot P(\\theta_m)$$\n\n* **问题：** 城市之间**信息隔绝**。如果城市 A 数据充足，它能得到可靠的 $\\theta_A$ 估计；但如果城市 B 只抛了 2 次硬币，它只能得到一个极端的、不可信的 $\\theta_B$ 估计。\n* **缺陷：** 模型没有利用到一个基本事实——**所有城市都属于“城市”这个群体**，它们可能共享一些共同的平均属性。\n\n### 2. 解决方案：引入超先验和分层结构\n\n我们期望能够构建一个统一的模型解决所有城市抛硬币的问题，为了让模型能够 **“互相学习”** 和 **“共享信息”**，我们需要在模型中引入一个新的层级和概念：**超参数**。\n\n* **超参数 ($\\alpha$)：** 假设所有城市 $\\theta_m$ 的真实概率都来自于一个**共同的超分布**（例如 $\\text{Beta}(\\alpha_1, \\alpha_2)$）。这里的 $\\alpha_1$ 和 $\\alpha_2$ 就是**超参数**。\n* **目的：** 通过超参数 $\\alpha$，我们不再假设每个城市的先验 $P(\\theta_m)$ 是固定的，而是假设 **$\\theta_m$ 的先验是由 $\\alpha$ 来控制的**。\n\n这就形成了**分层贝叶斯模型（Hierarchical Bayesian Model, HBM）**。\n\n换而言之，在简单贝叶斯中，我们设定先验，结合似然更新参数$\\theta$。而在分层贝叶斯中，我们不再人为设定先验，而是设定一个**超先验**$\\alpha$用以更新先验$\\theta$。\n\n### 案例引入:\n\n 假设我们在 $M$ 个不同的城市进行抛硬币实验。每个城市 $m$ 都有其独特的真实正面概率 $\\theta_1\\theta_2...\\theta_M$。我们不能简单地孤立分析每一个城市，而是需要让它们**共享信息**。\n\n **数据准备：将“城市”映射为索引**\n\n 我们首先将城市名称转换为模型可以处理的**分组索引**，并将观测数据绑定到这些索引上。\n\n| 城市名称    | 抛掷次数 ($N_m$) | 正面次数 ($k_m$) | **城市索引 ($m$)** | **局部参数 ($\\theta_m$)** |\n|:------- |:------------ |:------------ |:-------------- |:--------------------- |\n| 上海      | 1000         | 510          | 1              | $\\theta_1$            |\n| 深圳      | 10           | 9            | 2              | $\\theta_2$            |\n| 广州      | 500          | 260          | 3              | $\\theta_3$            |\n| **...** | **...**      | **...**      | **...**        | **...**               |\n| 纽约      | 800          | 415          | $M$            | $\\theta_M$            |\n\n* **参数数量：** 如果有 $M$ 个城市，模型将会有 $M$ 个需要推断的**局部参数** $\\theta_1, \\theta_2, \\dots, \\theta_M$。 **模型结构：通过索引建立层级联系** 城市信息不作为输入特征，而是作为**分组标签**，被编码到模型的**概率依赖结构**中，这是分层贝叶斯的核心。\n\n* **步骤 A：定义局部参数（底层）** 对于每个城市索引 $m$，我们假设其数据 $k_m$ 的生成依赖于其独特的局部参数 $\\theta_m$：\n  $$\\text{似然层：} k_m \\sim \\text{Binomial}(N_m, \\theta_m) \\quad \\text{其中 } m=1,\\dots,M$$\n  这告诉模型：“上海的数据只依赖于 $\\theta_{\\text{上海}}$。”\n\n* **步骤 B：定义超先验（中层，引入群体）** 关键在于下一层，我们通过**超参数 $\\alpha$** 将所有这些 $\\theta_m$ 联系起来。我们用通用的条件概率表达来描述这种依赖：\n  $$\\text{先验层：} \\theta_m \\sim p(\\theta|\\alpha)$$\n  这里的 $\\alpha$就是**超参数**，它代表了 **所有城市（即整个群体）** 的平均倾向和分散程度。\n  这个步骤告诉模型：“所有城市的硬币概率 $\\theta_m$ 都是从同一个 **全球硬币概率分布（由 $\\alpha$ 控制）** 中抽样出来的。”\n  这就是分层贝叶斯如何将“城市”数据给到模型的本质：\n  \n  城市信息作为**分组标签**，将数据分割成 $M$ 个单元，并让所有这 $M$ 个单元通过共同的**超先验 $\\alpha$** 产生联系，实现**信息共享**。\n\n---\n\n## 二、分层贝叶斯：从局部到全局的学习\n\n分层结构允许数据在不同层级流动，实现**数据共性**的提取。\n\n### 1. 分层结构的本质\n\n| 层级           | 参数/变量      | 信息流向的**结构依赖**                | 物理意义         |\n|:------------ |:---------- |:---------------------------- |:------------ |\n| **顶层 (超先验)** | $\\alpha$   | **$\\alpha$ 决定 $\\theta$ 的先验** | 群体的平均属性或变异性。 |\n| **中层 (先验)**  | $\\theta_m$ | **$\\theta$ 决定 $D$ 的似然**      | 单个城市硬币的真实概率。 |\n| **底层 (似然)**  | $D_m$      | **数据生成**                     | 单个城市的抛硬币结果。  |\n\n### 2. 推断流向与收缩效应\n\n在 HBM 中，数据的影响是双向的，从而催生了 HBM 最强大的优势——**收缩效应（Shrinkage）**。\n\n* **自下而上（集体学习）：** 所有城市的局部数据 $D_m$ 汇聚起来，共同推断出**超参数 $\\alpha$ 的后验分布** $P(\\alpha|D)$。我们用全体数据获得了关于**群体平均特征**的“集体智慧”。\n* **自上而下（收缩反馈）：** 这个**数据驱动的全局后验 $P(\\alpha|D)$**，反馈给每个局部参数 $\\theta_m$。它充当了每个 $\\theta_m$ 的 **“更明智的先验”**。\n* **收缩的魔力：** 对于数据稀疏的城市 B，其不可靠的估计值会被强大的**群体平均值**（由 $\\alpha$ 控制）**拉回**。这避免了极端的估计，使得模型推断更加**稳健**和**可信**。\n\n---\n\n## 三、结语：MCMC 的必然性\n\n分层贝叶斯模型虽然强大，但它带来了巨大的计算挑战：我们需要计算所有参数 **$(\\theta_1, \\dots, \\theta_M, \\alpha)$ 的联合后验分布**。\n\n由于参数数量庞大（高维）且后验形状复杂，传统的积分求解方法失效。因此，我们必须依赖于 **MCMC（Markov Chain Monte Carlo）采样算法**来近似。MCMC 成为了实现分层贝叶斯模型的**计算引擎**，它通过**样本的频率**来逼近复杂的**概率密度**。\n\n下一篇文章，我们将专注于 MCMC 的原理，彻底解开它是如何解决高维积分这一“维度灾难”的。\n","slug":"2）分层贝叶斯：贝叶斯算法 ","published":1,"updated":"2025-10-07T05:50:19.860Z","_id":"cuidoDNz9NcHD4NxNj8nR8Is7","comments":1,"layout":"post","photos":[],"content":"<h1 id=\"🚀-分层贝叶斯：从参数到分层\"><a href=\"#🚀-分层贝叶斯：从参数到分层\" class=\"headerlink\" title=\"🚀 分层贝叶斯：从参数到分层\"></a>🚀 分层贝叶斯：从参数到分层</h1><h2 id=\"引言：模型背后的“真相”——参数\"><a href=\"#引言：模型背后的“真相”——参数\" class=\"headerlink\" title=\"引言：模型背后的“真相”——参数\"></a>引言：模型背后的“真相”——参数</h2><p>在上一篇文章中，我们通过抛硬币实验介绍了贝叶斯定理的三大核心要素：先验、似然和后验。现在，我们需要将焦点放在那个不可见的、但支配着数据生成的关键要素——<strong>参数（Parameter）$\\theta$</strong>。</p>\n<p>在贝叶斯统计中，我们把模型中所有<strong>不可直接观测的、但影响观测结果的未知量</strong>都视为参数。</p>\n<ul>\n<li><strong>抛硬币中：</strong> $\\theta$ 是硬币正面朝上的<strong>真实概率</strong>。</li>\n<li><strong>回归分析中：</strong> $\\theta$ 可能是<strong>回归系数</strong>或<strong>误差项的方差</strong>。</li>\n</ul>\n<p>我们的终极目标就是利用数据 $D$，通过贝叶斯推断，找出这些参数 $\\theta$ 的<strong>后验分布</strong> $P(\\theta|D)$。而在预测阶段我们就可以通过这些参数预测新的数据分布$D_0$。</p>\n<hr>\n<h2 id=\"一、为什么需要升级到分层模型？\"><a href=\"#一、为什么需要升级到分层模型？\" class=\"headerlink\" title=\"一、为什么需要升级到分层模型？\"></a>一、为什么需要升级到分层模型？</h2><p>当我们处理的数据集变得复杂，包含<strong>多个相似但独立的单元</strong>时，简单的贝叶斯模型就会遇到瓶颈。</p>\n<h3 id=\"1-简单贝叶斯模型（独立分析）的缺陷\"><a href=\"#1-简单贝叶斯模型（独立分析）的缺陷\" class=\"headerlink\" title=\"1. 简单贝叶斯模型（独立分析）的缺陷\"></a>1. 简单贝叶斯模型（独立分析）的缺陷</h3><p>假设我们在 100 个不同的城市进行抛硬币实验，并为每个城市 $m$ 的参数 $\\theta_m$ 单独建立一个贝叶斯模型：</p>\n<p>$$P(\\theta_m|D_m) \\propto P(D_m|\\theta_m) \\cdot P(\\theta_m)$$</p>\n<ul>\n<li><strong>问题：</strong> 城市之间<strong>信息隔绝</strong>。如果城市 A 数据充足，它能得到可靠的 $\\theta_A$ 估计；但如果城市 B 只抛了 2 次硬币，它只能得到一个极端的、不可信的 $\\theta_B$ 估计。</li>\n<li><strong>缺陷：</strong> 模型没有利用到一个基本事实——<strong>所有城市都属于“城市”这个群体</strong>，它们可能共享一些共同的平均属性。</li>\n</ul>\n<h3 id=\"2-解决方案：引入超先验和分层结构\"><a href=\"#2-解决方案：引入超先验和分层结构\" class=\"headerlink\" title=\"2. 解决方案：引入超先验和分层结构\"></a>2. 解决方案：引入超先验和分层结构</h3><p>我们期望能够构建一个统一的模型解决所有城市抛硬币的问题，为了让模型能够 <strong>“互相学习”</strong> 和 <strong>“共享信息”</strong>，我们需要在模型中引入一个新的层级和概念：<strong>超参数</strong>。</p>\n<ul>\n<li><strong>超参数 ($\\alpha$)：</strong> 假设所有城市 $\\theta_m$ 的真实概率都来自于一个<strong>共同的超分布</strong>（例如 $\\text{Beta}(\\alpha_1, \\alpha_2)$）。这里的 $\\alpha_1$ 和 $\\alpha_2$ 就是<strong>超参数</strong>。</li>\n<li><strong>目的：</strong> 通过超参数 $\\alpha$，我们不再假设每个城市的先验 $P(\\theta_m)$ 是固定的，而是假设 <strong>$\\theta_m$ 的先验是由 $\\alpha$ 来控制的</strong>。</li>\n</ul>\n<p>这就形成了<strong>分层贝叶斯模型（Hierarchical Bayesian Model, HBM）</strong>。</p>\n<p>换而言之，在简单贝叶斯中，我们设定先验，结合似然更新参数$\\theta$。而在分层贝叶斯中，我们不再人为设定先验，而是设定一个<strong>超先验</strong>$\\alpha$用以更新先验$\\theta$。</p>\n<h3 id=\"案例引入\"><a href=\"#案例引入\" class=\"headerlink\" title=\"案例引入:\"></a>案例引入:</h3><p> 假设我们在 $M$ 个不同的城市进行抛硬币实验。每个城市 $m$ 都有其独特的真实正面概率 $\\theta_1\\theta_2…\\theta_M$。我们不能简单地孤立分析每一个城市，而是需要让它们<strong>共享信息</strong>。</p>\n<p> <strong>数据准备：将“城市”映射为索引</strong></p>\n<p> 我们首先将城市名称转换为模型可以处理的<strong>分组索引</strong>，并将观测数据绑定到这些索引上。</p>\n<table>\n<thead>\n<tr>\n<th align=\"left\">城市名称</th>\n<th align=\"left\">抛掷次数 ($N_m$)</th>\n<th align=\"left\">正面次数 ($k_m$)</th>\n<th align=\"left\"><strong>城市索引 ($m$)</strong></th>\n<th align=\"left\"><strong>局部参数 ($\\theta_m$)</strong></th>\n</tr>\n</thead>\n<tbody><tr>\n<td align=\"left\">上海</td>\n<td align=\"left\">1000</td>\n<td align=\"left\">510</td>\n<td align=\"left\">1</td>\n<td align=\"left\">$\\theta_1$</td>\n</tr>\n<tr>\n<td align=\"left\">深圳</td>\n<td align=\"left\">10</td>\n<td align=\"left\">9</td>\n<td align=\"left\">2</td>\n<td align=\"left\">$\\theta_2$</td>\n</tr>\n<tr>\n<td align=\"left\">广州</td>\n<td align=\"left\">500</td>\n<td align=\"left\">260</td>\n<td align=\"left\">3</td>\n<td align=\"left\">$\\theta_3$</td>\n</tr>\n<tr>\n<td align=\"left\"><strong>…</strong></td>\n<td align=\"left\"><strong>…</strong></td>\n<td align=\"left\"><strong>…</strong></td>\n<td align=\"left\"><strong>…</strong></td>\n<td align=\"left\"><strong>…</strong></td>\n</tr>\n<tr>\n<td align=\"left\">纽约</td>\n<td align=\"left\">800</td>\n<td align=\"left\">415</td>\n<td align=\"left\">$M$</td>\n<td align=\"left\">$\\theta_M$</td>\n</tr>\n</tbody></table>\n<ul>\n<li><p><strong>参数数量：</strong> 如果有 $M$ 个城市，模型将会有 $M$ 个需要推断的<strong>局部参数</strong> $\\theta_1, \\theta_2, \\dots, \\theta_M$。 <strong>模型结构：通过索引建立层级联系</strong> 城市信息不作为输入特征，而是作为<strong>分组标签</strong>，被编码到模型的<strong>概率依赖结构</strong>中，这是分层贝叶斯的核心。</p>\n</li>\n<li><p><strong>步骤 A：定义局部参数（底层）</strong> 对于每个城市索引 $m$，我们假设其数据 $k_m$ 的生成依赖于其独特的局部参数 $\\theta_m$：<br>$$\\text{似然层：} k_m \\sim \\text{Binomial}(N_m, \\theta_m) \\quad \\text{其中 } m&#x3D;1,\\dots,M$$<br>这告诉模型：“上海的数据只依赖于 $\\theta_{\\text{上海}}$。”</p>\n</li>\n<li><p><strong>步骤 B：定义超先验（中层，引入群体）</strong> 关键在于下一层，我们通过<strong>超参数 $\\alpha$</strong> 将所有这些 $\\theta_m$ 联系起来。我们用通用的条件概率表达来描述这种依赖：<br>$$\\text{先验层：} \\theta_m \\sim p(\\theta|\\alpha)$$<br>这里的 $\\alpha$就是<strong>超参数</strong>，它代表了 <strong>所有城市（即整个群体）</strong> 的平均倾向和分散程度。<br>这个步骤告诉模型：“所有城市的硬币概率 $\\theta_m$ 都是从同一个 <strong>全球硬币概率分布（由 $\\alpha$ 控制）</strong> 中抽样出来的。”<br>这就是分层贝叶斯如何将“城市”数据给到模型的本质：</p>\n<p>城市信息作为<strong>分组标签</strong>，将数据分割成 $M$ 个单元，并让所有这 $M$ 个单元通过共同的<strong>超先验 $\\alpha$</strong> 产生联系，实现<strong>信息共享</strong>。</p>\n</li>\n</ul>\n<hr>\n<h2 id=\"二、分层贝叶斯：从局部到全局的学习\"><a href=\"#二、分层贝叶斯：从局部到全局的学习\" class=\"headerlink\" title=\"二、分层贝叶斯：从局部到全局的学习\"></a>二、分层贝叶斯：从局部到全局的学习</h2><p>分层结构允许数据在不同层级流动，实现<strong>数据共性</strong>的提取。</p>\n<h3 id=\"1-分层结构的本质\"><a href=\"#1-分层结构的本质\" class=\"headerlink\" title=\"1. 分层结构的本质\"></a>1. 分层结构的本质</h3><table>\n<thead>\n<tr>\n<th align=\"left\">层级</th>\n<th align=\"left\">参数&#x2F;变量</th>\n<th align=\"left\">信息流向的<strong>结构依赖</strong></th>\n<th align=\"left\">物理意义</th>\n</tr>\n</thead>\n<tbody><tr>\n<td align=\"left\"><strong>顶层 (超先验)</strong></td>\n<td align=\"left\">$\\alpha$</td>\n<td align=\"left\"><strong>$\\alpha$ 决定 $\\theta$ 的先验</strong></td>\n<td align=\"left\">群体的平均属性或变异性。</td>\n</tr>\n<tr>\n<td align=\"left\"><strong>中层 (先验)</strong></td>\n<td align=\"left\">$\\theta_m$</td>\n<td align=\"left\"><strong>$\\theta$ 决定 $D$ 的似然</strong></td>\n<td align=\"left\">单个城市硬币的真实概率。</td>\n</tr>\n<tr>\n<td align=\"left\"><strong>底层 (似然)</strong></td>\n<td align=\"left\">$D_m$</td>\n<td align=\"left\"><strong>数据生成</strong></td>\n<td align=\"left\">单个城市的抛硬币结果。</td>\n</tr>\n</tbody></table>\n<h3 id=\"2-推断流向与收缩效应\"><a href=\"#2-推断流向与收缩效应\" class=\"headerlink\" title=\"2. 推断流向与收缩效应\"></a>2. 推断流向与收缩效应</h3><p>在 HBM 中，数据的影响是双向的，从而催生了 HBM 最强大的优势——<strong>收缩效应（Shrinkage）</strong>。</p>\n<ul>\n<li><strong>自下而上（集体学习）：</strong> 所有城市的局部数据 $D_m$ 汇聚起来，共同推断出<strong>超参数 $\\alpha$ 的后验分布</strong> $P(\\alpha|D)$。我们用全体数据获得了关于<strong>群体平均特征</strong>的“集体智慧”。</li>\n<li><strong>自上而下（收缩反馈）：</strong> 这个<strong>数据驱动的全局后验 $P(\\alpha|D)$</strong>，反馈给每个局部参数 $\\theta_m$。它充当了每个 $\\theta_m$ 的 <strong>“更明智的先验”</strong>。</li>\n<li><strong>收缩的魔力：</strong> 对于数据稀疏的城市 B，其不可靠的估计值会被强大的<strong>群体平均值</strong>（由 $\\alpha$ 控制）<strong>拉回</strong>。这避免了极端的估计，使得模型推断更加<strong>稳健</strong>和<strong>可信</strong>。</li>\n</ul>\n<hr>\n<h2 id=\"三、结语：MCMC-的必然性\"><a href=\"#三、结语：MCMC-的必然性\" class=\"headerlink\" title=\"三、结语：MCMC 的必然性\"></a>三、结语：MCMC 的必然性</h2><p>分层贝叶斯模型虽然强大，但它带来了巨大的计算挑战：我们需要计算所有参数 <strong>$(\\theta_1, \\dots, \\theta_M, \\alpha)$ 的联合后验分布</strong>。</p>\n<p>由于参数数量庞大（高维）且后验形状复杂，传统的积分求解方法失效。因此，我们必须依赖于 <strong>MCMC（Markov Chain Monte Carlo）采样算法</strong>来近似。MCMC 成为了实现分层贝叶斯模型的<strong>计算引擎</strong>，它通过<strong>样本的频率</strong>来逼近复杂的<strong>概率密度</strong>。</p>\n<p>下一篇文章，我们将专注于 MCMC 的原理，彻底解开它是如何解决高维积分这一“维度灾难”的。</p>\n","excerpt":"","more":"<h1 id=\"🚀-分层贝叶斯：从参数到分层\"><a href=\"#🚀-分层贝叶斯：从参数到分层\" class=\"headerlink\" title=\"🚀 分层贝叶斯：从参数到分层\"></a>🚀 分层贝叶斯：从参数到分层</h1><h2 id=\"引言：模型背后的“真相”——参数\"><a href=\"#引言：模型背后的“真相”——参数\" class=\"headerlink\" title=\"引言：模型背后的“真相”——参数\"></a>引言：模型背后的“真相”——参数</h2><p>在上一篇文章中，我们通过抛硬币实验介绍了贝叶斯定理的三大核心要素：先验、似然和后验。现在，我们需要将焦点放在那个不可见的、但支配着数据生成的关键要素——<strong>参数（Parameter）$\\theta$</strong>。</p>\n<p>在贝叶斯统计中，我们把模型中所有<strong>不可直接观测的、但影响观测结果的未知量</strong>都视为参数。</p>\n<ul>\n<li><strong>抛硬币中：</strong> $\\theta$ 是硬币正面朝上的<strong>真实概率</strong>。</li>\n<li><strong>回归分析中：</strong> $\\theta$ 可能是<strong>回归系数</strong>或<strong>误差项的方差</strong>。</li>\n</ul>\n<p>我们的终极目标就是利用数据 $D$，通过贝叶斯推断，找出这些参数 $\\theta$ 的<strong>后验分布</strong> $P(\\theta|D)$。而在预测阶段我们就可以通过这些参数预测新的数据分布$D_0$。</p>\n<hr>\n<h2 id=\"一、为什么需要升级到分层模型？\"><a href=\"#一、为什么需要升级到分层模型？\" class=\"headerlink\" title=\"一、为什么需要升级到分层模型？\"></a>一、为什么需要升级到分层模型？</h2><p>当我们处理的数据集变得复杂，包含<strong>多个相似但独立的单元</strong>时，简单的贝叶斯模型就会遇到瓶颈。</p>\n<h3 id=\"1-简单贝叶斯模型（独立分析）的缺陷\"><a href=\"#1-简单贝叶斯模型（独立分析）的缺陷\" class=\"headerlink\" title=\"1. 简单贝叶斯模型（独立分析）的缺陷\"></a>1. 简单贝叶斯模型（独立分析）的缺陷</h3><p>假设我们在 100 个不同的城市进行抛硬币实验，并为每个城市 $m$ 的参数 $\\theta_m$ 单独建立一个贝叶斯模型：</p>\n<p>$$P(\\theta_m|D_m) \\propto P(D_m|\\theta_m) \\cdot P(\\theta_m)$$</p>\n<ul>\n<li><strong>问题：</strong> 城市之间<strong>信息隔绝</strong>。如果城市 A 数据充足，它能得到可靠的 $\\theta_A$ 估计；但如果城市 B 只抛了 2 次硬币，它只能得到一个极端的、不可信的 $\\theta_B$ 估计。</li>\n<li><strong>缺陷：</strong> 模型没有利用到一个基本事实——<strong>所有城市都属于“城市”这个群体</strong>，它们可能共享一些共同的平均属性。</li>\n</ul>\n<h3 id=\"2-解决方案：引入超先验和分层结构\"><a href=\"#2-解决方案：引入超先验和分层结构\" class=\"headerlink\" title=\"2. 解决方案：引入超先验和分层结构\"></a>2. 解决方案：引入超先验和分层结构</h3><p>我们期望能够构建一个统一的模型解决所有城市抛硬币的问题，为了让模型能够 <strong>“互相学习”</strong> 和 <strong>“共享信息”</strong>，我们需要在模型中引入一个新的层级和概念：<strong>超参数</strong>。</p>\n<ul>\n<li><strong>超参数 ($\\alpha$)：</strong> 假设所有城市 $\\theta_m$ 的真实概率都来自于一个<strong>共同的超分布</strong>（例如 $\\text{Beta}(\\alpha_1, \\alpha_2)$）。这里的 $\\alpha_1$ 和 $\\alpha_2$ 就是<strong>超参数</strong>。</li>\n<li><strong>目的：</strong> 通过超参数 $\\alpha$，我们不再假设每个城市的先验 $P(\\theta_m)$ 是固定的，而是假设 <strong>$\\theta_m$ 的先验是由 $\\alpha$ 来控制的</strong>。</li>\n</ul>\n<p>这就形成了<strong>分层贝叶斯模型（Hierarchical Bayesian Model, HBM）</strong>。</p>\n<p>换而言之，在简单贝叶斯中，我们设定先验，结合似然更新参数$\\theta$。而在分层贝叶斯中，我们不再人为设定先验，而是设定一个<strong>超先验</strong>$\\alpha$用以更新先验$\\theta$。</p>\n<h3 id=\"案例引入\"><a href=\"#案例引入\" class=\"headerlink\" title=\"案例引入:\"></a>案例引入:</h3><p> 假设我们在 $M$ 个不同的城市进行抛硬币实验。每个城市 $m$ 都有其独特的真实正面概率 $\\theta_1\\theta_2…\\theta_M$。我们不能简单地孤立分析每一个城市，而是需要让它们<strong>共享信息</strong>。</p>\n<p> <strong>数据准备：将“城市”映射为索引</strong></p>\n<p> 我们首先将城市名称转换为模型可以处理的<strong>分组索引</strong>，并将观测数据绑定到这些索引上。</p>\n<table>\n<thead>\n<tr>\n<th align=\"left\">城市名称</th>\n<th align=\"left\">抛掷次数 ($N_m$)</th>\n<th align=\"left\">正面次数 ($k_m$)</th>\n<th align=\"left\"><strong>城市索引 ($m$)</strong></th>\n<th align=\"left\"><strong>局部参数 ($\\theta_m$)</strong></th>\n</tr>\n</thead>\n<tbody><tr>\n<td align=\"left\">上海</td>\n<td align=\"left\">1000</td>\n<td align=\"left\">510</td>\n<td align=\"left\">1</td>\n<td align=\"left\">$\\theta_1$</td>\n</tr>\n<tr>\n<td align=\"left\">深圳</td>\n<td align=\"left\">10</td>\n<td align=\"left\">9</td>\n<td align=\"left\">2</td>\n<td align=\"left\">$\\theta_2$</td>\n</tr>\n<tr>\n<td align=\"left\">广州</td>\n<td align=\"left\">500</td>\n<td align=\"left\">260</td>\n<td align=\"left\">3</td>\n<td align=\"left\">$\\theta_3$</td>\n</tr>\n<tr>\n<td align=\"left\"><strong>…</strong></td>\n<td align=\"left\"><strong>…</strong></td>\n<td align=\"left\"><strong>…</strong></td>\n<td align=\"left\"><strong>…</strong></td>\n<td align=\"left\"><strong>…</strong></td>\n</tr>\n<tr>\n<td align=\"left\">纽约</td>\n<td align=\"left\">800</td>\n<td align=\"left\">415</td>\n<td align=\"left\">$M$</td>\n<td align=\"left\">$\\theta_M$</td>\n</tr>\n</tbody></table>\n<ul>\n<li><p><strong>参数数量：</strong> 如果有 $M$ 个城市，模型将会有 $M$ 个需要推断的<strong>局部参数</strong> $\\theta_1, \\theta_2, \\dots, \\theta_M$。 <strong>模型结构：通过索引建立层级联系</strong> 城市信息不作为输入特征，而是作为<strong>分组标签</strong>，被编码到模型的<strong>概率依赖结构</strong>中，这是分层贝叶斯的核心。</p>\n</li>\n<li><p><strong>步骤 A：定义局部参数（底层）</strong> 对于每个城市索引 $m$，我们假设其数据 $k_m$ 的生成依赖于其独特的局部参数 $\\theta_m$：<br>$$\\text{似然层：} k_m \\sim \\text{Binomial}(N_m, \\theta_m) \\quad \\text{其中 } m&#x3D;1,\\dots,M$$<br>这告诉模型：“上海的数据只依赖于 $\\theta_{\\text{上海}}$。”</p>\n</li>\n<li><p><strong>步骤 B：定义超先验（中层，引入群体）</strong> 关键在于下一层，我们通过<strong>超参数 $\\alpha$</strong> 将所有这些 $\\theta_m$ 联系起来。我们用通用的条件概率表达来描述这种依赖：<br>$$\\text{先验层：} \\theta_m \\sim p(\\theta|\\alpha)$$<br>这里的 $\\alpha$就是<strong>超参数</strong>，它代表了 <strong>所有城市（即整个群体）</strong> 的平均倾向和分散程度。<br>这个步骤告诉模型：“所有城市的硬币概率 $\\theta_m$ 都是从同一个 <strong>全球硬币概率分布（由 $\\alpha$ 控制）</strong> 中抽样出来的。”<br>这就是分层贝叶斯如何将“城市”数据给到模型的本质：</p>\n<p>城市信息作为<strong>分组标签</strong>，将数据分割成 $M$ 个单元，并让所有这 $M$ 个单元通过共同的<strong>超先验 $\\alpha$</strong> 产生联系，实现<strong>信息共享</strong>。</p>\n</li>\n</ul>\n<hr>\n<h2 id=\"二、分层贝叶斯：从局部到全局的学习\"><a href=\"#二、分层贝叶斯：从局部到全局的学习\" class=\"headerlink\" title=\"二、分层贝叶斯：从局部到全局的学习\"></a>二、分层贝叶斯：从局部到全局的学习</h2><p>分层结构允许数据在不同层级流动，实现<strong>数据共性</strong>的提取。</p>\n<h3 id=\"1-分层结构的本质\"><a href=\"#1-分层结构的本质\" class=\"headerlink\" title=\"1. 分层结构的本质\"></a>1. 分层结构的本质</h3><table>\n<thead>\n<tr>\n<th align=\"left\">层级</th>\n<th align=\"left\">参数&#x2F;变量</th>\n<th align=\"left\">信息流向的<strong>结构依赖</strong></th>\n<th align=\"left\">物理意义</th>\n</tr>\n</thead>\n<tbody><tr>\n<td align=\"left\"><strong>顶层 (超先验)</strong></td>\n<td align=\"left\">$\\alpha$</td>\n<td align=\"left\"><strong>$\\alpha$ 决定 $\\theta$ 的先验</strong></td>\n<td align=\"left\">群体的平均属性或变异性。</td>\n</tr>\n<tr>\n<td align=\"left\"><strong>中层 (先验)</strong></td>\n<td align=\"left\">$\\theta_m$</td>\n<td align=\"left\"><strong>$\\theta$ 决定 $D$ 的似然</strong></td>\n<td align=\"left\">单个城市硬币的真实概率。</td>\n</tr>\n<tr>\n<td align=\"left\"><strong>底层 (似然)</strong></td>\n<td align=\"left\">$D_m$</td>\n<td align=\"left\"><strong>数据生成</strong></td>\n<td align=\"left\">单个城市的抛硬币结果。</td>\n</tr>\n</tbody></table>\n<h3 id=\"2-推断流向与收缩效应\"><a href=\"#2-推断流向与收缩效应\" class=\"headerlink\" title=\"2. 推断流向与收缩效应\"></a>2. 推断流向与收缩效应</h3><p>在 HBM 中，数据的影响是双向的，从而催生了 HBM 最强大的优势——<strong>收缩效应（Shrinkage）</strong>。</p>\n<ul>\n<li><strong>自下而上（集体学习）：</strong> 所有城市的局部数据 $D_m$ 汇聚起来，共同推断出<strong>超参数 $\\alpha$ 的后验分布</strong> $P(\\alpha|D)$。我们用全体数据获得了关于<strong>群体平均特征</strong>的“集体智慧”。</li>\n<li><strong>自上而下（收缩反馈）：</strong> 这个<strong>数据驱动的全局后验 $P(\\alpha|D)$</strong>，反馈给每个局部参数 $\\theta_m$。它充当了每个 $\\theta_m$ 的 <strong>“更明智的先验”</strong>。</li>\n<li><strong>收缩的魔力：</strong> 对于数据稀疏的城市 B，其不可靠的估计值会被强大的<strong>群体平均值</strong>（由 $\\alpha$ 控制）<strong>拉回</strong>。这避免了极端的估计，使得模型推断更加<strong>稳健</strong>和<strong>可信</strong>。</li>\n</ul>\n<hr>\n<h2 id=\"三、结语：MCMC-的必然性\"><a href=\"#三、结语：MCMC-的必然性\" class=\"headerlink\" title=\"三、结语：MCMC 的必然性\"></a>三、结语：MCMC 的必然性</h2><p>分层贝叶斯模型虽然强大，但它带来了巨大的计算挑战：我们需要计算所有参数 <strong>$(\\theta_1, \\dots, \\theta_M, \\alpha)$ 的联合后验分布</strong>。</p>\n<p>由于参数数量庞大（高维）且后验形状复杂，传统的积分求解方法失效。因此，我们必须依赖于 <strong>MCMC（Markov Chain Monte Carlo）采样算法</strong>来近似。MCMC 成为了实现分层贝叶斯模型的<strong>计算引擎</strong>，它通过<strong>样本的频率</strong>来逼近复杂的<strong>概率密度</strong>。</p>\n<p>下一篇文章，我们将专注于 MCMC 的原理，彻底解开它是如何解决高维积分这一“维度灾难”的。</p>\n"},{"title":"分层贝叶斯（3）：MCMC——理论与现实的桥梁","date":"2025-10-07T08:13:00.000Z","katex":true,"cover":"/img/bys.png","_content":"\n# ⚙️ 分层贝叶斯：MCMC——复杂后验的采样\n\n## 引言：当贝叶斯遇到“维度灾难”\n\n在前两篇文章中，我们了解了贝叶斯推断的基础（先验、似然、后验）以及分层贝叶斯模型（HBM）如何通过**超参数 $\\alpha$** 实现信息共享。\n\n分层模型虽然强大，但也带来了巨大的计算挑战。回想贝叶斯定理：\n\n$$P(\\theta|D) \\propto P(D|\\theta) P(\\theta)$$\n\n当我们有 $M$ 个局部参数 $\\theta_m$ 和超参数 $\\alpha$ 时，我们需要推断的是一个关于 $(\\theta_1, \\dots, \\theta_M, \\alpha)$ 的**联合后验分布**。在现实模型中，这个参数空间可能有数百甚至数千个维度。\n\n* **挑战：** 这种高维的后验分布 $P(\\theta|D)$ 形状复杂、多峰，**无法通过传统的解析积分求解**。  \n* **解决方案：** 我们需要一种方法来“近似”这个复杂的分布——这就是 **马尔可夫链蒙特卡洛（Markov Chain Monte Carlo, MCMC）** 采样方法。\n\n---\n\n## 一、从积分到采样：用“频率”逼近“密度”\n\n### 1. 从积分的痛点说起\n\n贝叶斯推断的核心在于后验分布：\n\n$$P(\\theta|D) = \\frac{P(D|\\theta)P(\\theta)}{P(D)}$$\n\n其中分母：\n\n$$P(D) = \\int P(D|\\theta)P(\\theta)\\, d\\theta$$\n\n通常无法解析求解。  \n于是我们希望通过**随机采样**来逼近这个积分。\n\n如果我们能够从后验分布 $P(\\theta|D)$ 中采样得到点集 $\\{\\theta^{(1)}, \\dots, \\theta^{(N)}\\}$，那么任意函数的期望就可以近似为：\n\n$$\\mathbb{E}[f(\\theta)] \\approx \\frac{1}{N}\\sum_{i=1}^N f(\\theta^{(i)})$$\n\n换句话说：  \n**积分太难？那就通过采样求平均。**\n\n---\n\n### 2. 蒙特卡洛（Monte Carlo）的思想\n\n蒙特卡洛方法用随机抽样替代复杂计算。  \n例如：要估计一个复杂形状区域的面积，只需在外接正方形中随机撒点，通过落入目标区域的比例估算面积。\n\n同样地，在贝叶斯推断中，我们通过**样本频率**近似**概率密度**。  \n样本在后验高密度区域出现得更频繁，自然反映了后验分布的形状。\n\n---\n\n## 二、MCMC 的核心：让样本“走向后验”\n\n### 1. 马尔可夫链：带有记忆衰减的随机游走\n\n马尔可夫链是一种**无记忆性随机过程**：\n\n$$P(\\theta^{(t+1)}|\\theta^{(t)}, \\theta^{(t-1)}, \\dots) = P(\\theta^{(t+1)}|\\theta^{(t)})$$\n\nMCMC 的目标是构造这样一条链，使其在长期运行后，状态分布稳定地收敛到目标分布 $P(\\theta|D)$。  \n只要链是**不可约（Irreducible）**、**非周期（Aperiodic）且满足平稳性（Stationarity）**，采样点就能代表后验分布。\n\n> MCMC 让链在高概率区域停留更久，低概率区域更快离开。  \n> 链走得越久，样本分布越接近真实后验。\n\n---\n\n### 2. Metropolis–Hastings 算法（MH）\n\nMH 是最经典的 MCMC 算法。  \n它通过“提出–接受–拒绝”的机制实现采样：\n\n1. **初始化** $\\theta^{(0)}$。  \n\n2. **提出候选样本** $\\theta'$，从提议分布 $q(\\theta'|\\theta^{(t)})$ 生成。  \n\n3. **计算接受率：**\n   \n   $$r = \\frac{P(D|\\theta') P(\\theta') q(\\theta^{(t)}|\\theta')}{P(D|\\theta^{(t)}) P(\\theta^{(t)}) q(\\theta'|\\theta^{(t)})}$$\n\n4. **接受或拒绝：**\n   \n   $$ \\theta^{(t+1)} =\\begin{cases}\\theta', & \\text{以概率 } \\min(1, r) \\text{ 接受}\\\\\\theta^{(t)}, & \\text{否则保持不变}\\end{cases}$$\n\n5. 重复迭代直到收敛。\n\n在充分运行后，样本序列 $\\{\\theta^{(1)}, \\dots, \\theta^{(N)}\\}$ 就服从目标后验分布。\n\n## 三、从采样到统计：如何“读懂”后验样本\n\n当 MCMC 完成采样后，我们获得一系列后验样本：  \n$\\{\\theta^{(1)}, \\theta^{(2)}, \\dots, \\theta^{(S)}\\}$。\n\n### 后验统计量估计\n\n| 统计量      | 数学定义                                 | 采样近似                                                |\n|:-------- |:------------------------------------ |:--------------------------------------------------- |\n| 后验均值     | $\\mathbb{E}[\\theta\\|D]$              | $\\frac{1}{S}\\sum_s \\theta^{(s)}$                    |\n| 方差       | $\\text{Var}[\\theta\\|D]$              | $\\frac{1}{S}\\sum_s (\\theta^{(s)} - \\bar{\\theta})^2$ |\n| 95% 可信区间 | $[\\theta_{2.5\\\\%},\\theta_{97.5\\\\%}]$ | 样本分位数估计                                             |\n\n这些结果提供了参数估计与不确定性量化。\n\n---\n\n## 四、从采样到预测：贝叶斯式前瞻\n\nMCMC 的威力不仅在于“知道参数”，更在于“用参数预测未来”。\n\n### 1. 贝叶斯预测分布\n\n$$P(\\tilde{D}|D) = \\int P(\\tilde{D}|\\theta) P(\\theta|D)\\, d\\theta$$\n\n利用采样点近似：\n\n$$P(\\tilde{D}|D) \\approx \\frac{1}{S} \\sum_{s=1}^S P(\\tilde{D}|\\theta^{(s)})$$\n\n### 2. 实现步骤\n\n1. 从后验样本集中随机选取 $\\theta^{(s)}$。  \n2. 代入似然函数 $P(\\tilde{D}|\\theta^{(s)})$，生成预测数据。  \n3. 汇总所有模拟结果，得到预测分布 $P(\\tilde{D}|D)$。  \n\n这种方法自然地将参数不确定性传播到预测结果中，使预测更加稳健。\n\n---\n\n## 五、小结：让不可算的积分变得可行\n\n| 阶段     | 目标                         | MCMC 的作用      |\n| ------ | -------------------------- | ------------- |\n| 参数推断   | $P(\\theta\\|D)$             | 构造马尔可夫链采样逼近后验 |\n| 不确定性评估 | $\\mathbb{E}[f(\\theta)\\|D]$ | 用采样均值替代积分     |\n| 预测阶段   | $P(\\tilde{D}\\|D)$          | 基于后验样本模拟未来数据  |\n\n> ***MCMC 是贝叶斯思想的与现实的桥梁。**  \n> 它让“不可积”的概率世界，通过采样变得可触、可算、可解释。\n","source":"_posts/3）分层贝叶斯：MCMC.md","raw":"---\ntitle: 分层贝叶斯（3）：MCMC——理论与现实的桥梁\ndate: 2025-10-07 16:13\n\ncategory: 算法\n\nkatex: true\ncover: /img/bys.png\n---\n\n# ⚙️ 分层贝叶斯：MCMC——复杂后验的采样\n\n## 引言：当贝叶斯遇到“维度灾难”\n\n在前两篇文章中，我们了解了贝叶斯推断的基础（先验、似然、后验）以及分层贝叶斯模型（HBM）如何通过**超参数 $\\alpha$** 实现信息共享。\n\n分层模型虽然强大，但也带来了巨大的计算挑战。回想贝叶斯定理：\n\n$$P(\\theta|D) \\propto P(D|\\theta) P(\\theta)$$\n\n当我们有 $M$ 个局部参数 $\\theta_m$ 和超参数 $\\alpha$ 时，我们需要推断的是一个关于 $(\\theta_1, \\dots, \\theta_M, \\alpha)$ 的**联合后验分布**。在现实模型中，这个参数空间可能有数百甚至数千个维度。\n\n* **挑战：** 这种高维的后验分布 $P(\\theta|D)$ 形状复杂、多峰，**无法通过传统的解析积分求解**。  \n* **解决方案：** 我们需要一种方法来“近似”这个复杂的分布——这就是 **马尔可夫链蒙特卡洛（Markov Chain Monte Carlo, MCMC）** 采样方法。\n\n---\n\n## 一、从积分到采样：用“频率”逼近“密度”\n\n### 1. 从积分的痛点说起\n\n贝叶斯推断的核心在于后验分布：\n\n$$P(\\theta|D) = \\frac{P(D|\\theta)P(\\theta)}{P(D)}$$\n\n其中分母：\n\n$$P(D) = \\int P(D|\\theta)P(\\theta)\\, d\\theta$$\n\n通常无法解析求解。  \n于是我们希望通过**随机采样**来逼近这个积分。\n\n如果我们能够从后验分布 $P(\\theta|D)$ 中采样得到点集 $\\{\\theta^{(1)}, \\dots, \\theta^{(N)}\\}$，那么任意函数的期望就可以近似为：\n\n$$\\mathbb{E}[f(\\theta)] \\approx \\frac{1}{N}\\sum_{i=1}^N f(\\theta^{(i)})$$\n\n换句话说：  \n**积分太难？那就通过采样求平均。**\n\n---\n\n### 2. 蒙特卡洛（Monte Carlo）的思想\n\n蒙特卡洛方法用随机抽样替代复杂计算。  \n例如：要估计一个复杂形状区域的面积，只需在外接正方形中随机撒点，通过落入目标区域的比例估算面积。\n\n同样地，在贝叶斯推断中，我们通过**样本频率**近似**概率密度**。  \n样本在后验高密度区域出现得更频繁，自然反映了后验分布的形状。\n\n---\n\n## 二、MCMC 的核心：让样本“走向后验”\n\n### 1. 马尔可夫链：带有记忆衰减的随机游走\n\n马尔可夫链是一种**无记忆性随机过程**：\n\n$$P(\\theta^{(t+1)}|\\theta^{(t)}, \\theta^{(t-1)}, \\dots) = P(\\theta^{(t+1)}|\\theta^{(t)})$$\n\nMCMC 的目标是构造这样一条链，使其在长期运行后，状态分布稳定地收敛到目标分布 $P(\\theta|D)$。  \n只要链是**不可约（Irreducible）**、**非周期（Aperiodic）且满足平稳性（Stationarity）**，采样点就能代表后验分布。\n\n> MCMC 让链在高概率区域停留更久，低概率区域更快离开。  \n> 链走得越久，样本分布越接近真实后验。\n\n---\n\n### 2. Metropolis–Hastings 算法（MH）\n\nMH 是最经典的 MCMC 算法。  \n它通过“提出–接受–拒绝”的机制实现采样：\n\n1. **初始化** $\\theta^{(0)}$。  \n\n2. **提出候选样本** $\\theta'$，从提议分布 $q(\\theta'|\\theta^{(t)})$ 生成。  \n\n3. **计算接受率：**\n   \n   $$r = \\frac{P(D|\\theta') P(\\theta') q(\\theta^{(t)}|\\theta')}{P(D|\\theta^{(t)}) P(\\theta^{(t)}) q(\\theta'|\\theta^{(t)})}$$\n\n4. **接受或拒绝：**\n   \n   $$ \\theta^{(t+1)} =\\begin{cases}\\theta', & \\text{以概率 } \\min(1, r) \\text{ 接受}\\\\\\theta^{(t)}, & \\text{否则保持不变}\\end{cases}$$\n\n5. 重复迭代直到收敛。\n\n在充分运行后，样本序列 $\\{\\theta^{(1)}, \\dots, \\theta^{(N)}\\}$ 就服从目标后验分布。\n\n## 三、从采样到统计：如何“读懂”后验样本\n\n当 MCMC 完成采样后，我们获得一系列后验样本：  \n$\\{\\theta^{(1)}, \\theta^{(2)}, \\dots, \\theta^{(S)}\\}$。\n\n### 后验统计量估计\n\n| 统计量      | 数学定义                                 | 采样近似                                                |\n|:-------- |:------------------------------------ |:--------------------------------------------------- |\n| 后验均值     | $\\mathbb{E}[\\theta\\|D]$              | $\\frac{1}{S}\\sum_s \\theta^{(s)}$                    |\n| 方差       | $\\text{Var}[\\theta\\|D]$              | $\\frac{1}{S}\\sum_s (\\theta^{(s)} - \\bar{\\theta})^2$ |\n| 95% 可信区间 | $[\\theta_{2.5\\\\%},\\theta_{97.5\\\\%}]$ | 样本分位数估计                                             |\n\n这些结果提供了参数估计与不确定性量化。\n\n---\n\n## 四、从采样到预测：贝叶斯式前瞻\n\nMCMC 的威力不仅在于“知道参数”，更在于“用参数预测未来”。\n\n### 1. 贝叶斯预测分布\n\n$$P(\\tilde{D}|D) = \\int P(\\tilde{D}|\\theta) P(\\theta|D)\\, d\\theta$$\n\n利用采样点近似：\n\n$$P(\\tilde{D}|D) \\approx \\frac{1}{S} \\sum_{s=1}^S P(\\tilde{D}|\\theta^{(s)})$$\n\n### 2. 实现步骤\n\n1. 从后验样本集中随机选取 $\\theta^{(s)}$。  \n2. 代入似然函数 $P(\\tilde{D}|\\theta^{(s)})$，生成预测数据。  \n3. 汇总所有模拟结果，得到预测分布 $P(\\tilde{D}|D)$。  \n\n这种方法自然地将参数不确定性传播到预测结果中，使预测更加稳健。\n\n---\n\n## 五、小结：让不可算的积分变得可行\n\n| 阶段     | 目标                         | MCMC 的作用      |\n| ------ | -------------------------- | ------------- |\n| 参数推断   | $P(\\theta\\|D)$             | 构造马尔可夫链采样逼近后验 |\n| 不确定性评估 | $\\mathbb{E}[f(\\theta)\\|D]$ | 用采样均值替代积分     |\n| 预测阶段   | $P(\\tilde{D}\\|D)$          | 基于后验样本模拟未来数据  |\n\n> ***MCMC 是贝叶斯思想的与现实的桥梁。**  \n> 它让“不可积”的概率世界，通过采样变得可触、可算、可解释。\n","slug":"3）分层贝叶斯：MCMC","published":1,"updated":"2025-10-07T06:20:01.859Z","_id":"cuid7UXLFEoRXt5OYeWRkh59E","comments":1,"layout":"post","photos":[],"content":"<h1 id=\"⚙️-分层贝叶斯：MCMC——复杂后验的采样\"><a href=\"#⚙️-分层贝叶斯：MCMC——复杂后验的采样\" class=\"headerlink\" title=\"⚙️ 分层贝叶斯：MCMC——复杂后验的采样\"></a>⚙️ 分层贝叶斯：MCMC——复杂后验的采样</h1><h2 id=\"引言：当贝叶斯遇到“维度灾难”\"><a href=\"#引言：当贝叶斯遇到“维度灾难”\" class=\"headerlink\" title=\"引言：当贝叶斯遇到“维度灾难”\"></a>引言：当贝叶斯遇到“维度灾难”</h2><p>在前两篇文章中，我们了解了贝叶斯推断的基础（先验、似然、后验）以及分层贝叶斯模型（HBM）如何通过<strong>超参数 $\\alpha$</strong> 实现信息共享。</p>\n<p>分层模型虽然强大，但也带来了巨大的计算挑战。回想贝叶斯定理：</p>\n<p>$$P(\\theta|D) \\propto P(D|\\theta) P(\\theta)$$</p>\n<p>当我们有 $M$ 个局部参数 $\\theta_m$ 和超参数 $\\alpha$ 时，我们需要推断的是一个关于 $(\\theta_1, \\dots, \\theta_M, \\alpha)$ 的<strong>联合后验分布</strong>。在现实模型中，这个参数空间可能有数百甚至数千个维度。</p>\n<ul>\n<li><strong>挑战：</strong> 这种高维的后验分布 $P(\\theta|D)$ 形状复杂、多峰，<strong>无法通过传统的解析积分求解</strong>。  </li>\n<li><strong>解决方案：</strong> 我们需要一种方法来“近似”这个复杂的分布——这就是 <strong>马尔可夫链蒙特卡洛（Markov Chain Monte Carlo, MCMC）</strong> 采样方法。</li>\n</ul>\n<hr>\n<h2 id=\"一、从积分到采样：用“频率”逼近“密度”\"><a href=\"#一、从积分到采样：用“频率”逼近“密度”\" class=\"headerlink\" title=\"一、从积分到采样：用“频率”逼近“密度”\"></a>一、从积分到采样：用“频率”逼近“密度”</h2><h3 id=\"1-从积分的痛点说起\"><a href=\"#1-从积分的痛点说起\" class=\"headerlink\" title=\"1. 从积分的痛点说起\"></a>1. 从积分的痛点说起</h3><p>贝叶斯推断的核心在于后验分布：</p>\n<p>$$P(\\theta|D) &#x3D; \\frac{P(D|\\theta)P(\\theta)}{P(D)}$$</p>\n<p>其中分母：</p>\n<p>$$P(D) &#x3D; \\int P(D|\\theta)P(\\theta), d\\theta$$</p>\n<p>通常无法解析求解。<br>于是我们希望通过<strong>随机采样</strong>来逼近这个积分。</p>\n<p>如果我们能够从后验分布 $P(\\theta|D)$ 中采样得到点集 ${\\theta^{(1)}, \\dots, \\theta^{(N)}}$，那么任意函数的期望就可以近似为：</p>\n<p>$$\\mathbb{E}[f(\\theta)] \\approx \\frac{1}{N}\\sum_{i&#x3D;1}^N f(\\theta^{(i)})$$</p>\n<p>换句话说：<br><strong>积分太难？那就通过采样求平均。</strong></p>\n<hr>\n<h3 id=\"2-蒙特卡洛（Monte-Carlo）的思想\"><a href=\"#2-蒙特卡洛（Monte-Carlo）的思想\" class=\"headerlink\" title=\"2. 蒙特卡洛（Monte Carlo）的思想\"></a>2. 蒙特卡洛（Monte Carlo）的思想</h3><p>蒙特卡洛方法用随机抽样替代复杂计算。<br>例如：要估计一个复杂形状区域的面积，只需在外接正方形中随机撒点，通过落入目标区域的比例估算面积。</p>\n<p>同样地，在贝叶斯推断中，我们通过<strong>样本频率</strong>近似<strong>概率密度</strong>。<br>样本在后验高密度区域出现得更频繁，自然反映了后验分布的形状。</p>\n<hr>\n<h2 id=\"二、MCMC-的核心：让样本“走向后验”\"><a href=\"#二、MCMC-的核心：让样本“走向后验”\" class=\"headerlink\" title=\"二、MCMC 的核心：让样本“走向后验”\"></a>二、MCMC 的核心：让样本“走向后验”</h2><h3 id=\"1-马尔可夫链：带有记忆衰减的随机游走\"><a href=\"#1-马尔可夫链：带有记忆衰减的随机游走\" class=\"headerlink\" title=\"1. 马尔可夫链：带有记忆衰减的随机游走\"></a>1. 马尔可夫链：带有记忆衰减的随机游走</h3><p>马尔可夫链是一种<strong>无记忆性随机过程</strong>：</p>\n<p>$$P(\\theta^{(t+1)}|\\theta^{(t)}, \\theta^{(t-1)}, \\dots) &#x3D; P(\\theta^{(t+1)}|\\theta^{(t)})$$</p>\n<p>MCMC 的目标是构造这样一条链，使其在长期运行后，状态分布稳定地收敛到目标分布 $P(\\theta|D)$。<br>只要链是<strong>不可约（Irreducible）</strong>、<strong>非周期（Aperiodic）且满足平稳性（Stationarity）</strong>，采样点就能代表后验分布。</p>\n<blockquote>\n<p>MCMC 让链在高概率区域停留更久，低概率区域更快离开。<br>链走得越久，样本分布越接近真实后验。</p>\n</blockquote>\n<hr>\n<h3 id=\"2-Metropolis–Hastings-算法（MH）\"><a href=\"#2-Metropolis–Hastings-算法（MH）\" class=\"headerlink\" title=\"2. Metropolis–Hastings 算法（MH）\"></a>2. Metropolis–Hastings 算法（MH）</h3><p>MH 是最经典的 MCMC 算法。<br>它通过“提出–接受–拒绝”的机制实现采样：</p>\n<ol>\n<li><p><strong>初始化</strong> $\\theta^{(0)}$。  </p>\n</li>\n<li><p><strong>提出候选样本</strong> $\\theta’$，从提议分布 $q(\\theta’|\\theta^{(t)})$ 生成。  </p>\n</li>\n<li><p><strong>计算接受率：</strong></p>\n<p>$$r &#x3D; \\frac{P(D|\\theta’) P(\\theta’) q(\\theta^{(t)}|\\theta’)}{P(D|\\theta^{(t)}) P(\\theta^{(t)}) q(\\theta’|\\theta^{(t)})}$$</p>\n</li>\n<li><p><strong>接受或拒绝：</strong></p>\n<p>$$ \\theta^{(t+1)} &#x3D;\\begin{cases}\\theta’, &amp; \\text{以概率 } \\min(1, r) \\text{ 接受}\\\\theta^{(t)}, &amp; \\text{否则保持不变}\\end{cases}$$</p>\n</li>\n<li><p>重复迭代直到收敛。</p>\n</li>\n</ol>\n<p>在充分运行后，样本序列 ${\\theta^{(1)}, \\dots, \\theta^{(N)}}$ 就服从目标后验分布。</p>\n<h2 id=\"三、从采样到统计：如何“读懂”后验样本\"><a href=\"#三、从采样到统计：如何“读懂”后验样本\" class=\"headerlink\" title=\"三、从采样到统计：如何“读懂”后验样本\"></a>三、从采样到统计：如何“读懂”后验样本</h2><p>当 MCMC 完成采样后，我们获得一系列后验样本：<br>${\\theta^{(1)}, \\theta^{(2)}, \\dots, \\theta^{(S)}}$。</p>\n<h3 id=\"后验统计量估计\"><a href=\"#后验统计量估计\" class=\"headerlink\" title=\"后验统计量估计\"></a>后验统计量估计</h3><table>\n<thead>\n<tr>\n<th align=\"left\">统计量</th>\n<th align=\"left\">数学定义</th>\n<th align=\"left\">采样近似</th>\n</tr>\n</thead>\n<tbody><tr>\n<td align=\"left\">后验均值</td>\n<td align=\"left\">$\\mathbb{E}[\\theta|D]$</td>\n<td align=\"left\">$\\frac{1}{S}\\sum_s \\theta^{(s)}$</td>\n</tr>\n<tr>\n<td align=\"left\">方差</td>\n<td align=\"left\">$\\text{Var}[\\theta|D]$</td>\n<td align=\"left\">$\\frac{1}{S}\\sum_s (\\theta^{(s)} - \\bar{\\theta})^2$</td>\n</tr>\n<tr>\n<td align=\"left\">95% 可信区间</td>\n<td align=\"left\">$[\\theta_{2.5\\%},\\theta_{97.5\\%}]$</td>\n<td align=\"left\">样本分位数估计</td>\n</tr>\n</tbody></table>\n<p>这些结果提供了参数估计与不确定性量化。</p>\n<hr>\n<h2 id=\"四、从采样到预测：贝叶斯式前瞻\"><a href=\"#四、从采样到预测：贝叶斯式前瞻\" class=\"headerlink\" title=\"四、从采样到预测：贝叶斯式前瞻\"></a>四、从采样到预测：贝叶斯式前瞻</h2><p>MCMC 的威力不仅在于“知道参数”，更在于“用参数预测未来”。</p>\n<h3 id=\"1-贝叶斯预测分布\"><a href=\"#1-贝叶斯预测分布\" class=\"headerlink\" title=\"1. 贝叶斯预测分布\"></a>1. 贝叶斯预测分布</h3><p>$$P(\\tilde{D}|D) &#x3D; \\int P(\\tilde{D}|\\theta) P(\\theta|D), d\\theta$$</p>\n<p>利用采样点近似：</p>\n<p>$$P(\\tilde{D}|D) \\approx \\frac{1}{S} \\sum_{s&#x3D;1}^S P(\\tilde{D}|\\theta^{(s)})$$</p>\n<h3 id=\"2-实现步骤\"><a href=\"#2-实现步骤\" class=\"headerlink\" title=\"2. 实现步骤\"></a>2. 实现步骤</h3><ol>\n<li>从后验样本集中随机选取 $\\theta^{(s)}$。  </li>\n<li>代入似然函数 $P(\\tilde{D}|\\theta^{(s)})$，生成预测数据。  </li>\n<li>汇总所有模拟结果，得到预测分布 $P(\\tilde{D}|D)$。</li>\n</ol>\n<p>这种方法自然地将参数不确定性传播到预测结果中，使预测更加稳健。</p>\n<hr>\n<h2 id=\"五、小结：让不可算的积分变得可行\"><a href=\"#五、小结：让不可算的积分变得可行\" class=\"headerlink\" title=\"五、小结：让不可算的积分变得可行\"></a>五、小结：让不可算的积分变得可行</h2><table>\n<thead>\n<tr>\n<th>阶段</th>\n<th>目标</th>\n<th>MCMC 的作用</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>参数推断</td>\n<td>$P(\\theta|D)$</td>\n<td>构造马尔可夫链采样逼近后验</td>\n</tr>\n<tr>\n<td>不确定性评估</td>\n<td>$\\mathbb{E}[f(\\theta)|D]$</td>\n<td>用采样均值替代积分</td>\n</tr>\n<tr>\n<td>预测阶段</td>\n<td>$P(\\tilde{D}|D)$</td>\n<td>基于后验样本模拟未来数据</td>\n</tr>\n</tbody></table>\n<blockquote>\n<p>*<strong>MCMC 是贝叶斯思想的与现实的桥梁。</strong><br>它让“不可积”的概率世界，通过采样变得可触、可算、可解释。</p>\n</blockquote>\n","excerpt":"","more":"<h1 id=\"⚙️-分层贝叶斯：MCMC——复杂后验的采样\"><a href=\"#⚙️-分层贝叶斯：MCMC——复杂后验的采样\" class=\"headerlink\" title=\"⚙️ 分层贝叶斯：MCMC——复杂后验的采样\"></a>⚙️ 分层贝叶斯：MCMC——复杂后验的采样</h1><h2 id=\"引言：当贝叶斯遇到“维度灾难”\"><a href=\"#引言：当贝叶斯遇到“维度灾难”\" class=\"headerlink\" title=\"引言：当贝叶斯遇到“维度灾难”\"></a>引言：当贝叶斯遇到“维度灾难”</h2><p>在前两篇文章中，我们了解了贝叶斯推断的基础（先验、似然、后验）以及分层贝叶斯模型（HBM）如何通过<strong>超参数 $\\alpha$</strong> 实现信息共享。</p>\n<p>分层模型虽然强大，但也带来了巨大的计算挑战。回想贝叶斯定理：</p>\n<p>$$P(\\theta|D) \\propto P(D|\\theta) P(\\theta)$$</p>\n<p>当我们有 $M$ 个局部参数 $\\theta_m$ 和超参数 $\\alpha$ 时，我们需要推断的是一个关于 $(\\theta_1, \\dots, \\theta_M, \\alpha)$ 的<strong>联合后验分布</strong>。在现实模型中，这个参数空间可能有数百甚至数千个维度。</p>\n<ul>\n<li><strong>挑战：</strong> 这种高维的后验分布 $P(\\theta|D)$ 形状复杂、多峰，<strong>无法通过传统的解析积分求解</strong>。  </li>\n<li><strong>解决方案：</strong> 我们需要一种方法来“近似”这个复杂的分布——这就是 <strong>马尔可夫链蒙特卡洛（Markov Chain Monte Carlo, MCMC）</strong> 采样方法。</li>\n</ul>\n<hr>\n<h2 id=\"一、从积分到采样：用“频率”逼近“密度”\"><a href=\"#一、从积分到采样：用“频率”逼近“密度”\" class=\"headerlink\" title=\"一、从积分到采样：用“频率”逼近“密度”\"></a>一、从积分到采样：用“频率”逼近“密度”</h2><h3 id=\"1-从积分的痛点说起\"><a href=\"#1-从积分的痛点说起\" class=\"headerlink\" title=\"1. 从积分的痛点说起\"></a>1. 从积分的痛点说起</h3><p>贝叶斯推断的核心在于后验分布：</p>\n<p>$$P(\\theta|D) &#x3D; \\frac{P(D|\\theta)P(\\theta)}{P(D)}$$</p>\n<p>其中分母：</p>\n<p>$$P(D) &#x3D; \\int P(D|\\theta)P(\\theta), d\\theta$$</p>\n<p>通常无法解析求解。<br>于是我们希望通过<strong>随机采样</strong>来逼近这个积分。</p>\n<p>如果我们能够从后验分布 $P(\\theta|D)$ 中采样得到点集 ${\\theta^{(1)}, \\dots, \\theta^{(N)}}$，那么任意函数的期望就可以近似为：</p>\n<p>$$\\mathbb{E}[f(\\theta)] \\approx \\frac{1}{N}\\sum_{i&#x3D;1}^N f(\\theta^{(i)})$$</p>\n<p>换句话说：<br><strong>积分太难？那就通过采样求平均。</strong></p>\n<hr>\n<h3 id=\"2-蒙特卡洛（Monte-Carlo）的思想\"><a href=\"#2-蒙特卡洛（Monte-Carlo）的思想\" class=\"headerlink\" title=\"2. 蒙特卡洛（Monte Carlo）的思想\"></a>2. 蒙特卡洛（Monte Carlo）的思想</h3><p>蒙特卡洛方法用随机抽样替代复杂计算。<br>例如：要估计一个复杂形状区域的面积，只需在外接正方形中随机撒点，通过落入目标区域的比例估算面积。</p>\n<p>同样地，在贝叶斯推断中，我们通过<strong>样本频率</strong>近似<strong>概率密度</strong>。<br>样本在后验高密度区域出现得更频繁，自然反映了后验分布的形状。</p>\n<hr>\n<h2 id=\"二、MCMC-的核心：让样本“走向后验”\"><a href=\"#二、MCMC-的核心：让样本“走向后验”\" class=\"headerlink\" title=\"二、MCMC 的核心：让样本“走向后验”\"></a>二、MCMC 的核心：让样本“走向后验”</h2><h3 id=\"1-马尔可夫链：带有记忆衰减的随机游走\"><a href=\"#1-马尔可夫链：带有记忆衰减的随机游走\" class=\"headerlink\" title=\"1. 马尔可夫链：带有记忆衰减的随机游走\"></a>1. 马尔可夫链：带有记忆衰减的随机游走</h3><p>马尔可夫链是一种<strong>无记忆性随机过程</strong>：</p>\n<p>$$P(\\theta^{(t+1)}|\\theta^{(t)}, \\theta^{(t-1)}, \\dots) &#x3D; P(\\theta^{(t+1)}|\\theta^{(t)})$$</p>\n<p>MCMC 的目标是构造这样一条链，使其在长期运行后，状态分布稳定地收敛到目标分布 $P(\\theta|D)$。<br>只要链是<strong>不可约（Irreducible）</strong>、<strong>非周期（Aperiodic）且满足平稳性（Stationarity）</strong>，采样点就能代表后验分布。</p>\n<blockquote>\n<p>MCMC 让链在高概率区域停留更久，低概率区域更快离开。<br>链走得越久，样本分布越接近真实后验。</p>\n</blockquote>\n<hr>\n<h3 id=\"2-Metropolis–Hastings-算法（MH）\"><a href=\"#2-Metropolis–Hastings-算法（MH）\" class=\"headerlink\" title=\"2. Metropolis–Hastings 算法（MH）\"></a>2. Metropolis–Hastings 算法（MH）</h3><p>MH 是最经典的 MCMC 算法。<br>它通过“提出–接受–拒绝”的机制实现采样：</p>\n<ol>\n<li><p><strong>初始化</strong> $\\theta^{(0)}$。  </p>\n</li>\n<li><p><strong>提出候选样本</strong> $\\theta’$，从提议分布 $q(\\theta’|\\theta^{(t)})$ 生成。  </p>\n</li>\n<li><p><strong>计算接受率：</strong></p>\n<p>$$r &#x3D; \\frac{P(D|\\theta’) P(\\theta’) q(\\theta^{(t)}|\\theta’)}{P(D|\\theta^{(t)}) P(\\theta^{(t)}) q(\\theta’|\\theta^{(t)})}$$</p>\n</li>\n<li><p><strong>接受或拒绝：</strong></p>\n<p>$$ \\theta^{(t+1)} &#x3D;\\begin{cases}\\theta’, &amp; \\text{以概率 } \\min(1, r) \\text{ 接受}\\\\theta^{(t)}, &amp; \\text{否则保持不变}\\end{cases}$$</p>\n</li>\n<li><p>重复迭代直到收敛。</p>\n</li>\n</ol>\n<p>在充分运行后，样本序列 ${\\theta^{(1)}, \\dots, \\theta^{(N)}}$ 就服从目标后验分布。</p>\n<h2 id=\"三、从采样到统计：如何“读懂”后验样本\"><a href=\"#三、从采样到统计：如何“读懂”后验样本\" class=\"headerlink\" title=\"三、从采样到统计：如何“读懂”后验样本\"></a>三、从采样到统计：如何“读懂”后验样本</h2><p>当 MCMC 完成采样后，我们获得一系列后验样本：<br>${\\theta^{(1)}, \\theta^{(2)}, \\dots, \\theta^{(S)}}$。</p>\n<h3 id=\"后验统计量估计\"><a href=\"#后验统计量估计\" class=\"headerlink\" title=\"后验统计量估计\"></a>后验统计量估计</h3><table>\n<thead>\n<tr>\n<th align=\"left\">统计量</th>\n<th align=\"left\">数学定义</th>\n<th align=\"left\">采样近似</th>\n</tr>\n</thead>\n<tbody><tr>\n<td align=\"left\">后验均值</td>\n<td align=\"left\">$\\mathbb{E}[\\theta|D]$</td>\n<td align=\"left\">$\\frac{1}{S}\\sum_s \\theta^{(s)}$</td>\n</tr>\n<tr>\n<td align=\"left\">方差</td>\n<td align=\"left\">$\\text{Var}[\\theta|D]$</td>\n<td align=\"left\">$\\frac{1}{S}\\sum_s (\\theta^{(s)} - \\bar{\\theta})^2$</td>\n</tr>\n<tr>\n<td align=\"left\">95% 可信区间</td>\n<td align=\"left\">$[\\theta_{2.5\\%},\\theta_{97.5\\%}]$</td>\n<td align=\"left\">样本分位数估计</td>\n</tr>\n</tbody></table>\n<p>这些结果提供了参数估计与不确定性量化。</p>\n<hr>\n<h2 id=\"四、从采样到预测：贝叶斯式前瞻\"><a href=\"#四、从采样到预测：贝叶斯式前瞻\" class=\"headerlink\" title=\"四、从采样到预测：贝叶斯式前瞻\"></a>四、从采样到预测：贝叶斯式前瞻</h2><p>MCMC 的威力不仅在于“知道参数”，更在于“用参数预测未来”。</p>\n<h3 id=\"1-贝叶斯预测分布\"><a href=\"#1-贝叶斯预测分布\" class=\"headerlink\" title=\"1. 贝叶斯预测分布\"></a>1. 贝叶斯预测分布</h3><p>$$P(\\tilde{D}|D) &#x3D; \\int P(\\tilde{D}|\\theta) P(\\theta|D), d\\theta$$</p>\n<p>利用采样点近似：</p>\n<p>$$P(\\tilde{D}|D) \\approx \\frac{1}{S} \\sum_{s&#x3D;1}^S P(\\tilde{D}|\\theta^{(s)})$$</p>\n<h3 id=\"2-实现步骤\"><a href=\"#2-实现步骤\" class=\"headerlink\" title=\"2. 实现步骤\"></a>2. 实现步骤</h3><ol>\n<li>从后验样本集中随机选取 $\\theta^{(s)}$。  </li>\n<li>代入似然函数 $P(\\tilde{D}|\\theta^{(s)})$，生成预测数据。  </li>\n<li>汇总所有模拟结果，得到预测分布 $P(\\tilde{D}|D)$。</li>\n</ol>\n<p>这种方法自然地将参数不确定性传播到预测结果中，使预测更加稳健。</p>\n<hr>\n<h2 id=\"五、小结：让不可算的积分变得可行\"><a href=\"#五、小结：让不可算的积分变得可行\" class=\"headerlink\" title=\"五、小结：让不可算的积分变得可行\"></a>五、小结：让不可算的积分变得可行</h2><table>\n<thead>\n<tr>\n<th>阶段</th>\n<th>目标</th>\n<th>MCMC 的作用</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>参数推断</td>\n<td>$P(\\theta|D)$</td>\n<td>构造马尔可夫链采样逼近后验</td>\n</tr>\n<tr>\n<td>不确定性评估</td>\n<td>$\\mathbb{E}[f(\\theta)|D]$</td>\n<td>用采样均值替代积分</td>\n</tr>\n<tr>\n<td>预测阶段</td>\n<td>$P(\\tilde{D}|D)$</td>\n<td>基于后验样本模拟未来数据</td>\n</tr>\n</tbody></table>\n<blockquote>\n<p>*<strong>MCMC 是贝叶斯思想的与现实的桥梁。</strong><br>它让“不可积”的概率世界，通过采样变得可触、可算、可解释。</p>\n</blockquote>\n"}],"PostAsset":[],"PostCategory":[{"post_id":"cuidlEkf9c-m5o6skFFETl2wC","category_id":"cuidgPvbAZnTxLik2uzhatT8N","_id":"cuidkQi0rYM7ESg-drk-xh0gG"},{"post_id":"cuid9DXnwje6XE0HKV7s7I578","category_id":"cuidgPvbAZnTxLik2uzhatT8N","_id":"cuid7g2dHG81bbbtPzLDFlGDY"},{"post_id":"cuidwUe8hGfUsuJ1gxvO-9axr","category_id":"cuid-Y7OmDUvgoEY9pvx4ncaC","_id":"cuidNjZ4bPfhEk9iic1e4diFw"},{"post_id":"cuidoDNz9NcHD4NxNj8nR8Is7","category_id":"cuid-Y7OmDUvgoEY9pvx4ncaC","_id":"cuid4ySurB3GNGNLj0iCKOfGr"},{"post_id":"cuid9DXnwje6XE0HKV7s7I578","category_id":"cuidgPvbAZnTxLik2uzhatT8N","_id":"cuidT5uDAqwpePyfuRC0Emzic"},{"post_id":"cuid9DXnwje6XE0HKV7s7I578","category_id":"cuidgPvbAZnTxLik2uzhatT8N","_id":"cuidcGXDULocZ6KZoBC9osXPJ"},{"post_id":"cuidlEkf9c-m5o6skFFETl2wC","category_id":"cuidgPvbAZnTxLik2uzhatT8N","_id":"cuidi9Mp51XNyT9tBFWrvQvtl"},{"post_id":"cuidwUe8hGfUsuJ1gxvO-9axr","category_id":"cuid-Y7OmDUvgoEY9pvx4ncaC","_id":"cuideY03VJmlBwMbbFzMtDHw7"},{"post_id":"cuidoDNz9NcHD4NxNj8nR8Is7","category_id":"cuid-Y7OmDUvgoEY9pvx4ncaC","_id":"cuidfoyl9pKxL9nQgTw59OmcP"},{"post_id":"cuid7UXLFEoRXt5OYeWRkh59E","category_id":"cuid-Y7OmDUvgoEY9pvx4ncaC","_id":"cuid6mTOG1wRW-GQrCP09b0yE"},{"post_id":"cuidwUe8hGfUsuJ1gxvO-9axr","category_id":"cuid-Y7OmDUvgoEY9pvx4ncaC","_id":"cuidCVhB7WBtPPgFMwiCF5xk_"},{"post_id":"cuid7UXLFEoRXt5OYeWRkh59E","category_id":"cuid-Y7OmDUvgoEY9pvx4ncaC","_id":"cuidiMPjvhH1lE2Lc9gIRv3ug"},{"post_id":"cuidoDNz9NcHD4NxNj8nR8Is7","category_id":"cuid-Y7OmDUvgoEY9pvx4ncaC","_id":"cuid155RW7c4ofra9GzUIVZcG"},{"post_id":"cuidlEkf9c-m5o6skFFETl2wC","category_id":"cuidgPvbAZnTxLik2uzhatT8N","_id":"cuidfkhEyxolO4T_72B3ByHJg"},{"post_id":"cuid9DXnwje6XE0HKV7s7I578","category_id":"cuidgPvbAZnTxLik2uzhatT8N","_id":"cuidEJQdX0f3p_lPQUUx0br-o"}],"PostTag":[],"Tag":[]}}