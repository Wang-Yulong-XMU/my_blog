---
title: 分层贝叶斯（1）：贝叶斯方法
date: 2025-10-06 16:55

category: 算法

katex: true
cover: /img/bys.png

---

# 🚀 贝叶斯定理：先验、后验与似然

## 引言：当“信念”遇到“数据”

在日常生活中，我们经常根据新的观察来修正自己的看法或信念。**贝叶斯方法（Bayesian Method)** 正是将这种人类的认知过程——**信念的更新**——用严谨的数学形式固定下来。

在机器学习与统计学中，**贝叶斯定理**提供了一个框架，用于**量化不确定性**，并根据观测数据对模型参数进行更新。本文将从最简单的 **朴素贝叶斯（Naive Bayes）** 分类器入手，通过一个直观的**抛硬币实验**，理解贝叶斯公式中的三大核心要素：**先验（Prior）**、**似然（Likelihood）** 与 **后验（Posterior）**。

---

## 一、引子：抛硬币实验与参数  $\theta$

假设你手中有一枚硬币，我们想知道它**正面朝上的真实概率** $\theta$ 是多少。这个 $\theta$ 是我们模型中的**参数（Parameter）**，它是我们希望根据数据推断的 **“真相”**。

### 1. 初始信念：先验分布 $P(\theta)$

在观测数据 $D$ 之前，我们对 $\theta$ 的取值已有初始信念，这就是 **先验分布（Prior Distribution）**。

* **定义：** $P(\theta)$ 描述了观测数据之前，我们对参数 $\theta$ **所有可能取值的信念与不确定性**。
* **要求：** **先验必须是一个概率分布**，而非单一数值。
* **作用：** 代表我们在看到数据之前的主观假设，它会影响数据量小时的推断结果。

### 2. 数据支持度：似然函数 $P(D|\theta)$

假设我们抛硬币 $N$ 次，得到 $k$ 次正面（数据记为 $D$）。由于抛硬币符合**二项分布**，其似然函数 $P(D|\theta)$ 为：
$$P(D|\theta) = \binom{N}{k} \theta^k (1-\theta)^{N-k}$$

* **定义：** 似然函数 $P(D|\theta)$ 描述在**给定特定参数 $\theta$** 的情况下，观察到现有数据 $D$ 的**相对概率**。
* **核心：** 似然反映了 **“数据对参数 $\theta$ 的支持度”**。它是一个关于 $\theta$ 的函数，不要求归一化。

### 3. 更新后的信念：后验分布 $P(\theta|D)$

结合了先验和似然后，我们更新对 $\theta$ 的认识，得到**后验分布（Posterior Distribution）**。

后验分布的计算遵循**贝叶斯定理**：

$$P(\theta|D) = \frac{P(D|\theta) P(\theta)}{P(D)}$$

* **定义：** 后验分布 $P(\theta|D)$ 描述了在观测数据后，对参数 $\theta$ 的**更新信念**。
* **直观意义：** 后验是**先验信息**与**数据证据**的加权融合。
* **推断简化：** 在参数推断中，我们通常只关注后验的形状，故可使用比例形式：

$$P(\theta|D) \propto P(D|\theta) P(\theta)$$

---

## 二、贝叶斯定理与朴素贝叶斯

### 1. 证据项 $P(D)$ 的角色

公式分母 $P(D)$ 为**证据（Evidence）或边缘似然（Marginal Likelihood)**，它的定义是一个积分：

$$P(D) = \int P(D|\theta) P(\theta) \, d\theta$$

* **功能：** $P(D)$ 确保后验分布积分为 1，实现**归一化**。
* **推断焦点：** 由于 $P(D)$ 是一个常数，且在复杂模型中难以解析求解，我们在推断参数**形状**时，通常只关注**分子（联合概率密度）**。

### 2. 从参数推断到分类任务

硬币实验说明了贝叶斯推断的思想：**利用数据修正对参数的信念**。在分类问题中，我们推广为：**利用特征数据 $X$ 修正对类别 $C$ 的信念**，从而得到：

$$P(C|X) = \frac{P(X|C) P(C)}{P(X)}$$

* $P(C|X)$：**后验**分类概率。
* $P(X|C)$：特征的**似然**（在 $C$ 下生成 $X$ 的概率）。
* $P(C)$：类别的**先验**概率。

### 3. “朴素”假设：条件独立性

由于特征 $X = \{x_1, x_2, \dots, x_n\}$ 通常维度高且特征间可能相关，直接计算 $P(X|C)$ 非常复杂。

**朴素贝叶斯**通过一个“朴素”的假设来简化计算：**在给定类别 $C$ 的条件下，所有特征间条件独立。**

$$P(X|C) = \prod_{i=1}^n P(x_i|C)$$

这一独立性假设极大简化了计算，使朴素贝叶斯在文本分类等任务中表现高效。

# 🔄 贝叶斯算法的核心：推断与预测

贝叶斯算法的核心在于一个**学习循环**，它利用新的证据持续更新我们对模型参数的信念。这个过程可以清晰地划分为两大阶段：**参数推断（Inference）和新数据预测（Prediction）**。

---

## 一、推断阶段：参数信念的更新

推断阶段的目标是利用观测到的数据 $D$ 来修正模型参数 $\theta$ 的不确定性，核心在于计算**后验分布 $P(\theta|D)$**。

### 1. 基础更新：贝叶斯定理

在接收到一批数据 $D$ 时，我们使用贝叶斯定理进行一次性更新：

$$P(\theta|D) = \frac{P(D|\theta) P(\theta)}{P(D)}$$

在推断**参数形状**时，我们通常只关注**分子**（似然与先验的联合概率密度 $P(D|\theta) P(\theta)$），因为后验分布的形状完全由它决定。

### 2. 连续学习：顺序贝叶斯更新

贝叶斯更新可以进行**迭代**。当我们获得第二批数据 $D'$ 时，上一步的**后验 $P(\theta|D)$** 直接成为下一步的**先验**：

$$P(\theta|D, D') = \frac{P(D'|\theta) P(\theta|D)}{P(D')}$$

这种**顺序贝叶斯更新（Sequential Bayesian Updating）** 遵循直觉：每次看到新数据，都用它来修正已有的信念。

**迭代更新过程的抽象表示：**

$$\text{Prior} \xrightarrow[\text{data } D]{\text{update}} \text{Posterior} \xrightarrow[\text{as new prior}]{\text{new data } D'} \text{Posterior}' \xrightarrow[\text{as new prior}]{\text{new data } D''} \cdots$$

每次新的观测，都会使我们的不确定性逐步减少，后验分布也会越来越集中在真实参数附近。

---

## 二、证据项 $P(D)$：归一化与模型解释

证据项 $P(D)$ 位于贝叶斯定理的分母，被称为**边缘似然（Marginal Likelihood）**。

### 1. 由归一化条件推导积分表达式

由于后验分布必须归一化 ($\int P(\theta|D)\, d\theta = 1$)，我们可以推导出 $P(D)$ 的积分表达式：

$$P(D) = \int P(D|\theta) P(\theta)\, d\theta$$

* **直觉解释：** 数据的概率 $P(D)$ 是对 **“在所有可能参数下生成数据的可能性 $P(D|\theta)$”** 的加权平均（由先验 $P(\theta)$ 加权）。
* **功能：** $P(D)$ 在推断阶段作为**归一化常数**。

### 2. 离散参数的求和形式

当参数 $\theta$ 是离散变量时，积分变为求和：

$$P(D) = \sum_{\theta} P(D|\theta) P(\theta)$$

---

## 三、预测阶段：量化新数据的不确定性

预测阶段的目标是利用推断出的**后验分布 $P(\theta|D)$**，来计算和预测**新数据 $\tilde{D}$ 的分布 $P(\tilde{D}|D)$**。

### 预测分布公式

我们通过对所有可能的参数 $\theta$ 进行积分，将参数的不确定性整合到预测中：

$$P(\tilde{D}|D) = \int P(\tilde{D}|\theta) P(\theta|D) \, d\theta$$

* **$P(\tilde{D}|\theta)$：** 在给定**特定参数 $\theta$** 下，新数据 $\tilde{D}$ 出现的概率（预测似然）。
* **$P(\theta|D)$：** 对参数 $\theta$ 的**后验信念**，充当加权函数。

**贝叶斯预测的关键：** 预测分布 $P(\tilde{D}|D)$ 提供了**完整的概率分布**，这自然地整合了模型参数 $\theta$ 的**所有不确定性**，从而提供比点估计更稳健的预测区间。

---

## 走向分层贝叶斯

简单的贝叶斯模型假设所有数据点都由**单一的 $\theta$** 或**独立的 $\theta_m$** 产生，但当我们面对**多个相似但独立的子问题**（如不同城市、不同样本群体）时，这种独立分析会导致数据稀疏的个体推断不可信。

此时，我们需要引入**分层（Hierarchical）结构**和**超参数（Hyperparameters）**，让参数之间也具有关联性，从而实现**信息共享**。

下一篇：**分层贝叶斯（二）：从参数到分层**。
